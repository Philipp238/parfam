{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d92658-7dd6-4686-abac-b57d0b4f841a",
   "metadata": {},
   "source": [
    "# <center> Optimizing code</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b16178-e7cb-4695-b7e5-aed0569bb718",
   "metadata": {},
   "source": [
    "In this notebook, we will try to optimize some of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910ad67-4f85-48fb-9f44-546746c3f024",
   "metadata": {},
   "source": [
    "## Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2624c1d4-a138-4d3a-8e35-6729d3d2357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy\n",
    "from sympy import symbols\n",
    "from scipy.optimize import dual_annealing, basinhopping\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils import convert_to_module\n",
    "import scipy\n",
    "\n",
    "from itertools import product\n",
    "import copy\n",
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d26942-1161-4841-a416-68705a2bdfa0",
   "metadata": {},
   "source": [
    "## Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dc818376-dc5d-42b9-a8c1-0029515d7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParFamTorch:\n",
    "    def __init__(self, n_input, degree_input_polynomials, degree_output_polynomials, width, functions=[],\n",
    "                 function_names=[], input_names=None, degree_input_polynomials_specific=None,\n",
    "                 degree_output_polynomials_specific=None, degree_input_denominator=0, degree_output_denominator=0,\n",
    "                 degree_input_polynomials_denominator_specific=None, normalize_denom=True, maximal_potence=3,\n",
    "                 degree_output_polynomials_denominator_specific=None, enforce_function=True, device='cpu'):\n",
    "        self.maximal_potence = maximal_potence\n",
    "        self.n_input = n_input\n",
    "        self.degree_input_polynomials = degree_input_polynomials\n",
    "        self.degree_output_polynomials = degree_output_polynomials\n",
    "        self.degree_input_denominator = degree_input_denominator\n",
    "        self.degree_output_denominator = degree_output_denominator\n",
    "        self.width = width\n",
    "        self.functions = functions\n",
    "        self.enforce_function = enforce_function\n",
    "        self.device = device\n",
    "        # Numerator\n",
    "        if degree_input_polynomials_specific:\n",
    "            if len(degree_input_polynomials_specific) == len(functions) * width:\n",
    "                self.degree_input_polynomials_specific = self.n_input * [\n",
    "                    self.maximal_potence] + degree_input_polynomials_specific\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f'degree_input_polynomials_specific {degree_input_polynomials_specific} has to have the same length'\n",
    "                    f' as functions {functions} times width {width}.')\n",
    "        else:\n",
    "            self.degree_input_polynomials_specific = [self.maximal_potence for _ in range(self.n_input)]\n",
    "        if degree_output_polynomials_specific:\n",
    "            self.degree_output_polynomials_specific = self.n_input * [\n",
    "                self.maximal_potence] + degree_output_polynomials_specific\n",
    "        else:\n",
    "            self.degree_output_polynomials_specific = [self.maximal_potence for _ in\n",
    "                                                       range(self.n_input + self.width * len(functions))]\n",
    "        assert len(self.degree_output_polynomials_specific) == self.n_input + self.width * len(\n",
    "            self.functions)  # TODO: more often\n",
    "        self.monomial_list_input = ParFamTorch.list_monomials(self.n_input, self.degree_input_polynomials,\n",
    "                                                              self.degree_input_polynomials_specific)\n",
    "        self.monomial_list_output = ParFamTorch.list_monomials(self.n_input + self.width * len(functions),\n",
    "                                                               self.degree_output_polynomials,\n",
    "                                                               self.degree_output_polynomials_specific)\n",
    "        if self.enforce_function and len(self.functions) > 0:\n",
    "            self.monomial_list_output = [monomial for monomial in self.monomial_list_output if\n",
    "                                         sum(monomial[self.n_input:]) > 0]\n",
    "\n",
    "        # Denominator\n",
    "        if degree_input_polynomials_denominator_specific:\n",
    "            self.degree_input_polynomials_denominator_specific = self.n_input * [\n",
    "                self.maximal_potence] + degree_input_polynomials_denominator_specific\n",
    "        else:\n",
    "            self.degree_input_polynomials_denominator_specific = self.n_input * [self.maximal_potence]\n",
    "        if degree_output_polynomials_denominator_specific:\n",
    "            self.degree_output_polynomials_denominator_specific = self.n_input * [\n",
    "                self.maximal_potence] + degree_output_polynomials_denominator_specific\n",
    "        else:\n",
    "            self.degree_output_polynomials_denominator_specific = [self.maximal_potence for _ in\n",
    "                                                                   range(self.n_input + self.width * len(functions))]\n",
    "\n",
    "        if self.degree_input_denominator > 0:\n",
    "            self.monomial_list_input_denominator = ParFamTorch.list_monomials(self.n_input,\n",
    "                                                                              self.degree_input_denominator,\n",
    "                                                                              self.degree_input_polynomials_denominator_specific)\n",
    "        else:\n",
    "            self.monomial_list_input_denominator = []\n",
    "        if self.degree_output_denominator > 0:\n",
    "            self.monomial_list_output_denominator = ParFamTorch.list_monomials(self.n_input,\n",
    "                                                                               self.degree_output_denominator,\n",
    "                                                                               self.degree_output_polynomials_denominator_specific)\n",
    "        else:\n",
    "            self.monomial_list_output_denominator = []\n",
    "        self.normalize_denom = normalize_denom\n",
    "        self.n_coefficients_first_layer_numerator = len(self.monomial_list_input)\n",
    "        self.n_coefficients_first_layer_denominator = len(self.monomial_list_input_denominator)\n",
    "        self.n_coefficients_first_layer = len(self.monomial_list_input) + len(self.monomial_list_input_denominator)\n",
    "        self.n_coefficients_last_layer_numerator = len(self.monomial_list_output)\n",
    "        self.n_coefficients_last_layer_denominator = len(self.monomial_list_output_denominator)\n",
    "        self.n_coefficients_last_layer = len(self.monomial_list_output) + len(self.monomial_list_output_denominator)\n",
    "        self.input_monomials = None\n",
    "        self.input_monomials_denominator = None\n",
    "        self.output_monomials_dict = None\n",
    "        self.output_monomials_denominator_dict = None\n",
    "        self.function_names = function_names\n",
    "        if input_names is None:\n",
    "            self.input_names = [sympy.symbols(f'x{i}') for i in range(n_input)]\n",
    "        else:\n",
    "            self.input_names = input_names\n",
    "\n",
    "    def get_formula(self, coefficients, decimals=3, verbose=False):\n",
    "        coefficients = torch.tensor(coefficients, device=self.device)\n",
    "        if len(self.function_names) != len(self.functions):\n",
    "            raise ValueError(f'Length of function_names {self.function_names} and functions {self.functions} is not the'\n",
    "                             f' same, please fix this to compute the formula')\n",
    "        feature_names = copy.copy(self.input_names)\n",
    "\n",
    "        t_0 = time.time()\n",
    "        for i, function_name in enumerate(self.function_names):\n",
    "            current_coefficients = coefficients[\n",
    "                                   i * self.n_coefficients_first_layer: i * self.n_coefficients_first_layer\n",
    "                                                                        + self.n_coefficients_first_layer_numerator]\n",
    "            numerator = ParFamTorch._get_symbolic_polynomial(self.n_input, self.input_names,\n",
    "                                                             self.degree_input_polynomials, current_coefficients,\n",
    "                                                             self.degree_input_polynomials_specific,\n",
    "                                                             self.monomial_list_input, decimals=decimals)\n",
    "            if self.degree_input_denominator > 0:\n",
    "                current_coefficients = coefficients[i * self.n_coefficients_first_layer\n",
    "                                                    + self.n_coefficients_first_layer_numerator:\n",
    "                                                    (i + 1) * self.n_coefficients_first_layer]\n",
    "                if self.normalize_denom:\n",
    "                    current_coefficients = current_coefficients / torch.norm(current_coefficients)\n",
    "                denominator = ParFamTorch._get_symbolic_polynomial(self.n_input, self.input_names,\n",
    "                                                                   self.degree_input_denominator, current_coefficients,\n",
    "                                                                   self.degree_input_polynomials_denominator_specific,\n",
    "                                                                   self.monomial_list_input_denominator,\n",
    "                                                                   decimals=decimals)\n",
    "            else:\n",
    "                denominator = 1.0\n",
    "            if isinstance(denominator, float) and abs(denominator) < 10 ** (-4):\n",
    "                denominator = 10 ** (-4)\n",
    "            feature_name = function_name(numerator / denominator)\n",
    "            feature_names.append(feature_name)\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Computing the formula for the input layer took {(t_1 - t_0):.3f} seconds')\n",
    "\n",
    "        t_0 = time.time()\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[\n",
    "                                   -self.n_coefficients_last_layer:-self.n_coefficients_last_layer_denominator]\n",
    "        else:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer:]\n",
    "        numerator = ParFamTorch._get_symbolic_polynomial(len(feature_names), feature_names,\n",
    "                                                         self.degree_output_polynomials,\n",
    "                                                         current_coefficients, self.degree_output_polynomials_specific,\n",
    "                                                         multiindices=self.monomial_list_output, decimals=decimals)\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer_denominator:]\n",
    "            if self.normalize_denom:\n",
    "                current_coefficients = current_coefficients / torch.norm(current_coefficients)\n",
    "            denominator = ParFamTorch._get_symbolic_polynomial(len(feature_names), feature_names,\n",
    "                                                               self.degree_output_denominator,\n",
    "                                                               current_coefficients,\n",
    "                                                               self.degree_output_polynomials_denominator_specific,\n",
    "                                                               multiindices=self.monomial_list_output_denominator,\n",
    "                                                               decimals=decimals)\n",
    "        else:\n",
    "            denominator = 1\n",
    "        formula = numerator / denominator\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Computing the end formula took {(t_1 - t_0):.3f} seconds')\n",
    "\n",
    "        t_0 = time.time()\n",
    "        # try:\n",
    "        #    formula = sympy.simplify(formula)\n",
    "        # except TimeoutError:\n",
    "        #     print(f'Simplifying took too long, so we omit it this time.')\n",
    "        if isinstance(coefficients[0], float):\n",
    "            if verbose:\n",
    "                print(f'Estimated expression (before simplification): {formula}')\n",
    "            timelimit = True\n",
    "            if timelimit:\n",
    "                formula_dict = {'Formula': formula}\n",
    "                queue = multiprocessing.Queue()\n",
    "                queue.put(formula_dict)\n",
    "                p = multiprocessing.Process(target=_symplify, args=(queue,))\n",
    "                p.start()\n",
    "\n",
    "                # Wait for 10 seconds or until process finishes\n",
    "                p.join(0.001)\n",
    "\n",
    "                # If thread is still active\n",
    "                if p.is_alive():\n",
    "                    print(\"running... let's kill it...\")\n",
    "\n",
    "                    # Terminate - may not work if process is stuck for good\n",
    "                    p.terminate()\n",
    "                    # OR Kill - will work for sure, no chance for process to finish nicely however\n",
    "                    # p.kill()\n",
    "\n",
    "                    p.join()\n",
    "                else:\n",
    "                    p.terminate()\n",
    "                    p.join()\n",
    "                    formula = queue.get()['Formula']\n",
    "            else:\n",
    "                formula = sympy.simplify(formula)\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Simplifying the formula took {(t_1 - t_0):.3f} seconds')\n",
    "        return formula\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_symbolic_polynomial(n_input, input_names, degree, coefficients, degrees_specific, multiindices,\n",
    "                                 decimals):\n",
    "        formula = 0\n",
    "        # multiindices = ParFamTorch.list_monomials(n_input, degree, degrees_specific)\n",
    "\n",
    "        for i, multiindex in enumerate(multiindices):\n",
    "            coefficient = coefficients[i]\n",
    "            if isinstance(coefficient, float) and np.abs(coefficient) < 0.000000001:  # 10**(-decimals)\n",
    "                continue\n",
    "            if isinstance(coefficient, torch.Tensor) and torch.abs(coefficient) < 0.000000001:  # 10**(-decimals)\n",
    "                continue\n",
    "            monomial = 1\n",
    "            for j, index in enumerate(multiindex):\n",
    "                monomial *= input_names[j] ** index\n",
    "            if isinstance(coefficient, float):\n",
    "                formula += np.round(coefficient, decimals=3) * monomial\n",
    "            elif isinstance(coefficient, torch.Tensor):\n",
    "                formula += np.round(coefficient.cpu().detach().numpy(), decimals=3) * monomial\n",
    "            else:\n",
    "                formula += coefficient * monomial\n",
    "        return formula\n",
    "\n",
    "    def get_number_parameters(self):\n",
    "        return self.width * len(self.functions) * self.n_coefficients_first_layer + self.n_coefficients_last_layer\n",
    "\n",
    "    def prepare_input_monomials(self, x):\n",
    "        # Convert x to torch tensor if x is a numpy array\n",
    "        x = convert_to_module(x, torch, device=self.device)\n",
    "        self.input_monomials = ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input,\n",
    "                                                                          device=self.device)\n",
    "        if self.degree_input_denominator > 0:\n",
    "            self.input_monomials_denominator = \\\n",
    "                ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input_denominator, device=self.device)\n",
    "        self.output_monomials_dict = self._compute_output_monomial_features_dict(x, self.monomial_list_output)\n",
    "        if self.degree_output_denominator > 0:\n",
    "            self.output_monomials_denominator_dict = \\\n",
    "                self._compute_output_monomial_features_dict(x, self.monomial_list_output_denominator)\n",
    "\n",
    "    def testing_mode(self):\n",
    "        self.input_monomials = None\n",
    "        self.input_monomials_denominator = None\n",
    "        self.output_monomials_dict = None\n",
    "        self.output_monomials_denominator_dict = None\n",
    "\n",
    "    def predict(self, coefficients, x, symbolic=False):\n",
    "        x = convert_to_module(x, torch, device=self.device)\n",
    "        coefficients = convert_to_module(coefficients, torch, device=self.device)\n",
    "        if self.input_monomials is None:\n",
    "            input_monomials = ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input,\n",
    "                                                                         device=self.device)\n",
    "        else:\n",
    "            input_monomials = copy.copy(self.input_monomials)\n",
    "        if self.input_monomials_denominator is None and self.degree_input_denominator > 0:\n",
    "            input_monomials_denominator = ParFamTorch._evaluate_monomial_features_v1(x,\n",
    "                                                                                     self.monomial_list_input_denominator,\n",
    "                                                                                     device=self.device)\n",
    "        else:\n",
    "            input_monomials_denominator = copy.copy(self.input_monomials_denominator)\n",
    "        hidden_layer = torch.zeros((x.shape[0], x.shape[1] + self.width * len(self.functions)), device=self.device)\n",
    "        hidden_layer[:, :x.shape[1]] = x\n",
    "        feature_number = x.shape[1]\n",
    "        for i, function in enumerate(self.width * self.functions):\n",
    "            current_coefficients = coefficients[\n",
    "                                   i * self.n_coefficients_first_layer: i * self.n_coefficients_first_layer\n",
    "                                                                        + self.n_coefficients_first_layer_numerator]\n",
    "            numerator = ParFamTorch._evaluate_polynomial_v2(inputs=x, coefficients=current_coefficients,\n",
    "                                                            multiindices=self.monomial_list_input,\n",
    "                                                            monomial_features=input_monomials, device=self.device)\n",
    "            if self.degree_input_denominator > 0:\n",
    "                current_coefficients = coefficients[\n",
    "                                       i * self.n_coefficients_first_layer + self.n_coefficients_first_layer_numerator:\n",
    "                                       (i + 1) * self.n_coefficients_first_layer]\n",
    "                if self.normalize_denom:\n",
    "                    current_coefficients = current_coefficients / torch.linalg.norm(current_coefficients)\n",
    "                denominator = ParFamTorch._evaluate_polynomial_v2(inputs=x, coefficients=current_coefficients,\n",
    "                                                                  multiindices=self.monomial_list_input_denominator,\n",
    "                                                                  monomial_features=input_monomials_denominator,\n",
    "                                                                  device=self.device)\n",
    "                denominator = self.stabilize_denominator(denominator)\n",
    "            else:\n",
    "                denominator = 1\n",
    "\n",
    "            try:\n",
    "                hidden_layer[:, feature_number + i] = function(numerator / denominator)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[\n",
    "                                   -self.n_coefficients_last_layer:-self.n_coefficients_last_layer_denominator]\n",
    "        else:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer:]\n",
    "        numerator = ParFamTorch._evaluate_polynomial_v2(inputs=hidden_layer, coefficients=current_coefficients,\n",
    "                                                        multiindices=self.monomial_list_output,\n",
    "                                                        monomial_features=None, n_input=self.n_input,\n",
    "                                                        monomial_features_dict=self.output_monomials_dict,\n",
    "                                                        device=self.device)\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer_denominator:]\n",
    "            if self.normalize_denom:\n",
    "                current_coefficients = current_coefficients / torch.norm(current_coefficients)\n",
    "            denominator = ParFamTorch._evaluate_polynomial_v2(inputs=hidden_layer, coefficients=current_coefficients,\n",
    "                                                              multiindices=self.monomial_list_output_denominator,\n",
    "                                                              monomial_features=None, n_input=self.n_input,\n",
    "                                                              monomial_features_dict=self.output_monomials_denominator_dict,\n",
    "                                                              device=self.device)\n",
    "            denominator = self.stabilize_denominator(denominator)\n",
    "        else:\n",
    "            denominator = 1\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _compute_output_monomial_features_dict(self, inputs, multiindices):\n",
    "        monomial_feature_dict = {}\n",
    "        for i, multiindex in enumerate(multiindices):\n",
    "            reduced_multiindex = multiindex[:inputs.shape[1]]\n",
    "            if reduced_multiindex not in monomial_feature_dict.keys():\n",
    "                monomial_feature_dict[reduced_multiindex] = torch.ones(inputs.shape[0], dtype=torch.float64,\n",
    "                                                                       device=self.device)\n",
    "                for j, index in enumerate(reduced_multiindex):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_feature_dict[reduced_multiindex] *= inputs[:, j]\n",
    "                    else:\n",
    "                        monomial_feature_dict[reduced_multiindex] *= inputs[:, j] ** index\n",
    "        return monomial_feature_dict\n",
    "\n",
    "    def get_normalized_coefficients(self, coefficients):\n",
    "        coefficients = convert_to_module(coefficients, torch, device=self.device)\n",
    "        for i, function in enumerate(self.width * self.functions):\n",
    "            if self.degree_input_denominator > 0 and self.normalize_denom:\n",
    "                denominator_indices = slice(\n",
    "                    i * self.n_coefficients_first_layer + self.n_coefficients_first_layer_numerator, (\n",
    "                            i + 1) * self.n_coefficients_first_layer)\n",
    "                coefficients[denominator_indices] /= torch.norm(coefficients[denominator_indices], p=2)\n",
    "\n",
    "        if self.degree_output_denominator > 0 and self.normalize_denom:\n",
    "            denominator_indices = slice(-self.n_coefficients_last_layer_denominator, None)\n",
    "            coefficients[denominator_indices] /= torch.norm(coefficients[denominator_indices], p=2)\n",
    "        return coefficients\n",
    "\n",
    "    def stabilize_denominator(self, denominator):\n",
    "        denominator[torch.abs(denominator) < 10 ** (-5)] = 10 ** (-5)\n",
    "        return denominator\n",
    "\n",
    "    def denominator_reg(self, coefficients):\n",
    "        reg = 0\n",
    "        if self.degree_input_denominator > 0:\n",
    "            for i, function in enumerate(self.width * self.functions):\n",
    "                current_coefficients = coefficients[\n",
    "                                       i * self.n_coefficients_first_layer + self.n_coefficients_first_layer_numerator:\n",
    "                                       (i + 1) * self.n_coefficients_first_layer]\n",
    "                reg += (torch.norm(current_coefficients) - 1) ** 2\n",
    "        if self.degree_input_denominator > 0:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer_denominator:]\n",
    "            reg += (torch.norm(current_coefficients) - 1) ** 2\n",
    "        return reg\n",
    "\n",
    "    def predict_batch(self, coefficients, x, symbolic=False):\n",
    "        batch_size = coefficients.shape[1]\n",
    "        if self.input_monomials is None:\n",
    "            input_monomials = ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input,\n",
    "                                                                         device=self.device)\n",
    "        else:\n",
    "            input_monomials = copy.copy(self.input_monomials)\n",
    "        hidden_layer = np.zeros((x.shape[0], x.shape[1] + self.width * len(self.functions), batch_size))\n",
    "        hidden_layer[:, :x.shape[1]] = np.repeat(x.reshape(*x.shape, 1), batch_size, axis=2)\n",
    "        feature_number = x.shape[1]\n",
    "        for i, function in enumerate(self.width * self.functions):\n",
    "            current_coefficients = coefficients[\n",
    "                                   i * self.n_coefficients_first_layer: (i + 1) * self.n_coefficients_first_layer]\n",
    "            hidden_layer[:, feature_number + i] = function(\n",
    "                ParFamTorch._evaluate_polynomial_v2(inputs=x, coefficients=current_coefficients,\n",
    "                                                    multiindices=self.monomial_list_input,\n",
    "                                                    monomial_features=input_monomials, device=self.device))\n",
    "\n",
    "        current_coefficients = coefficients[-self.n_coefficients_last_layer:]\n",
    "        output = ParFamTorch._evaluate_polynomial_v2(inputs=hidden_layer, coefficients=current_coefficients,\n",
    "                                                     multiindices=self.monomial_list_output,\n",
    "                                                     monomial_features=None, batch_size=batch_size, device=self.device)\n",
    "        return output\n",
    "\n",
    "    def get_mixed_reg(self, n_features, coefficients):\n",
    "        penalty = 0\n",
    "        for feature in range(n_features + self.width * len(self.functions)):\n",
    "            active_feature_coefficients = []\n",
    "            if feature < n_features:\n",
    "                continue\n",
    "            for i, index in enumerate(self.monomial_list_output):\n",
    "                active_feature_coefficients.append(index[feature] > 0)\n",
    "            if sum(np.abs(coefficients[-self.n_coefficients_last_layer:][active_feature_coefficients]) > 0.01) > 1:\n",
    "                penalty += torch.norm(\n",
    "                    coefficients[-self.n_coefficients_last_layer:][active_feature_coefficients],\n",
    "                    p=1)\n",
    "        return penalty\n",
    "\n",
    "    def get_random_coefficients_unique(self, n_functions_max=3):\n",
    "        \"\"\"\n",
    "        Get random coefficients, which relate to meaningful functions.\n",
    "        :param n_functions_max: Number of features to be used, i.e., at most n_functions_max output coefficients are\n",
    "                                non-zero\n",
    "        :return: numpy array, which gives the coefficients\n",
    "        \"\"\"\n",
    "        input_coefficients = np.zeros(self.n_coefficients_first_layer * self.width * len(self.functions))\n",
    "        output_coefficients = np.zeros(self.n_coefficients_last_layer)\n",
    "\n",
    "        monomial_list_output_lists = [list(multi_index) for multi_index in self.monomial_list_output]\n",
    "\n",
    "        # Get the active output monomials\n",
    "        n_functions = 1 + np.random.randint(n_functions_max)\n",
    "        indices = np.random.choice(len(monomial_list_output_lists), size=n_functions,\n",
    "                                   replace=False)\n",
    "        active_output_monomials = []\n",
    "        for index in indices:\n",
    "            active_output_monomials.append(monomial_list_output_lists[index])\n",
    "        # Check if there is any input feature which is used with more than one different degree, e.g.,\n",
    "        # sin(P_1(x)) and sin(P_1(x))^2. Drop them if yes.\n",
    "        dict_active_output_monomials = {}\n",
    "        active_output_monomials_to_remove = []\n",
    "        for multi_index in active_output_monomials:\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                if index in dict_active_output_monomials.keys():\n",
    "                    if dict_active_output_monomials[index] != multiplicity:\n",
    "                        # remove the coefficient if this relates to an already used feature, but with a different degree\n",
    "                        # this time\n",
    "                        active_output_monomials_to_remove.append(multi_index)\n",
    "                        break\n",
    "                else:\n",
    "                    dict_active_output_monomials[index] = multiplicity\n",
    "        for monomial_to_remove in active_output_monomials_to_remove:\n",
    "            active_output_monomials.remove(monomial_to_remove)\n",
    "\n",
    "        # Recreate the dict_active_output_monomials to ensure that the removal did not change it\n",
    "        dict_active_output_monomials = {}\n",
    "        active_output_monomials_to_remove = []\n",
    "        for multi_index in active_output_monomials:\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                dict_active_output_monomials[index] = multiplicity\n",
    "\n",
    "        # Ensure the uniqueness of the formula through the following steps:\n",
    "        # 1. If only one function is used, cosine is used ==> So if (0,0,1) is the only active coefficient, than it will\n",
    "        # be substituted by (0,1,0)\n",
    "        active_output_monomials_to_remove = []\n",
    "        active_output_monomials_to_add = []\n",
    "        if not 1 in dict_active_output_monomials.keys() and 2 in dict_active_output_monomials.keys():\n",
    "            for multi_index in active_output_monomials:\n",
    "                if multi_index[2] != 0:\n",
    "                    # active_output_monomials_to_remove.append(multi_index)\n",
    "                    multi_index[1] = multi_index[2]\n",
    "                    multi_index[2] = 0\n",
    "        # 2. If both functions are used, but with different degrees, ensure that cosine has a lower degree than sine\n",
    "        # ==> So if (0,2,0) and (0,0,1) are active they will both be substituted by (0,0,2) and (0,1,0)\n",
    "        if len(dict_active_output_monomials) == 2:\n",
    "            if dict_active_output_monomials[1] > dict_active_output_monomials[2]:\n",
    "                # swap the multiplicities\n",
    "                dict_active_output_monomials[1], dict_active_output_monomials[2] = dict_active_output_monomials[2], \\\n",
    "                    dict_active_output_monomials[1]\n",
    "                for multi_index in active_output_monomials:\n",
    "                    if multi_index[1] != 0:\n",
    "                        multi_index[1] = dict_active_output_monomials[1]\n",
    "                    if multi_index[2] != 0:\n",
    "                        multi_index[2] = dict_active_output_monomials[2]\n",
    "                        if sum(multi_index) > self.degree_output_polynomials:\n",
    "                            multi_index[2] = dict_active_output_monomials[1]\n",
    "        # 3. If both functions are used with the same degree, make sure that\n",
    "        # a) if there is only 1 active monomial including either cos or sin, we are done.\n",
    "        # b) if there are 2 active monomials including either cos or sin, make sure that cos is the function used in the\n",
    "        # monomial with lower overall degree ==> i.e., (2,2,0) and (1,0,2) will be substituted by (1,2,0) and (2,0,2).\n",
    "        # If this is also the same, e.g., for (0,2,2) and (2,0,2), then the coefficients are chosen such that cosine\n",
    "        # is active in the one without sine, so they will be substituted by (0,2,2) and (2,2,0).\n",
    "        # c)\n",
    "        if len(dict_active_output_monomials) == 2 and dict_active_output_monomials[1] == dict_active_output_monomials[\n",
    "            2]:\n",
    "            degree = dict_active_output_monomials[1]\n",
    "            active_output_monomials_cos_sin = []\n",
    "            for multi_index in active_output_monomials:\n",
    "                if multi_index[1] != 0 or multi_index[2] != 0:\n",
    "                    active_output_monomials_cos_sin.append(multi_index)\n",
    "\n",
    "            # Case b)\n",
    "            if len(active_output_monomials_cos_sin) == 2:\n",
    "                active_output_monomials_cos = []\n",
    "                active_output_monomials_sin = []\n",
    "                for multi_index in active_output_monomials_cos_sin:\n",
    "                    if multi_index[1] != 0:\n",
    "                        active_output_monomials_cos.append(multi_index)\n",
    "                    if multi_index[2] != 0:\n",
    "                        active_output_monomials_sin.append(multi_index)\n",
    "                if len(active_output_monomials_cos) == 1:\n",
    "                    # When cos is active all the time it is not a problem\n",
    "                    if len(active_output_monomials_sin) == 2:\n",
    "                        # Case b) and sine is more often used than cosine ==> swap\n",
    "                        for multi_index in active_output_monomials_sin:\n",
    "                            if multi_index[1] == 0:\n",
    "                                multi_index[1] = degree\n",
    "                                multi_index[2] = 0\n",
    "                    else:\n",
    "                        # both are only used once\n",
    "                        if sum(active_output_monomials_cos[0]) > sum(active_output_monomials_sin[0]):\n",
    "                            # Cosines multi index has a higher overall degree ==> swap\n",
    "                            active_output_monomials_cos[0][1] = 0\n",
    "                            active_output_monomials_cos[0][2] = degree\n",
    "                            active_output_monomials_sin[0][1] = degree\n",
    "                            active_output_monomials_sin[0][2] = 0\n",
    "\n",
    "            if len(active_output_monomials_cos_sin) == 3:\n",
    "                # TODO\n",
    "                pass\n",
    "\n",
    "        active_output_monomials_indices = []\n",
    "        for output_polynomial in active_output_monomials:\n",
    "            index = self.monomial_list_output.index(tuple(output_polynomial))\n",
    "            active_output_monomials_indices.append(index)\n",
    "            output_coefficients[index] = 1\n",
    "\n",
    "        # Get the partitions in the input layer which is related to the active functions\n",
    "        active_input_partitions = set()\n",
    "        for i, multi_index in enumerate(active_output_monomials):\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                active_input_partitions.add(index - self.n_input)\n",
    "\n",
    "        # Select randomly the active coefficients in the partitions\n",
    "        for active_input_partition in active_input_partitions:\n",
    "            n_coeffs = 1 + np.random.randint(self.n_coefficients_first_layer)\n",
    "            if n_coeffs == 1:\n",
    "                # make sure that not only the coefficient for the constant term is chosen\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer - 1), size=n_coeffs) + 1\n",
    "            else:\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer), size=n_coeffs,\n",
    "                                                 replace=False)\n",
    "            for chosen_coeff in chosen_coeffs:\n",
    "                input_coefficients[active_input_partition * self.n_coefficients_first_layer + chosen_coeff] = 1\n",
    "        return np.concatenate([input_coefficients, output_coefficients])\n",
    "\n",
    "    def get_random_coefficients(self, n_functions_max=3):\n",
    "        \"\"\"\n",
    "        Get random coefficients, which relate to meaningful functions.\n",
    "        :param n_functions_max: Number of features to be used, i.e., at most n_functions_max output coefficients are\n",
    "                                non-zero\n",
    "        :return: numpy array, which gives the coefficients\n",
    "        \"\"\"\n",
    "        input_coefficients = np.zeros(self.n_coefficients_first_layer * self.width * len(self.functions))\n",
    "        output_coefficients = np.zeros(self.n_coefficients_last_layer)\n",
    "\n",
    "        # Get the active output coefficients\n",
    "        n_functions = 1 + np.random.randint(n_functions_max)\n",
    "        active_output_monomials_indices = np.random.choice(range(len(self.monomial_list_output)), size=n_functions,\n",
    "                                                           replace=False)\n",
    "        for output_index in active_output_monomials_indices:\n",
    "            output_coefficients[output_index] = 1\n",
    "\n",
    "        # Get the related input coefficients\n",
    "        active_output_monomials = [self.monomial_list_output[index] for index in active_output_monomials_indices]\n",
    "        # Get the partitions in the input layer which is related to the active functions\n",
    "        active_input_partitions = set()\n",
    "        dict_active_output_monomials = {}\n",
    "        for i, multi_index in enumerate(active_output_monomials):\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                # Check if there is any input feature which is used with more than one different degree, e.g.,\n",
    "                # sin(P_1(x)) and sin(P_1(x))^2\n",
    "                if index in dict_active_output_monomials.keys():\n",
    "                    if dict_active_output_monomials[index] == multiplicity:\n",
    "                        active_input_partitions.add(index - self.n_input)\n",
    "                    else:\n",
    "                        # remove the coefficient if this relates to an already used feature, but with a different degree\n",
    "                        # this time\n",
    "                        output_coefficients[active_output_monomials_indices[i]] = 0\n",
    "                else:\n",
    "                    dict_active_output_monomials[index] = multiplicity\n",
    "                    active_input_partitions.add(index - self.n_input)\n",
    "\n",
    "        # Select randomly the active coefficients in the partitions\n",
    "        for active_input_partition in active_input_partitions:\n",
    "            n_coeffs = 1 + np.random.randint(self.n_coefficients_first_layer)\n",
    "            if n_coeffs == 1:\n",
    "                # make sure that not only the coefficient for the constant term is chosen\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer - 1), size=n_coeffs) + 1\n",
    "            else:\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer), size=n_coeffs,\n",
    "                                                 replace=False)\n",
    "            for chosen_coeff in chosen_coeffs:\n",
    "                input_coefficients[active_input_partition * self.n_coefficients_first_layer + chosen_coeff] = 1\n",
    "        return np.concatenate([input_coefficients, output_coefficients])\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_polynomial_v2(inputs, coefficients, multiindices, device, monomial_features=None, batch_size=1,\n",
    "                                monomial_features_dict=None, n_input=None):\n",
    "        if monomial_features is None:\n",
    "            monomial_features = ParFamTorch._evaluate_monomial_features_v1(inputs, multiindices, batchsize=batch_size,\n",
    "                                                                           monomial_features_dict=monomial_features_dict,\n",
    "                                                                           n_input=n_input, device=device)\n",
    "        if batch_size == 1:\n",
    "            return torch.matmul(monomial_features, coefficients)  # No batches, simple matrix-vector product\n",
    "        else:\n",
    "            # Batches, so we use the matrix-vector product over the left-most indices\n",
    "            return torch.einsum('ij...,j...->i...', monomial_features, coefficients)\n",
    "\n",
    "    @staticmethod\n",
    "    def list_monomials_uniform_degree(n_input, degree, device):\n",
    "        multi_indices = []\n",
    "        indices = torch.arange(degree + 1, device=device)\n",
    "        repeat_indices = [indices for _ in range(n_input)]\n",
    "        for i in product(*repeat_indices):\n",
    "            if sum(i) <= degree:\n",
    "                multi_indices += [i]\n",
    "        return multi_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def list_monomials(n_input, degree, degrees_specific):\n",
    "        multi_indices = []\n",
    "        repeat_indices = [range(degree_specific + 1) for degree_specific in degrees_specific]\n",
    "        for i in product(*repeat_indices):\n",
    "            if sum(i) <= degree:\n",
    "                multi_indices += [i]\n",
    "        return multi_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_monomial_features_v1(inputs, multiindices, device, batchsize=1, monomial_features_dict=None,\n",
    "                                       n_input=None):\n",
    "        if batchsize == 1:\n",
    "            monomial_features = torch.ones((inputs.shape[0], len(multiindices)), dtype=torch.float64, device=device)\n",
    "        else:\n",
    "            monomial_features = torch.ones((inputs.shape[0], len(multiindices), batchsize), dtype=torch.float64,\n",
    "                                           device=device)\n",
    "        if monomial_features_dict is None:\n",
    "            for i, multiindex in enumerate(multiindices):\n",
    "                for j, index in enumerate(multiindex):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_features[:, i] *= inputs[:, j]\n",
    "                    else:\n",
    "                        monomial_features[:, i] *= inputs[:, j] ** index\n",
    "        else:\n",
    "            for i, multiindex in enumerate(multiindices):\n",
    "                reduced_multiindex_input = multiindex[:n_input]\n",
    "                reduced_multiindex_func = multiindex[n_input:]\n",
    "                monomial_features[:, i] = monomial_features_dict[reduced_multiindex_input]\n",
    "                for j, index in enumerate(reduced_multiindex_func):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_features[:, i] *= inputs[:, j + n_input]\n",
    "                    else:\n",
    "                        monomial_features[:, i] *= inputs[:, j + n_input] ** index\n",
    "        return monomial_features\n",
    "\n",
    "    @staticmethod\n",
    "    def get_monomial_mask(n_input, degree):\n",
    "        half_degree = int(torch.ceil(degree / 2))\n",
    "        inputs = torch.ones((1, n_input), device=device) * 2\n",
    "        y = inputs.reshape(*inputs.shape, 1) ** (torch.arange(half_degree) + 1).reshape(1, 1, half_degree)\n",
    "        y_flattened = y.reshape(inputs.shape[0], half_degree * n_input)\n",
    "        y_flattened_with_1 = torch.concatenate((torch.ones((y.shape[0], 1), device=device), y_flattened), axis=1)\n",
    "        z = torch.expand_dims(y_flattened_with_1, axis=1) * torch.expand_dims(y_flattened_with_1, axis=2)\n",
    "        z = torch.squeeze(z, axis=0)\n",
    "        return (0 < z.triu()) & (z.triu() <= 2 ** degree)\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, x, y, model, lambda_0, lambda_1, n_params, lambda_mixed=0, lambda_denom=0,\n",
    "                 n_best_coefficients=10, mask=None, lambda_05=None, lambda_1_cut=None, lambda_1_piecewise=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.lambda_0 = lambda_0  # regularization factor for p=0 regularization (cutoff value of 0.01)\n",
    "        self.lambda_1 = lambda_1  # regularization factor for p=1 regularization\n",
    "        self.lambda_1_piecewise = lambda_1_piecewise  # regularization factor for piecewise p=1 regularization: the\n",
    "        # smaller coefficient, the higher the regularization parameter\n",
    "        self.lambda_1_cut = lambda_1_cut  # regularization factor for f(x)=min(|x|, \\sqrt(x))\n",
    "        self.lambda_05 = lambda_05  # regularization factor for p=0.5 regularization\n",
    "        self.lambda_mixed = lambda_mixed  # regularization factor for functions which are used more than once; default\n",
    "        self.lambda_denom = lambda_denom  # regularization factor for the coefficients of the denominator, to keep their\n",
    "        # norm close to 1, as otherwise the values of all parameters can be pressed to 0 due to overparametrization\n",
    "        self.loss_list = []\n",
    "        self.l2_dist_list = []\n",
    "        self.reg_list = []\n",
    "        self.n_active_parameters_list = []\n",
    "        self.best_losses = np.full((n_best_coefficients,), float('inf'))\n",
    "        self.mask = mask\n",
    "        self.n_params = n_params\n",
    "        self.device = self.model.device\n",
    "\n",
    "        self.coefficients_current = None\n",
    "        self.loss_current = None\n",
    "        if self.mask == None:\n",
    "            self.best_coefficients = np.inf * np.ones((n_best_coefficients, n_params))\n",
    "        else:\n",
    "            self.n_active_coefficients = sum(mask)\n",
    "            self.best_coefficients = np.inf * np.ones((n_best_coefficients, self.n_active_coefficients))\n",
    "        self.evaluations = 0\n",
    "\n",
    "    def convert_evaluator_variables_to_module(self, module):\n",
    "        self.x = convert_to_module(self.x, module, device=self.device)\n",
    "        self.y = convert_to_module(self.y, module, device=self.device)\n",
    "        self.best_coefficients = convert_to_module(self.best_coefficients, module, device=self.device)\n",
    "        self.best_losses = convert_to_module(self.best_losses, module, device=self.device)\n",
    "\n",
    "    def loss_func_torch(self):\n",
    "        # convert array to tensors?\n",
    "        self.convert_evaluator_variables_to_module(torch)\n",
    "        self.evaluations += 1\n",
    "        if self.mask is None:\n",
    "            y_pred = self.model.predict(self.coefficients_current, self.x)\n",
    "        else:\n",
    "            coefficients_extended = torch.zeros(self.n_params, device=self.device, dtype=torch.double)\n",
    "            coefficients_extended[self.mask] = self.coefficients_current\n",
    "            y_pred = self.model.predict(coefficients_extended, self.x)\n",
    "        if y_pred.shape != self.y.shape:\n",
    "            print(f'Careful, there is a shape mismatch in the loss function: '\n",
    "                  f'y_pred.shape {y_pred.shape} != y.shape {self.y.shape}')\n",
    "        # Use torch.norm since torch.linalg.norm does not support tensors with requires grad\n",
    "        rel_l2_dist = torch.norm(y_pred - self.y, p=2) / torch.norm(self.y, p=2)\n",
    "        n_active_parameters = torch.sum(torch.abs(self.coefficients_current) > 0.01)\n",
    "        reg_0 = self.lambda_0 * n_active_parameters\n",
    "        reg_1 = self.lambda_1 * torch.norm(self.coefficients_current, p=1)\n",
    "        reg_1_piecewise = 0\n",
    "        if self.lambda_1_piecewise:\n",
    "            reg_1_piecewise += torch.norm(\n",
    "                self.coefficients_current[self.coefficients_current < 0.1]) * self.lambda_1_piecewise\n",
    "            reg_1_piecewise += torch.norm(self.coefficients_current[(0.1 < self.coefficients_current) & (\n",
    "                    self.coefficients_current < 1)]) * self.lambda_1_piecewise * 0.1 + self.lambda_1_piecewise * 0.1\n",
    "            reg_1_piecewise += torch.norm(\n",
    "                self.coefficients_current[self.coefficients_current > 1]) * self.lambda_1_piecewise * 0.01 + \\\n",
    "                               self.lambda_1_piecewise * 0.1 + self.lambda_1_piecewise * 0.9 * 0.1\n",
    "\n",
    "        if self.lambda_05:\n",
    "            reg_05 = self.lambda_05 * torch.norm(self.coefficients_current, p=1)\n",
    "        else:\n",
    "            reg_05 = 0\n",
    "        if self.lambda_1_cut:\n",
    "            reg_1_cut = self.lambda_1_cut * torch.sum(\n",
    "                torch.minimum(torch.abs(self.coefficients_current), torch.sqrt(torch.abs(self.coefficients_current))))\n",
    "        else:\n",
    "            reg_1_cut = 0\n",
    "\n",
    "        # Penalizing it when one term is used more than once seemed like a good idea, but apparently it rather harms the\n",
    "        # problem at hand\n",
    "        if self.lambda_mixed > 0:\n",
    "            reg_2 = self.model.get_mixed_reg(n_features=self.x.shape[1], coefficients=self.coefficients_current)\n",
    "        else:\n",
    "            reg_2 = 0  # Default\n",
    "        if self.lambda_denom:\n",
    "            reg_3 = self.lambda_denom * self.model.denominator_reg(self.coefficients_current)\n",
    "        else:\n",
    "            reg_3 = 0\n",
    "        self.loss_current = rel_l2_dist + reg_0 + reg_1 + reg_2 + reg_3 + reg_05 + reg_1_cut + reg_1_piecewise\n",
    "        self.l2_dist_list.append(rel_l2_dist)\n",
    "        self.reg_list.append(reg_0 + reg_1)\n",
    "        self.loss_list.append(self.loss_current)\n",
    "        self.n_active_parameters_list.append(n_active_parameters)\n",
    "\n",
    "        if self.loss_current < self.best_losses[-1]:\n",
    "            self.best_losses[-1] = self.loss_current\n",
    "            self.best_coefficients[-1] = self.coefficients_current\n",
    "            order = torch.argsort(self.best_losses)\n",
    "            self.best_losses = self.best_losses[order]\n",
    "            self.best_coefficients = self.best_coefficients[order]\n",
    "        return 0\n",
    "\n",
    "    def gradient(self, _):\n",
    "        # The if-query should be unnecessary. Test later for specific solver....\n",
    "        # if np.linalg.norm(coefficients - self.coefficients_current.detach().numpy()) > 1e-3:\n",
    "        #     coefficients = convert_to_module(coefficients, torch)\n",
    "        #     coefficients.requires_grad_()\n",
    "        #     self.loss_current = self.loss_func_torch(coefficients)\n",
    "        self.loss_current.backward()\n",
    "        return convert_to_module(self.coefficients_current.grad, np, device=self.device)\n",
    "\n",
    "    def loss_func(self, coefficients):\n",
    "        self.coefficients_current = convert_to_module(coefficients, torch, device=self.device)\n",
    "        self.coefficients_current.requires_grad_()\n",
    "        self.loss_func_torch()\n",
    "        return convert_to_module(self.loss_current, np, device=self.device)\n",
    "\n",
    "    # def loss_func(self, coefficients):\n",
    "    #     self.coefficients_current = convert_to_module(coefficients, torch)\n",
    "    #     self.coefficients_current.requires_grad_()\n",
    "    #     self.loss_func_torch()\n",
    "    #     self.loss_current.backward()\n",
    "    #     gradient = convert_to_module(self.coefficients_current.grad, np)\n",
    "    #     return convert_to_module(self.loss_current, np), gradient\n",
    "\n",
    "    def plot_training_statistics(self, width, height):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(width, height))\n",
    "        axs[0, 0].plot(self.loss_list)\n",
    "        axs[0, 0].set_yscale('log')\n",
    "        axs[0, 0].set_title(f'Loss')\n",
    "\n",
    "        axs[1, 0].plot(self.l2_dist_list)\n",
    "        axs[1, 0].set_yscale('log')\n",
    "        axs[1, 0].set_title(f'Training l2 distance')\n",
    "\n",
    "        axs[0, 1].plot(self.reg_list)\n",
    "        # axs[0, 1].set_yscale('log')\n",
    "        axs[0, 1].set_title(f'L0 + L1 Regularization')\n",
    "\n",
    "        axs[1, 1].plot(self.n_active_parameters_list)\n",
    "        axs[1, 1].set_ylim([0, np.max(self.n_active_parameters_list) + 1])\n",
    "        # axs[1, 1].set_yscale('log')\n",
    "        axs[1, 1].set_title(f'Number active terms')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def fit_lbfgs(self, coefficients, verbose):\n",
    "        t_0 = time.time()\n",
    "        coefficients.requires_grad = True\n",
    "        optimizer = torch.optim.LBFGS([coefficients], line_search_fn='strong_wolfe')\n",
    "        for i in range(50):\n",
    "            def closure():\n",
    "                if torch.is_grad_enabled():\n",
    "                    optimizer.zero_grad()\n",
    "                loss = self.loss_func_torch(coefficients)\n",
    "                if loss.requires_grad:\n",
    "                    loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "        # ret = basinhopping(evaluator.gradient, niter=50, x0=x0, minimizer_kwargs={'jac': True})\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Training time: {t_1 - t_0}')\n",
    "        self.model.testing_mode()\n",
    "        if verbose:\n",
    "            print(f'Coefficients: {coefficients}')\n",
    "        relative_l2_distance = np.linalg.norm(\n",
    "            self.y - self.model.predict(torch.tensor(coefficients, device=self.device), self.x).cpu().detach().numpy(),\n",
    "            ord=2) / np.linalg.norm(self.y, ord=2)\n",
    "        if verbose:\n",
    "            print(f'Relative l2 distance: {relative_l2_distance}')\n",
    "        return coefficients, relative_l2_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "507bd4cc-3c8e-48b3-b5b0-ad388b435790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Target formula: sin((7.13565254211426*x + 1)/(0.1*x + 2))\n",
      "Number parameters: 16\n",
      "Running time  7.257528305053711\n"
     ]
    }
   ],
   "source": [
    "# Input parameters\n",
    "device = 'cuda'\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "print(f'Using {device}')\n",
    "a = 5 * torch.randn(1)\n",
    "x = np.arange(1, 10, 0.05)\n",
    "x = x.reshape(len(x), 1)\n",
    "x = torch.tensor(x).to(device)\n",
    "\n",
    "test_model = False\n",
    "\n",
    "def func(a, x, module):\n",
    "    # Good approximations with both, however, never yields a simple formula\n",
    "    return module.sin((a[0] * x + 1) / (0.1 * x + 2))\n",
    "\n",
    "y = func(a, x, torch).squeeze(-1)\n",
    "print(f'Target formula: {func(a, sympy.Symbol(\"x\"), sympy)}')\n",
    "\n",
    "functions = [torch.sin]\n",
    "function_names = [sympy.sin]\n",
    "\n",
    "model = ParFamTorch(n_input=1, degree_input_polynomials=2, degree_output_polynomials=2, width=1,\n",
    "                    functions=functions, function_names=function_names, maximal_potence=2,\n",
    "                    degree_output_polynomials_specific=[1], enforce_function=False,\n",
    "                    degree_input_denominator=2, degree_output_denominator=2, normalize_denom=True,\n",
    "                    degree_output_polynomials_denominator_specific=[1], device=device)\n",
    "\n",
    "n_params = model.get_number_parameters()\n",
    "print(f'Number parameters: {n_params}')\n",
    "evaluator = Evaluator(x, y, model=model, lambda_0=0, lambda_1=0.001, lambda_denom=0, n_params=n_params)\n",
    "model.prepare_input_monomials(x)\n",
    "\n",
    "lw = [-10] * n_params\n",
    "up = [10] * n_params\n",
    "t_0 = time.time()\n",
    "x0 = np.random.randn(n_params)\n",
    "ret = basinhopping(evaluator.loss_func, niter=2, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})\n",
    "t_1 = time.time()\n",
    "print(\"Running time \", t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0c33e069-b498-41ad-a0a7-588cd5e65b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         336137 function calls in 6.642 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     1131    3.345    0.003    3.345    0.003 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
       "     2328    0.708    0.000    0.771    0.000 3236217983.py:615(_evaluate_monomial_features_v1)\n",
       "     1164    0.405    0.000    2.487    0.002 3236217983.py:698(loss_func_torch)\n",
       "     1164    0.286    0.000    1.837    0.002 3236217983.py:236(predict)\n",
       "     2328    0.148    0.000    0.206    0.000 3236217983.py:332(stabilize_denominator)\n",
       "     3493    0.137    0.000    0.137    0.000 {built-in method torch.tensor}\n",
       "     4656    0.127    0.000    0.127    0.000 {built-in method torch.matmul}\n",
       "     2295    0.110    0.000    0.110    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
       "     3492    0.099    0.000    0.099    0.000 {built-in method torch.norm}\n",
       "        3    0.098    0.033    6.642    2.214 _optimize.py:1318(_minimize_bfgs)\n",
       "     3492    0.083    0.000    0.083    0.000 {built-in method torch.abs}\n",
       "     1164    0.077    0.000    0.077    0.000 {built-in method torch.frobenius_norm}\n",
       "     2328    0.062    0.000    0.062    0.000 {built-in method torch.ones}\n",
       "     4656    0.053    0.000    0.245    0.000 functional.py:1345(norm)\n",
       "     2328    0.052    0.000    0.104    0.000 _tensor.py:207(_reduce_ex_internal)\n",
       "     4552    0.047    0.000    0.047    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "     1164    0.046    0.000    0.046    0.000 {built-in method torch.sum}\n",
       "     1164    0.045    0.000    0.045    0.000 {built-in method torch.zeros}\n",
       "     1164    0.044    0.000    0.044    0.000 {built-in method torch._C._linalg.linalg_norm}\n",
       "    13974    0.041    0.000    0.158    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "     1131    0.035    0.000    0.035    0.000 {built-in method torch.ones_like}\n",
       "     1161    0.028    0.000    3.582    0.003 _linesearch.py:77(derphi)\n",
       "    10443    0.026    0.000    0.254    0.000 utils.py:14(convert_to_module)\n",
       "      718    0.023    0.000    6.227    0.009 _linesearch.py:91(scalar_search_wolfe1)\n",
       "     2401    0.022    0.000    0.061    0.000 numeric.py:2407(array_equal)\n",
       "     1164    0.022    0.000    0.022    0.000 {built-in method torch.sin}\n",
       "     2328    0.019    0.000    0.019    0.000 {method 'set_' of 'torch._C._TensorBase' objects}\n",
       "     2295    0.018    0.000    0.018    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n",
       "     1161    0.018    0.000    2.618    0.002 _linesearch.py:73(phi)\n",
       "     2328    0.017    0.000    0.227    0.000 copy.py:66(copy)\n",
       "     2148    0.016    0.000    0.060    0.000 _optimize.py:235(vecnorm)\n",
       "     1164    0.015    0.000    2.681    0.002 _differentiable_functions.py:132(fun_wrapped)\n",
       "     1131    0.014    0.000    3.416    0.003 _tensor.py:340(backward)\n",
       "    39702    0.013    0.000    0.016    0.000 {built-in method builtins.isinstance}\n",
       "     4656    0.013    0.000    0.911    0.000 3236217983.py:583(_evaluate_polynomial_v2)\n",
       "      165    0.013    0.000    0.013    0.000 {built-in method torch.argsort}\n",
       "     1164    0.013    0.000    2.648    0.002 3236217983.py:769(loss_func)\n",
       "     2328    0.013    0.000    0.030    0.000 _tensor.py:196(storage)\n",
       "     1131    0.011    0.000    3.571    0.003 _differentiable_functions.py:162(grad_wrapped)\n",
       "     4656    0.010    0.000    0.012    0.000 storage.py:366(__init__)\n",
       "     1131    0.010    0.000    3.539    0.003 3236217983.py:760(gradient)\n",
       "     2328    0.009    0.000    0.083    0.000 _utils.py:131(_rebuild_tensor)\n",
       "     1131    0.009    0.000    3.402    0.003 __init__.py:85(backward)\n",
       "     1131    0.009    0.000    0.010    0.000 _tensor.py:1071(grad)\n",
       "     2298    0.009    0.000    0.012    0.000 shape_base.py:23(atleast_1d)\n",
       "     1131    0.009    0.000    0.046    0.000 __init__.py:30(_make_grads)\n",
       "     2295    0.009    0.000    0.009    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
       "     9315    0.008    0.000    0.008    0.000 {built-in method builtins.getattr}\n",
       "     4656    0.008    0.000    0.010    0.000 storage.py:299(__new__)\n",
       "     2151    0.008    0.000    0.029    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "     2328    0.007    0.000    0.089    0.000 _utils.py:137(_rebuild_tensor_v2)\n",
       "     4808    0.007    0.000    0.033    0.000 <__array_function__ internals>:177(dot)\n",
       "     2309    0.006    0.000    0.006    0.000 {built-in method numpy.array}\n",
       "      718    0.006    0.000    6.235    0.009 _linesearch.py:31(line_search_wolfe1)\n",
       "     1217    0.006    0.000    2.730    0.002 _differentiable_functions.py:264(fun)\n",
       "      718    0.006    0.000    6.453    0.009 _optimize.py:1144(_line_search_wolfe12)\n",
       "     4656    0.005    0.000    0.007    0.000 _VF.py:25(__getattr__)\n",
       "     2401    0.005    0.000    0.037    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "     9246    0.005    0.000    0.005    0.000 {built-in method torch._C._has_torch_function_unary}\n",
       "     2298    0.005    0.000    0.022    0.000 <__array_function__ internals>:177(atleast_1d)\n",
       "     1184    0.005    0.000    3.614    0.003 _differentiable_functions.py:270(grad)\n",
       "     1164    0.004    0.000    0.009    0.000 3236217983.py:692(convert_evaluator_variables_to_module)\n",
       "     2401    0.004    0.000    0.070    0.000 <__array_function__ internals>:177(array_equal)\n",
       "     1161    0.004    0.000    0.015    0.000 _differentiable_functions.py:240(update_x)\n",
       "    11586    0.004    0.000    0.004    0.000 {built-in method builtins.len}\n",
       "     1430    0.004    0.000    0.017    0.000 fromnumeric.py:2188(sum)\n",
       "     2308    0.004    0.000    0.016    0.000 <__array_function__ internals>:177(copy)\n",
       "     2328    0.004    0.000    0.093    0.000 copy.py:258(_reconstruct)\n",
       "     2328    0.003    0.000    0.108    0.000 _tensor.py:175(__reduce_ex__)\n",
       "     1164    0.003    0.000    0.003    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "     1164    0.003    0.000    0.009    0.000 numeric.py:1878(isscalar)\n",
       "     2328    0.003    0.000    0.003    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "      718    0.003    0.000    0.019    0.000 fromnumeric.py:2703(amax)\n",
       "     8419    0.003    0.000    0.003    0.000 {built-in method numpy.asarray}\n",
       "     1187    0.003    0.000    3.577    0.003 _differentiable_functions.py:254(_update_grad)\n",
       "     2401    0.003    0.000    0.032    0.000 _methods.py:61(_all)\n",
       "     4656    0.003    0.000    0.003    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "     1131    0.003    0.000    3.574    0.003 _differentiable_functions.py:166(update_grad)\n",
       "     8089    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}\n",
       "     3492    0.003    0.000    0.003    0.000 functional.py:1450(<listcomp>)\n",
       "     1430    0.003    0.000    0.022    0.000 <__array_function__ internals>:177(sum)\n",
       "     1220    0.003    0.000    2.685    0.002 _differentiable_functions.py:249(_update_fun)\n",
       "     2151    0.002    0.000    0.002    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "     1439    0.002    0.000    0.002    0.000 {built-in method numpy.zeros}\n",
       "     2328    0.002    0.000    0.004    0.000 _namedtensor_internals.py:10(check_serializing_named_tensor)\n",
       "     1164    0.002    0.000    0.002    0.000 {method 'requires_grad_' of 'torch._C._TensorBase' objects}\n",
       "     2295    0.002    0.000    0.003    0.000 __init__.py:281(is_tensor)\n",
       "       40    0.002    0.000    0.004    0.000 _linesearch.py:462(_cubicmin)\n",
       "     6984    0.002    0.000    0.002    0.000 storage.py:437(_untyped)\n",
       "     1164    0.002    0.000    2.683    0.002 _differentiable_functions.py:154(update_fun)\n",
       "     4656    0.002    0.000    0.002    0.000 {built-in method __new__ of type object at 0x55b9e68d4620}\n",
       "     1166    0.002    0.000    0.002    0.000 {built-in method _abc._abc_instancecheck}\n",
       "     2328    0.002    0.000    0.002    0.000 hooks.py:54(warn_if_has_hooks)\n",
       "     2328    0.002    0.000    0.002    0.000 {method '_storage' of 'torch._C._TensorBase' objects}\n",
       "     2308    0.002    0.000    0.008    0.000 function_base.py:871(copy)\n",
       "     2328    0.002    0.000    0.002    0.000 {method 'stride' of 'torch._C._TensorBase' objects}\n",
       "      718    0.002    0.000    0.023    0.000 <__array_function__ internals>:177(amax)\n",
       "     4808    0.002    0.000    0.002    0.000 multiarray.py:740(dot)\n",
       "     4656    0.001    0.000    0.001    0.000 {method 'get' of 'dict' objects}\n",
       "     2328    0.001    0.000    0.001    0.000 {method 'data_ptr' of 'torch._C._TensorBase' objects}\n",
       "     2328    0.001    0.000    0.001    0.000 {method 'has_names' of 'torch._C._TensorBase' objects}\n",
       "     2328    0.001    0.000    0.001    0.000 {method 'storage_offset' of 'torch._C._TensorBase' objects}\n",
       "     1164    0.001    0.000    0.001    0.000 {method 'item' of 'numpy.ndarray' objects}\n",
       "     1131    0.001    0.000    0.001    0.000 __init__.py:77(_tensor_or_tensors_to_tuple)\n",
       "     1166    0.001    0.000    0.003    0.000 abc.py:96(__instancecheck__)\n",
       "      733    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "     2401    0.001    0.000    0.001    0.000 numeric.py:2403(_array_equal_dispatcher)\n",
       "     2301    0.001    0.000    0.001    0.000 {built-in method numpy.asanyarray}\n",
       "       53    0.001    0.000    0.130    0.002 _linesearch.py:274(phi)\n",
       "     2334    0.001    0.000    0.001    0.000 {built-in method builtins.issubclass}\n",
       "     2308    0.001    0.000    0.001    0.000 function_base.py:867(_copy_dispatcher)\n",
       "        6    0.001    0.000    0.190    0.032 _linesearch.py:517(_zoom)\n",
       "     1131    0.001    0.000    0.001    0.000 {method 'numel' of 'torch._C._TensorBase' objects}\n",
       "     2298    0.001    0.000    0.001    0.000 shape_base.py:19(_atleast_1d_dispatcher)\n",
       "     2151    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
       "       20    0.001    0.000    0.075    0.004 _linesearch.py:280(derphi)\n",
       "      130    0.000    0.000    0.001    0.000 _ufunc_config.py:33(seterr)\n",
       "     1430    0.000    0.000    0.000    0.000 fromnumeric.py:2183(_sum_dispatcher)\n",
       "      130    0.000    0.000    0.000    0.000 _ufunc_config.py:132(geterr)\n",
       "      718    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "       23    0.000    0.000    0.001    0.000 _linesearch.py:496(_quadmin)\n",
       "      718    0.000    0.000    0.000    0.000 fromnumeric.py:2698(_amax_dispatcher)\n",
       "      130    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "       65    0.000    0.000    0.001    0.000 _ufunc_config.py:430(__enter__)\n",
       "       43    0.000    0.000    0.000    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
       "      260    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "       65    0.000    0.000    0.001    0.000 _ufunc_config.py:435(__exit__)\n",
       "       40    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
       "        6    0.000    0.000    0.211    0.035 _linesearch.py:317(scalar_search_wolfe2)\n",
       "        3    0.000    0.000    0.014    0.005 _differentiable_functions.py:86(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'uniform' of 'numpy.random.mtrand.RandomState' objects}\n",
       "        1    0.000    0.000    6.642    6.642 {built-in method builtins.exec}\n",
       "        2    0.000    0.000    5.180    2.590 _basinhopping.py:91(_monte_carlo_step)\n",
       "       65    0.000    0.000    0.000    0.000 _ufunc_config.py:426(__init__)\n",
       "        3    0.000    0.000    6.642    2.214 _minimize.py:45(minimize)\n",
       "        3    0.000    0.000    0.000    0.000 twodim_base.py:162(eye)\n",
       "        1    0.000    0.000    6.642    6.642 2998932354.py:1(basinhopping)\n",
       "        6    0.000    0.000    0.211    0.035 _linesearch.py:181(line_search_wolfe2)\n",
       "        3    0.000    0.000    0.000    0.000 linalg.py:2342(norm)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _warnings.warn}\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:458(__enter__)\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
       "       52    0.000    0.000    0.000    0.000 _optimize.py:172(__getattr__)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:321(accept_reject)\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
       "        1    0.000    0.000    1.462    1.462 _basinhopping.py:57(__init__)\n",
       "        2    0.000    0.000    5.180    2.590 _basinhopping.py:145(one_cycle)\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
       "        3    0.000    0.000    0.015    0.005 _optimize.py:244(_prepare_scalar_function)\n",
       "        3    0.000    0.000    0.000    0.000 shape_base.py:81(atleast_2d)\n",
       "        3    0.000    0.000    0.000    0.000 _basinhopping.py:19(_add)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:272(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'dot' of 'numpy.ndarray' objects}\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
       "        1    0.000    0.000    6.642    6.642 <string>:1(<module>)\n",
       "        3    0.000    0.000    6.642    2.214 _basinhopping.py:287(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 _minimize.py:973(standardize_constraints)\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 _util.py:176(check_random_state)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:23(update)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2333(any)\n",
       "       18    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(norm)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
       "       24    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
       "        3    0.000    0.000    0.000    0.000 _base.py:1301(isspmatrix)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:342(__call__)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:234(take_step)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(any)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method math.exp}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "        3    0.000    0.000    0.000    0.000 linalg.py:117(isComplexType)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(shape)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:215(__call__)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:314(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 linalg.py:2338(_norm_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:241(report)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:203(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:268(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:16(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1991(shape)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:282(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 _optimize.py:324(hess)\n",
       "        3    0.000    0.000    0.000    0.000 _optimize.py:216(_check_unknown_options)\n",
       "        3    0.000    0.000    0.000    0.000 shape_base.py:77(_atleast_2d_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2328(_any_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 _linesearch.py:399(<lambda>)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:30(get_lowest)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1987(_shape_dispatcher)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun basinhopping(evaluator.loss_func, niter=2, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1964ef47-72b3-4321-8e07-db1992e882d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.73 s  1.05 s per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit basinhopping(evaluator.loss_func, niter=2, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ba1ec-38e4-4049-9c8f-cdcd8a257600",
   "metadata": {},
   "source": [
    "## New Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "34169bdd-723a-4404-9d0a-6eca3a94dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParFamTorch:\n",
    "    def __init__(self, n_input, degree_input_polynomials, degree_output_polynomials, width, functions=[],\n",
    "                 function_names=[], input_names=None, degree_input_polynomials_specific=None,\n",
    "                 degree_output_polynomials_specific=None, degree_input_denominator=0, degree_output_denominator=0,\n",
    "                 degree_input_polynomials_denominator_specific=None, normalize_denom=True, maximal_potence=3,\n",
    "                 degree_output_polynomials_denominator_specific=None, enforce_function=True, device='cpu'):\n",
    "        self.maximal_potence = maximal_potence\n",
    "        self.n_input = n_input\n",
    "        self.degree_input_polynomials = degree_input_polynomials\n",
    "        self.degree_output_polynomials = degree_output_polynomials\n",
    "        self.degree_input_denominator = degree_input_denominator\n",
    "        self.degree_output_denominator = degree_output_denominator\n",
    "        self.width = width\n",
    "        self.functions = functions\n",
    "        self.enforce_function = enforce_function\n",
    "        self.device = device\n",
    "        # Numerator\n",
    "        if degree_input_polynomials_specific:\n",
    "            if len(degree_input_polynomials_specific) == len(functions) * width:\n",
    "                self.degree_input_polynomials_specific = self.n_input * [\n",
    "                    self.maximal_potence] + degree_input_polynomials_specific\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f'degree_input_polynomials_specific {degree_input_polynomials_specific} has to have the same length'\n",
    "                    f' as functions {functions} times width {width}.')\n",
    "        else:\n",
    "            self.degree_input_polynomials_specific = [self.maximal_potence for _ in range(self.n_input)]\n",
    "        if degree_output_polynomials_specific:\n",
    "            self.degree_output_polynomials_specific = self.n_input * [\n",
    "                self.maximal_potence] + degree_output_polynomials_specific\n",
    "        else:\n",
    "            self.degree_output_polynomials_specific = [self.maximal_potence for _ in\n",
    "                                                       range(self.n_input + self.width * len(functions))]\n",
    "        assert len(self.degree_output_polynomials_specific) == self.n_input + self.width * len(\n",
    "            self.functions)  # TODO: more often\n",
    "        self.monomial_list_input = ParFamTorch.list_monomials(self.n_input, self.degree_input_polynomials,\n",
    "                                                              self.degree_input_polynomials_specific)\n",
    "        self.monomial_list_output = ParFamTorch.list_monomials(self.n_input + self.width * len(functions),\n",
    "                                                               self.degree_output_polynomials,\n",
    "                                                               self.degree_output_polynomials_specific)\n",
    "        if self.enforce_function and len(self.functions) > 0:\n",
    "            self.monomial_list_output = [monomial for monomial in self.monomial_list_output if\n",
    "                                         sum(monomial[self.n_input:]) > 0]\n",
    "\n",
    "        # Denominator\n",
    "        if degree_input_polynomials_denominator_specific:\n",
    "            self.degree_input_polynomials_denominator_specific = self.n_input * [\n",
    "                self.maximal_potence] + degree_input_polynomials_denominator_specific\n",
    "        else:\n",
    "            self.degree_input_polynomials_denominator_specific = self.n_input * [self.maximal_potence]\n",
    "        if degree_output_polynomials_denominator_specific:\n",
    "            self.degree_output_polynomials_denominator_specific = self.n_input * [\n",
    "                self.maximal_potence] + degree_output_polynomials_denominator_specific\n",
    "        else:\n",
    "            self.degree_output_polynomials_denominator_specific = [self.maximal_potence for _ in\n",
    "                                                                   range(self.n_input + self.width * len(functions))]\n",
    "\n",
    "        if self.degree_input_denominator > 0:\n",
    "            self.monomial_list_input_denominator = ParFamTorch.list_monomials(self.n_input,\n",
    "                                                                              self.degree_input_denominator,\n",
    "                                                                              self.degree_input_polynomials_denominator_specific)\n",
    "        else:\n",
    "            self.monomial_list_input_denominator = []\n",
    "        if self.degree_output_denominator > 0:\n",
    "            self.monomial_list_output_denominator = ParFamTorch.list_monomials(self.n_input,\n",
    "                                                                               self.degree_output_denominator,\n",
    "                                                                               self.degree_output_polynomials_denominator_specific)\n",
    "        else:\n",
    "            self.monomial_list_output_denominator = []\n",
    "        self.normalize_denom = normalize_denom\n",
    "        self.n_coefficients_first_layer_numerator = len(self.monomial_list_input)\n",
    "        self.n_coefficients_first_layer_denominator = len(self.monomial_list_input_denominator)\n",
    "        self.n_coefficients_first_layer = len(self.monomial_list_input) + len(self.monomial_list_input_denominator)\n",
    "        self.n_coefficients_last_layer_numerator = len(self.monomial_list_output)\n",
    "        self.n_coefficients_last_layer_denominator = len(self.monomial_list_output_denominator)\n",
    "        self.n_coefficients_last_layer = len(self.monomial_list_output) + len(self.monomial_list_output_denominator)\n",
    "        self.input_monomials = None\n",
    "        self.input_monomials_denominator = None\n",
    "        self.output_monomials_dict = None\n",
    "        self.output_monomials_denominator_dict = None\n",
    "        self.function_names = function_names\n",
    "        if input_names is None:\n",
    "            self.input_names = [sympy.symbols(f'x{i}') for i in range(n_input)]\n",
    "        else:\n",
    "            self.input_names = input_names\n",
    "\n",
    "    def get_formula(self, coefficients, decimals=3, verbose=False):\n",
    "        coefficients = torch.tensor(coefficients, device=self.device)\n",
    "        if len(self.function_names) != len(self.functions):\n",
    "            raise ValueError(f'Length of function_names {self.function_names} and functions {self.functions} is not the'\n",
    "                             f' same, please fix this to compute the formula')\n",
    "        feature_names = copy.copy(self.input_names)\n",
    "\n",
    "        t_0 = time.time()\n",
    "        for i, function_name in enumerate(self.function_names):\n",
    "            current_coefficients = coefficients[\n",
    "                                   i * self.n_coefficients_first_layer: i * self.n_coefficients_first_layer\n",
    "                                                                        + self.n_coefficients_first_layer_numerator]\n",
    "            numerator = ParFamTorch._get_symbolic_polynomial(self.n_input, self.input_names,\n",
    "                                                             self.degree_input_polynomials, current_coefficients,\n",
    "                                                             self.degree_input_polynomials_specific,\n",
    "                                                             self.monomial_list_input, decimals=decimals)\n",
    "            if self.degree_input_denominator > 0:\n",
    "                current_coefficients = coefficients[i * self.n_coefficients_first_layer\n",
    "                                                    + self.n_coefficients_first_layer_numerator:\n",
    "                                                    (i + 1) * self.n_coefficients_first_layer]\n",
    "                if self.normalize_denom:\n",
    "                    current_coefficients = current_coefficients / torch.norm(current_coefficients)\n",
    "                denominator = ParFamTorch._get_symbolic_polynomial(self.n_input, self.input_names,\n",
    "                                                                   self.degree_input_denominator, current_coefficients,\n",
    "                                                                   self.degree_input_polynomials_denominator_specific,\n",
    "                                                                   self.monomial_list_input_denominator,\n",
    "                                                                   decimals=decimals)\n",
    "            else:\n",
    "                denominator = 1.0\n",
    "            if isinstance(denominator, float) and abs(denominator) < 10 ** (-4):\n",
    "                denominator = 10 ** (-4)\n",
    "            feature_name = function_name(numerator / denominator)\n",
    "            feature_names.append(feature_name)\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Computing the formula for the input layer took {(t_1 - t_0):.3f} seconds')\n",
    "\n",
    "        t_0 = time.time()\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[\n",
    "                                   -self.n_coefficients_last_layer:-self.n_coefficients_last_layer_denominator]\n",
    "        else:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer:]\n",
    "        numerator = ParFamTorch._get_symbolic_polynomial(len(feature_names), feature_names,\n",
    "                                                         self.degree_output_polynomials,\n",
    "                                                         current_coefficients, self.degree_output_polynomials_specific,\n",
    "                                                         multiindices=self.monomial_list_output, decimals=decimals)\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer_denominator:]\n",
    "            if self.normalize_denom:\n",
    "                current_coefficients = current_coefficients / torch.norm(current_coefficients)\n",
    "            denominator = ParFamTorch._get_symbolic_polynomial(len(feature_names), feature_names,\n",
    "                                                               self.degree_output_denominator,\n",
    "                                                               current_coefficients,\n",
    "                                                               self.degree_output_polynomials_denominator_specific,\n",
    "                                                               multiindices=self.monomial_list_output_denominator,\n",
    "                                                               decimals=decimals)\n",
    "        else:\n",
    "            denominator = 1\n",
    "        formula = numerator / denominator\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Computing the end formula took {(t_1 - t_0):.3f} seconds')\n",
    "\n",
    "        t_0 = time.time()\n",
    "        # try:\n",
    "        #    formula = sympy.simplify(formula)\n",
    "        # except TimeoutError:\n",
    "        #     print(f'Simplifying took too long, so we omit it this time.')\n",
    "        if isinstance(coefficients[0], float):\n",
    "            if verbose:\n",
    "                print(f'Estimated expression (before simplification): {formula}')\n",
    "            timelimit = True\n",
    "            if timelimit:\n",
    "                formula_dict = {'Formula': formula}\n",
    "                queue = multiprocessing.Queue()\n",
    "                queue.put(formula_dict)\n",
    "                p = multiprocessing.Process(target=_symplify, args=(queue,))\n",
    "                p.start()\n",
    "\n",
    "                # Wait for 10 seconds or until process finishes\n",
    "                p.join(0.001)\n",
    "\n",
    "                # If thread is still active\n",
    "                if p.is_alive():\n",
    "                    print(\"running... let's kill it...\")\n",
    "\n",
    "                    # Terminate - may not work if process is stuck for good\n",
    "                    p.terminate()\n",
    "                    # OR Kill - will work for sure, no chance for process to finish nicely however\n",
    "                    # p.kill()\n",
    "\n",
    "                    p.join()\n",
    "                else:\n",
    "                    p.terminate()\n",
    "                    p.join()\n",
    "                    formula = queue.get()['Formula']\n",
    "            else:\n",
    "                formula = sympy.simplify(formula)\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Simplifying the formula took {(t_1 - t_0):.3f} seconds')\n",
    "        return formula\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_symbolic_polynomial(n_input, input_names, degree, coefficients, degrees_specific, multiindices,\n",
    "                                 decimals):\n",
    "        formula = 0\n",
    "        # multiindices = ParFamTorch.list_monomials(n_input, degree, degrees_specific)\n",
    "\n",
    "        for i, multiindex in enumerate(multiindices):\n",
    "            coefficient = coefficients[i]\n",
    "            if isinstance(coefficient, float) and np.abs(coefficient) < 0.000000001:  # 10**(-decimals)\n",
    "                continue\n",
    "            if isinstance(coefficient, torch.Tensor) and torch.abs(coefficient) < 0.000000001:  # 10**(-decimals)\n",
    "                continue\n",
    "            monomial = 1\n",
    "            for j, index in enumerate(multiindex):\n",
    "                monomial *= input_names[j] ** index\n",
    "            if isinstance(coefficient, float):\n",
    "                formula += np.round(coefficient, decimals=3) * monomial\n",
    "            elif isinstance(coefficient, torch.Tensor):\n",
    "                formula += np.round(coefficient.cpu().detach().numpy(), decimals=3) * monomial\n",
    "            else:\n",
    "                formula += coefficient * monomial\n",
    "        return formula\n",
    "\n",
    "    def get_number_parameters(self):\n",
    "        return self.width * len(self.functions) * self.n_coefficients_first_layer + self.n_coefficients_last_layer\n",
    "\n",
    "    def prepare_input_monomials(self, x):\n",
    "        # Convert x to torch tensor if x is a numpy array\n",
    "        #x = convert_to_module(x, torch, device=self.device)\n",
    "        self.input_monomials = ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input,\n",
    "                                                                          device=self.device)\n",
    "        if self.degree_input_denominator > 0:\n",
    "            self.input_monomials_denominator = \\\n",
    "                ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input_denominator, device=self.device)\n",
    "        self.output_monomials_dict = self._compute_output_monomial_features_dict(x, self.monomial_list_output)\n",
    "        if self.degree_output_denominator > 0:\n",
    "            self.output_monomials_denominator_dict = \\\n",
    "                self._compute_output_monomial_features_dict(x, self.monomial_list_output_denominator)\n",
    "\n",
    "    def testing_mode(self):\n",
    "        self.input_monomials = None\n",
    "        self.input_monomials_denominator = None\n",
    "        self.output_monomials_dict = None\n",
    "        self.output_monomials_denominator_dict = None\n",
    "\n",
    "    def predict(self, coefficients, x, symbolic=False):\n",
    "        #x = convert_to_module(x, torch, device=self.device)\n",
    "        #coefficients = convert_to_module(coefficients, torch, device=self.device)\n",
    "        if self.input_monomials is None:\n",
    "            input_monomials = ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input,\n",
    "                                                                         device=self.device)\n",
    "        else:\n",
    "            input_monomials = copy.copy(self.input_monomials)\n",
    "        if self.input_monomials_denominator is None and self.degree_input_denominator > 0:\n",
    "            input_monomials_denominator = ParFamTorch._evaluate_monomial_features_v1(x,\n",
    "                                                                                     self.monomial_list_input_denominator,\n",
    "                                                                                     device=self.device)\n",
    "        else:\n",
    "            input_monomials_denominator = copy.copy(self.input_monomials_denominator)\n",
    "        hidden_layer = torch.zeros((x.shape[0], x.shape[1] + self.width * len(self.functions)), device=self.device)\n",
    "        hidden_layer[:, :x.shape[1]] = x\n",
    "        feature_number = x.shape[1]\n",
    "        for i, function in enumerate(self.width * self.functions):\n",
    "            current_coefficients = coefficients[\n",
    "                                   i * self.n_coefficients_first_layer: i * self.n_coefficients_first_layer\n",
    "                                                                        + self.n_coefficients_first_layer_numerator]\n",
    "            numerator = ParFamTorch._evaluate_polynomial_v2(inputs=x, coefficients=current_coefficients,\n",
    "                                                            multiindices=self.monomial_list_input,\n",
    "                                                            monomial_features=input_monomials, device=self.device)\n",
    "            if self.degree_input_denominator > 0:\n",
    "                current_coefficients = coefficients[\n",
    "                                       i * self.n_coefficients_first_layer + self.n_coefficients_first_layer_numerator:\n",
    "                                       (i + 1) * self.n_coefficients_first_layer]\n",
    "                if self.normalize_denom:\n",
    "                    current_coefficients = current_coefficients / torch.linalg.norm(current_coefficients)\n",
    "                denominator = ParFamTorch._evaluate_polynomial_v2(inputs=x, coefficients=current_coefficients,\n",
    "                                                                  multiindices=self.monomial_list_input_denominator,\n",
    "                                                                  monomial_features=input_monomials_denominator,\n",
    "                                                                  device=self.device)\n",
    "                denominator = self.stabilize_denominator(denominator)\n",
    "            else:\n",
    "                denominator = 1\n",
    "\n",
    "            try:\n",
    "                hidden_layer[:, feature_number + i] = function(numerator / denominator)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[\n",
    "                                   -self.n_coefficients_last_layer:-self.n_coefficients_last_layer_denominator]\n",
    "        else:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer:]\n",
    "        numerator = ParFamTorch._evaluate_polynomial_v2(inputs=hidden_layer, coefficients=current_coefficients,\n",
    "                                                        multiindices=self.monomial_list_output,\n",
    "                                                        monomial_features=None, n_input=self.n_input,\n",
    "                                                        monomial_features_dict=self.output_monomials_dict,\n",
    "                                                        device=self.device)\n",
    "        if self.degree_output_denominator > 0:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer_denominator:]\n",
    "            if self.normalize_denom:\n",
    "                current_coefficients = current_coefficients / torch.norm(current_coefficients)\n",
    "            denominator = ParFamTorch._evaluate_polynomial_v2(inputs=hidden_layer, coefficients=current_coefficients,\n",
    "                                                              multiindices=self.monomial_list_output_denominator,\n",
    "                                                              monomial_features=None, n_input=self.n_input,\n",
    "                                                              monomial_features_dict=self.output_monomials_denominator_dict,\n",
    "                                                              device=self.device)\n",
    "            denominator = self.stabilize_denominator(denominator)\n",
    "        else:\n",
    "            denominator = 1\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _compute_output_monomial_features_dict(self, inputs, multiindices):\n",
    "        monomial_feature_dict = {}\n",
    "        for i, multiindex in enumerate(multiindices):\n",
    "            reduced_multiindex = multiindex[:inputs.shape[1]]\n",
    "            if reduced_multiindex not in monomial_feature_dict.keys():\n",
    "                monomial_feature_dict[reduced_multiindex] = torch.ones(inputs.shape[0], dtype=torch.float64,\n",
    "                                                                       device=self.device)\n",
    "                for j, index in enumerate(reduced_multiindex):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_feature_dict[reduced_multiindex] *= inputs[:, j]\n",
    "                    else:\n",
    "                        monomial_feature_dict[reduced_multiindex] *= inputs[:, j] ** index\n",
    "        return monomial_feature_dict\n",
    "\n",
    "    def get_normalized_coefficients(self, coefficients):\n",
    "        #coefficients = convert_to_module(coefficients, torch, device=self.device)\n",
    "        for i, function in enumerate(self.width * self.functions):\n",
    "            if self.degree_input_denominator > 0 and self.normalize_denom:\n",
    "                denominator_indices = slice(\n",
    "                    i * self.n_coefficients_first_layer + self.n_coefficients_first_layer_numerator, (\n",
    "                            i + 1) * self.n_coefficients_first_layer)\n",
    "                coefficients[denominator_indices] /= torch.norm(coefficients[denominator_indices], p=2)\n",
    "\n",
    "        if self.degree_output_denominator > 0 and self.normalize_denom:\n",
    "            denominator_indices = slice(-self.n_coefficients_last_layer_denominator, None)\n",
    "            coefficients[denominator_indices] /= torch.norm(coefficients[denominator_indices], p=2)\n",
    "        return coefficients\n",
    "\n",
    "    def stabilize_denominator(self, denominator):\n",
    "        denominator[torch.abs(denominator) < 10 ** (-5)] = 10 ** (-5)\n",
    "        return denominator\n",
    "\n",
    "    def denominator_reg(self, coefficients):\n",
    "        reg = 0\n",
    "        if self.degree_input_denominator > 0:\n",
    "            for i, function in enumerate(self.width * self.functions):\n",
    "                current_coefficients = coefficients[\n",
    "                                       i * self.n_coefficients_first_layer + self.n_coefficients_first_layer_numerator:\n",
    "                                       (i + 1) * self.n_coefficients_first_layer]\n",
    "                reg += (torch.norm(current_coefficients) - 1) ** 2\n",
    "        if self.degree_input_denominator > 0:\n",
    "            current_coefficients = coefficients[-self.n_coefficients_last_layer_denominator:]\n",
    "            reg += (torch.norm(current_coefficients) - 1) ** 2\n",
    "        return reg\n",
    "\n",
    "    def predict_batch(self, coefficients, x, symbolic=False):\n",
    "        batch_size = coefficients.shape[1]\n",
    "        if self.input_monomials is None:\n",
    "            input_monomials = ParFamTorch._evaluate_monomial_features_v1(x, self.monomial_list_input,\n",
    "                                                                         device=self.device)\n",
    "        else:\n",
    "            input_monomials = copy.copy(self.input_monomials)\n",
    "        hidden_layer = np.zeros((x.shape[0], x.shape[1] + self.width * len(self.functions), batch_size))\n",
    "        hidden_layer[:, :x.shape[1]] = np.repeat(x.reshape(*x.shape, 1), batch_size, axis=2)\n",
    "        feature_number = x.shape[1]\n",
    "        for i, function in enumerate(self.width * self.functions):\n",
    "            current_coefficients = coefficients[\n",
    "                                   i * self.n_coefficients_first_layer: (i + 1) * self.n_coefficients_first_layer]\n",
    "            hidden_layer[:, feature_number + i] = function(\n",
    "                ParFamTorch._evaluate_polynomial_v2(inputs=x, coefficients=current_coefficients,\n",
    "                                                    multiindices=self.monomial_list_input,\n",
    "                                                    monomial_features=input_monomials, device=self.device))\n",
    "\n",
    "        current_coefficients = coefficients[-self.n_coefficients_last_layer:]\n",
    "        output = ParFamTorch._evaluate_polynomial_v2(inputs=hidden_layer, coefficients=current_coefficients,\n",
    "                                                     multiindices=self.monomial_list_output,\n",
    "                                                     monomial_features=None, batch_size=batch_size, device=self.device)\n",
    "        return output\n",
    "\n",
    "    def get_mixed_reg(self, n_features, coefficients):\n",
    "        penalty = 0\n",
    "        for feature in range(n_features + self.width * len(self.functions)):\n",
    "            active_feature_coefficients = []\n",
    "            if feature < n_features:\n",
    "                continue\n",
    "            for i, index in enumerate(self.monomial_list_output):\n",
    "                active_feature_coefficients.append(index[feature] > 0)\n",
    "            if sum(np.abs(coefficients[-self.n_coefficients_last_layer:][active_feature_coefficients]) > 0.01) > 1:\n",
    "                penalty += torch.norm(\n",
    "                    coefficients[-self.n_coefficients_last_layer:][active_feature_coefficients],\n",
    "                    p=1)\n",
    "        return penalty\n",
    "\n",
    "    def get_random_coefficients_unique(self, n_functions_max=3):\n",
    "        \"\"\"\n",
    "        Get random coefficients, which relate to meaningful functions.\n",
    "        :param n_functions_max: Number of features to be used, i.e., at most n_functions_max output coefficients are\n",
    "                                non-zero\n",
    "        :return: numpy array, which gives the coefficients\n",
    "        \"\"\"\n",
    "        input_coefficients = np.zeros(self.n_coefficients_first_layer * self.width * len(self.functions))\n",
    "        output_coefficients = np.zeros(self.n_coefficients_last_layer)\n",
    "\n",
    "        monomial_list_output_lists = [list(multi_index) for multi_index in self.monomial_list_output]\n",
    "\n",
    "        # Get the active output monomials\n",
    "        n_functions = 1 + np.random.randint(n_functions_max)\n",
    "        indices = np.random.choice(len(monomial_list_output_lists), size=n_functions,\n",
    "                                   replace=False)\n",
    "        active_output_monomials = []\n",
    "        for index in indices:\n",
    "            active_output_monomials.append(monomial_list_output_lists[index])\n",
    "        # Check if there is any input feature which is used with more than one different degree, e.g.,\n",
    "        # sin(P_1(x)) and sin(P_1(x))^2. Drop them if yes.\n",
    "        dict_active_output_monomials = {}\n",
    "        active_output_monomials_to_remove = []\n",
    "        for multi_index in active_output_monomials:\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                if index in dict_active_output_monomials.keys():\n",
    "                    if dict_active_output_monomials[index] != multiplicity:\n",
    "                        # remove the coefficient if this relates to an already used feature, but with a different degree\n",
    "                        # this time\n",
    "                        active_output_monomials_to_remove.append(multi_index)\n",
    "                        break\n",
    "                else:\n",
    "                    dict_active_output_monomials[index] = multiplicity\n",
    "        for monomial_to_remove in active_output_monomials_to_remove:\n",
    "            active_output_monomials.remove(monomial_to_remove)\n",
    "\n",
    "        # Recreate the dict_active_output_monomials to ensure that the removal did not change it\n",
    "        dict_active_output_monomials = {}\n",
    "        active_output_monomials_to_remove = []\n",
    "        for multi_index in active_output_monomials:\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                dict_active_output_monomials[index] = multiplicity\n",
    "\n",
    "        # Ensure the uniqueness of the formula through the following steps:\n",
    "        # 1. If only one function is used, cosine is used ==> So if (0,0,1) is the only active coefficient, than it will\n",
    "        # be substituted by (0,1,0)\n",
    "        active_output_monomials_to_remove = []\n",
    "        active_output_monomials_to_add = []\n",
    "        if not 1 in dict_active_output_monomials.keys() and 2 in dict_active_output_monomials.keys():\n",
    "            for multi_index in active_output_monomials:\n",
    "                if multi_index[2] != 0:\n",
    "                    # active_output_monomials_to_remove.append(multi_index)\n",
    "                    multi_index[1] = multi_index[2]\n",
    "                    multi_index[2] = 0\n",
    "        # 2. If both functions are used, but with different degrees, ensure that cosine has a lower degree than sine\n",
    "        # ==> So if (0,2,0) and (0,0,1) are active they will both be substituted by (0,0,2) and (0,1,0)\n",
    "        if len(dict_active_output_monomials) == 2:\n",
    "            if dict_active_output_monomials[1] > dict_active_output_monomials[2]:\n",
    "                # swap the multiplicities\n",
    "                dict_active_output_monomials[1], dict_active_output_monomials[2] = dict_active_output_monomials[2], \\\n",
    "                    dict_active_output_monomials[1]\n",
    "                for multi_index in active_output_monomials:\n",
    "                    if multi_index[1] != 0:\n",
    "                        multi_index[1] = dict_active_output_monomials[1]\n",
    "                    if multi_index[2] != 0:\n",
    "                        multi_index[2] = dict_active_output_monomials[2]\n",
    "                        if sum(multi_index) > self.degree_output_polynomials:\n",
    "                            multi_index[2] = dict_active_output_monomials[1]\n",
    "        # 3. If both functions are used with the same degree, make sure that\n",
    "        # a) if there is only 1 active monomial including either cos or sin, we are done.\n",
    "        # b) if there are 2 active monomials including either cos or sin, make sure that cos is the function used in the\n",
    "        # monomial with lower overall degree ==> i.e., (2,2,0) and (1,0,2) will be substituted by (1,2,0) and (2,0,2).\n",
    "        # If this is also the same, e.g., for (0,2,2) and (2,0,2), then the coefficients are chosen such that cosine\n",
    "        # is active in the one without sine, so they will be substituted by (0,2,2) and (2,2,0).\n",
    "        # c)\n",
    "        if len(dict_active_output_monomials) == 2 and dict_active_output_monomials[1] == dict_active_output_monomials[\n",
    "            2]:\n",
    "            degree = dict_active_output_monomials[1]\n",
    "            active_output_monomials_cos_sin = []\n",
    "            for multi_index in active_output_monomials:\n",
    "                if multi_index[1] != 0 or multi_index[2] != 0:\n",
    "                    active_output_monomials_cos_sin.append(multi_index)\n",
    "\n",
    "            # Case b)\n",
    "            if len(active_output_monomials_cos_sin) == 2:\n",
    "                active_output_monomials_cos = []\n",
    "                active_output_monomials_sin = []\n",
    "                for multi_index in active_output_monomials_cos_sin:\n",
    "                    if multi_index[1] != 0:\n",
    "                        active_output_monomials_cos.append(multi_index)\n",
    "                    if multi_index[2] != 0:\n",
    "                        active_output_monomials_sin.append(multi_index)\n",
    "                if len(active_output_monomials_cos) == 1:\n",
    "                    # When cos is active all the time it is not a problem\n",
    "                    if len(active_output_monomials_sin) == 2:\n",
    "                        # Case b) and sine is more often used than cosine ==> swap\n",
    "                        for multi_index in active_output_monomials_sin:\n",
    "                            if multi_index[1] == 0:\n",
    "                                multi_index[1] = degree\n",
    "                                multi_index[2] = 0\n",
    "                    else:\n",
    "                        # both are only used once\n",
    "                        if sum(active_output_monomials_cos[0]) > sum(active_output_monomials_sin[0]):\n",
    "                            # Cosines multi index has a higher overall degree ==> swap\n",
    "                            active_output_monomials_cos[0][1] = 0\n",
    "                            active_output_monomials_cos[0][2] = degree\n",
    "                            active_output_monomials_sin[0][1] = degree\n",
    "                            active_output_monomials_sin[0][2] = 0\n",
    "\n",
    "            if len(active_output_monomials_cos_sin) == 3:\n",
    "                # TODO\n",
    "                pass\n",
    "\n",
    "        active_output_monomials_indices = []\n",
    "        for output_polynomial in active_output_monomials:\n",
    "            index = self.monomial_list_output.index(tuple(output_polynomial))\n",
    "            active_output_monomials_indices.append(index)\n",
    "            output_coefficients[index] = 1\n",
    "\n",
    "        # Get the partitions in the input layer which is related to the active functions\n",
    "        active_input_partitions = set()\n",
    "        for i, multi_index in enumerate(active_output_monomials):\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                active_input_partitions.add(index - self.n_input)\n",
    "\n",
    "        # Select randomly the active coefficients in the partitions\n",
    "        for active_input_partition in active_input_partitions:\n",
    "            n_coeffs = 1 + np.random.randint(self.n_coefficients_first_layer)\n",
    "            if n_coeffs == 1:\n",
    "                # make sure that not only the coefficient for the constant term is chosen\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer - 1), size=n_coeffs) + 1\n",
    "            else:\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer), size=n_coeffs,\n",
    "                                                 replace=False)\n",
    "            for chosen_coeff in chosen_coeffs:\n",
    "                input_coefficients[active_input_partition * self.n_coefficients_first_layer + chosen_coeff] = 1\n",
    "        return np.concatenate([input_coefficients, output_coefficients])\n",
    "\n",
    "    def get_random_coefficients(self, n_functions_max=3):\n",
    "        \"\"\"\n",
    "        Get random coefficients, which relate to meaningful functions.\n",
    "        :param n_functions_max: Number of features to be used, i.e., at most n_functions_max output coefficients are\n",
    "                                non-zero\n",
    "        :return: numpy array, which gives the coefficients\n",
    "        \"\"\"\n",
    "        input_coefficients = np.zeros(self.n_coefficients_first_layer * self.width * len(self.functions))\n",
    "        output_coefficients = np.zeros(self.n_coefficients_last_layer)\n",
    "\n",
    "        # Get the active output coefficients\n",
    "        n_functions = 1 + np.random.randint(n_functions_max)\n",
    "        active_output_monomials_indices = np.random.choice(range(len(self.monomial_list_output)), size=n_functions,\n",
    "                                                           replace=False)\n",
    "        for output_index in active_output_monomials_indices:\n",
    "            output_coefficients[output_index] = 1\n",
    "\n",
    "        # Get the related input coefficients\n",
    "        active_output_monomials = [self.monomial_list_output[index] for index in active_output_monomials_indices]\n",
    "        # Get the partitions in the input layer which is related to the active functions\n",
    "        active_input_partitions = set()\n",
    "        dict_active_output_monomials = {}\n",
    "        for i, multi_index in enumerate(active_output_monomials):\n",
    "            for index, multiplicity in enumerate(multi_index):\n",
    "                if index < self.n_input or multiplicity == 0:\n",
    "                    continue\n",
    "                # Check if there is any input feature which is used with more than one different degree, e.g.,\n",
    "                # sin(P_1(x)) and sin(P_1(x))^2\n",
    "                if index in dict_active_output_monomials.keys():\n",
    "                    if dict_active_output_monomials[index] == multiplicity:\n",
    "                        active_input_partitions.add(index - self.n_input)\n",
    "                    else:\n",
    "                        # remove the coefficient if this relates to an already used feature, but with a different degree\n",
    "                        # this time\n",
    "                        output_coefficients[active_output_monomials_indices[i]] = 0\n",
    "                else:\n",
    "                    dict_active_output_monomials[index] = multiplicity\n",
    "                    active_input_partitions.add(index - self.n_input)\n",
    "\n",
    "        # Select randomly the active coefficients in the partitions\n",
    "        for active_input_partition in active_input_partitions:\n",
    "            n_coeffs = 1 + np.random.randint(self.n_coefficients_first_layer)\n",
    "            if n_coeffs == 1:\n",
    "                # make sure that not only the coefficient for the constant term is chosen\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer - 1), size=n_coeffs) + 1\n",
    "            else:\n",
    "                chosen_coeffs = np.random.choice(range(self.n_coefficients_first_layer), size=n_coeffs,\n",
    "                                                 replace=False)\n",
    "            for chosen_coeff in chosen_coeffs:\n",
    "                input_coefficients[active_input_partition * self.n_coefficients_first_layer + chosen_coeff] = 1\n",
    "        return np.concatenate([input_coefficients, output_coefficients])\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_polynomial_v2(inputs, coefficients, multiindices, device, monomial_features=None, batch_size=1,\n",
    "                                monomial_features_dict=None, n_input=None):\n",
    "        if monomial_features is None:\n",
    "            monomial_features = ParFamTorch._evaluate_monomial_features_v1(inputs, multiindices, batchsize=batch_size,\n",
    "                                                                           monomial_features_dict=monomial_features_dict,\n",
    "                                                                           n_input=n_input, device=device)\n",
    "        if batch_size == 1:\n",
    "            return torch.matmul(monomial_features, coefficients)  # No batches, simple matrix-vector product\n",
    "        else:\n",
    "            # Batches, so we use the matrix-vector product over the left-most indices\n",
    "            return torch.einsum('ij...,j...->i...', monomial_features, coefficients)\n",
    "\n",
    "    @staticmethod\n",
    "    def list_monomials_uniform_degree(n_input, degree, device):\n",
    "        multi_indices = []\n",
    "        indices = torch.arange(degree + 1, device=device)\n",
    "        repeat_indices = [indices for _ in range(n_input)]\n",
    "        for i in product(*repeat_indices):\n",
    "            if sum(i) <= degree:\n",
    "                multi_indices += [i]\n",
    "        return multi_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def list_monomials(n_input, degree, degrees_specific):\n",
    "        multi_indices = []\n",
    "        repeat_indices = [range(degree_specific + 1) for degree_specific in degrees_specific]\n",
    "        for i in product(*repeat_indices):\n",
    "            if sum(i) <= degree:\n",
    "                multi_indices += [i]\n",
    "        return multi_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_monomial_features_v1(inputs, multiindices, device, batchsize=1, monomial_features_dict=None,\n",
    "                                       n_input=None):\n",
    "        if batchsize == 1:\n",
    "            monomial_features = torch.ones((inputs.shape[0], len(multiindices)), dtype=torch.float64, device=device)\n",
    "        else:\n",
    "            monomial_features = torch.ones((inputs.shape[0], len(multiindices), batchsize), dtype=torch.float64,\n",
    "                                           device=device)\n",
    "        if monomial_features_dict is None:\n",
    "            for i, multiindex in enumerate(multiindices):\n",
    "                for j, index in enumerate(multiindex):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_features[:, i] *= inputs[:, j]\n",
    "                    else:\n",
    "                        monomial_features[:, i] *= inputs[:, j] ** index\n",
    "        else:\n",
    "            for i, multiindex in enumerate(multiindices):\n",
    "                reduced_multiindex_input = multiindex[:n_input]\n",
    "                reduced_multiindex_func = multiindex[n_input:]\n",
    "                monomial_features[:, i] = monomial_features_dict[reduced_multiindex_input]\n",
    "                for j, index in enumerate(reduced_multiindex_func):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_features[:, i] *= inputs[:, j + n_input]\n",
    "                    else:\n",
    "                        monomial_features[:, i] *= inputs[:, j + n_input] ** index\n",
    "        return monomial_features\n",
    "\n",
    "    @staticmethod\n",
    "    def get_monomial_mask(n_input, degree):\n",
    "        half_degree = int(torch.ceil(degree / 2))\n",
    "        inputs = torch.ones((1, n_input), device=device) * 2\n",
    "        y = inputs.reshape(*inputs.shape, 1) ** (torch.arange(half_degree) + 1).reshape(1, 1, half_degree)\n",
    "        y_flattened = y.reshape(inputs.shape[0], half_degree * n_input)\n",
    "        y_flattened_with_1 = torch.concatenate((torch.ones((y.shape[0], 1), device=device), y_flattened), axis=1)\n",
    "        z = torch.expand_dims(y_flattened_with_1, axis=1) * torch.expand_dims(y_flattened_with_1, axis=2)\n",
    "        z = torch.squeeze(z, axis=0)\n",
    "        return (0 < z.triu()) & (z.triu() <= 2 ** degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9487472a-e70d-4a71-8796-8b1770041e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, x, y, model, lambda_0, lambda_1, n_params, lambda_mixed=0, lambda_denom=0,\n",
    "                 n_best_coefficients=10, mask=None, lambda_05=None, lambda_1_cut=None, lambda_1_piecewise=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.model = model\n",
    "        self.lambda_0 = lambda_0  # regularization factor for p=0 regularization (cutoff value of 0.01)\n",
    "        self.lambda_1 = lambda_1  # regularization factor for p=1 regularization\n",
    "        self.lambda_1_piecewise = lambda_1_piecewise  # regularization factor for piecewise p=1 regularization: the\n",
    "        # smaller coefficient, the higher the regularization parameter\n",
    "        self.lambda_1_cut = lambda_1_cut  # regularization factor for f(x)=min(|x|, \\sqrt(x))\n",
    "        self.lambda_05 = lambda_05  # regularization factor for p=0.5 regularization\n",
    "        self.lambda_mixed = lambda_mixed  # regularization factor for functions which are used more than once; default\n",
    "        self.lambda_denom = lambda_denom  # regularization factor for the coefficients of the denominator, to keep their\n",
    "        # norm close to 1, as otherwise the values of all parameters can be pressed to 0 due to overparametrization\n",
    "        self.loss_list = []\n",
    "        self.l2_dist_list = []\n",
    "        self.reg_list = []\n",
    "        self.n_active_parameters_list = []\n",
    "        self.best_losses = np.full((n_best_coefficients,), float('inf'))\n",
    "        self.mask = mask\n",
    "        self.n_params = n_params\n",
    "        self.device = self.model.device\n",
    "\n",
    "        self.coefficients_current = None\n",
    "        self.loss_current = None\n",
    "        if self.mask == None:\n",
    "            self.best_coefficients = np.inf * np.ones((n_best_coefficients, n_params))\n",
    "        else:\n",
    "            self.n_active_coefficients = sum(mask)\n",
    "            self.best_coefficients = np.inf * np.ones((n_best_coefficients, self.n_active_coefficients))\n",
    "        self.evaluations = 0\n",
    "\n",
    "    #def convert_evaluator_variables_to_module(self, module):\n",
    "    #    self.x = convert_to_module(self.x, module, device=self.device)\n",
    "    #    self.y = convert_to_module(self.y, module, device=self.device)\n",
    "    #    self.best_coefficients = convert_to_module(self.best_coefficients, module, device=self.device)\n",
    "    #    self.best_losses = convert_to_module(self.best_losses, module, device=self.device)\n",
    "\n",
    "    def loss_func_torch(self):\n",
    "        # convert array to tensors?\n",
    "        #self.convert_evaluator_variables_to_module(torch)\n",
    "        self.evaluations += 1\n",
    "        if self.mask is None:\n",
    "            y_pred = self.model.predict(self.coefficients_current, self.x)\n",
    "        else:\n",
    "            coefficients_extended = torch.zeros(self.n_params, device=self.device, dtype=torch.double)\n",
    "            coefficients_extended[self.mask] = self.coefficients_current\n",
    "            y_pred = self.model.predict(coefficients_extended, self.x)\n",
    "        if y_pred.shape != self.y.shape:\n",
    "            print(f'Careful, there is a shape mismatch in the loss function: '\n",
    "                  f'y_pred.shape {y_pred.shape} != y.shape {self.y.shape}')\n",
    "        # Use torch.norm since torch.linalg.norm does not support tensors with requires grad\n",
    "        rel_l2_dist = torch.norm(y_pred - self.y, p=2) / torch.norm(self.y, p=2)\n",
    "        n_active_parameters = torch.sum(torch.abs(self.coefficients_current) > 0.01)\n",
    "        reg_0 = self.lambda_0 * n_active_parameters\n",
    "        reg_1 = self.lambda_1 * torch.norm(self.coefficients_current, p=1)\n",
    "        reg_1_piecewise = 0\n",
    "        if self.lambda_1_piecewise:\n",
    "            reg_1_piecewise += torch.norm(\n",
    "                self.coefficients_current[self.coefficients_current < 0.1]) * self.lambda_1_piecewise\n",
    "            reg_1_piecewise += torch.norm(self.coefficients_current[(0.1 < self.coefficients_current) & (\n",
    "                    self.coefficients_current < 1)]) * self.lambda_1_piecewise * 0.1 + self.lambda_1_piecewise * 0.1\n",
    "            reg_1_piecewise += torch.norm(\n",
    "                self.coefficients_current[self.coefficients_current > 1]) * self.lambda_1_piecewise * 0.01 + \\\n",
    "                               self.lambda_1_piecewise * 0.1 + self.lambda_1_piecewise * 0.9 * 0.1\n",
    "\n",
    "        if self.lambda_05:\n",
    "            reg_05 = self.lambda_05 * torch.norm(self.coefficients_current, p=1)\n",
    "        else:\n",
    "            reg_05 = 0\n",
    "        if self.lambda_1_cut:\n",
    "            reg_1_cut = self.lambda_1_cut * torch.sum(\n",
    "                torch.minimum(torch.abs(self.coefficients_current), torch.sqrt(torch.abs(self.coefficients_current))))\n",
    "        else:\n",
    "            reg_1_cut = 0\n",
    "\n",
    "        # Penalizing it when one term is used more than once seemed like a good idea, but apparently it rather harms the\n",
    "        # problem at hand\n",
    "        if self.lambda_mixed > 0:\n",
    "            reg_2 = self.model.get_mixed_reg(n_features=self.x.shape[1], coefficients=self.coefficients_current)\n",
    "        else:\n",
    "            reg_2 = 0  # Default\n",
    "        if self.lambda_denom:\n",
    "            reg_3 = self.lambda_denom * self.model.denominator_reg(self.coefficients_current)\n",
    "        else:\n",
    "            reg_3 = 0\n",
    "        self.loss_current = rel_l2_dist + reg_0 + reg_1 + reg_2 + reg_3 + reg_05 + reg_1_cut + reg_1_piecewise\n",
    "        self.l2_dist_list.append(rel_l2_dist)\n",
    "        self.reg_list.append(reg_0 + reg_1)\n",
    "        self.loss_list.append(self.loss_current)\n",
    "        self.n_active_parameters_list.append(n_active_parameters)\n",
    "\n",
    "        #if self.loss_current < self.best_losses[-1]:\n",
    "        #    self.best_losses[-1] = self.loss_current\n",
    "        #    self.best_coefficients[-1] = self.coefficients_current\n",
    "        #    order = torch.argsort(self.best_losses)\n",
    "        #    self.best_losses = self.best_losses[order]\n",
    "        #    self.best_coefficients = self.best_coefficients[order]\n",
    "        return 0\n",
    "\n",
    "    def gradient(self, _):\n",
    "        # The if-query should be unnecessary. Test later for specific solver....\n",
    "        # if np.linalg.norm(coefficients - self.coefficients_current.detach().numpy()) > 1e-3:\n",
    "        #     coefficients = convert_to_module(coefficients, torch)\n",
    "        #     coefficients.requires_grad_()\n",
    "        #     self.loss_current = self.loss_func_torch(coefficients)\n",
    "        self.loss_current.backward()\n",
    "        return convert_to_module(self.coefficients_current.grad, np, device=self.device)\n",
    "\n",
    "    def loss_func(self, coefficients):\n",
    "        #self.coefficients_current = convert_to_module(coefficients, torch, device=self.device)\n",
    "        self.coefficients_current = torch.from_numpy(coefficients).to(self.device)\n",
    "        self.coefficients_current.requires_grad_()\n",
    "        self.loss_func_torch()\n",
    "        return (self.loss_current).cpu().detach().numpy()\n",
    "\n",
    "    # def loss_func(self, coefficients):\n",
    "    #     self.coefficients_current = convert_to_module(coefficients, torch)\n",
    "    #     self.coefficients_current.requires_grad_()\n",
    "    #     self.loss_func_torch()\n",
    "    #     self.loss_current.backward()\n",
    "    #     gradient = convert_to_module(self.coefficients_current.grad, np)\n",
    "    #     return convert_to_module(self.loss_current, np), gradient\n",
    "\n",
    "    def plot_training_statistics(self, width, height):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(width, height))\n",
    "        axs[0, 0].plot(self.loss_list)\n",
    "        axs[0, 0].set_yscale('log')\n",
    "        axs[0, 0].set_title(f'Loss')\n",
    "\n",
    "        axs[1, 0].plot(self.l2_dist_list)\n",
    "        axs[1, 0].set_yscale('log')\n",
    "        axs[1, 0].set_title(f'Training l2 distance')\n",
    "\n",
    "        axs[0, 1].plot(self.reg_list)\n",
    "        # axs[0, 1].set_yscale('log')\n",
    "        axs[0, 1].set_title(f'L0 + L1 Regularization')\n",
    "\n",
    "        axs[1, 1].plot(self.n_active_parameters_list)\n",
    "        axs[1, 1].set_ylim([0, np.max(self.n_active_parameters_list) + 1])\n",
    "        # axs[1, 1].set_yscale('log')\n",
    "        axs[1, 1].set_title(f'Number active terms')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def fit_lbfgs(self, coefficients, verbose):\n",
    "        t_0 = time.time()\n",
    "        coefficients.requires_grad = True\n",
    "        optimizer = torch.optim.LBFGS([coefficients], line_search_fn='strong_wolfe')\n",
    "        for i in range(50):\n",
    "            def closure():\n",
    "                if torch.is_grad_enabled():\n",
    "                    optimizer.zero_grad()\n",
    "                loss = self.loss_func_torch(coefficients)\n",
    "                if loss.requires_grad:\n",
    "                    loss.backward()\n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "        # ret = basinhopping(evaluator.gradient, niter=50, x0=x0, minimizer_kwargs={'jac': True})\n",
    "        t_1 = time.time()\n",
    "        if verbose:\n",
    "            print(f'Training time: {t_1 - t_0}')\n",
    "        self.model.testing_mode()\n",
    "        if verbose:\n",
    "            print(f'Coefficients: {coefficients}')\n",
    "        relative_l2_distance = np.linalg.norm(\n",
    "            self.y - self.model.predict(torch.tensor(coefficients, device=self.device), self.x).cpu().detach().numpy(),\n",
    "            ord=2) / np.linalg.norm(self.y, ord=2)\n",
    "        if verbose:\n",
    "            print(f'Relative l2 distance: {relative_l2_distance}')\n",
    "        return coefficients, relative_l2_distance\n",
    "\n",
    "\n",
    "def round_expr(expr, num_digits):\n",
    "    return expr.xreplace({n: round(n, num_digits) for n in expr.atoms(sympy.Number)})\n",
    "\n",
    "\n",
    "def get_active_coefficients(coefficients, model, x):\n",
    "    y = model.predict(coefficients, x)\n",
    "    a = np.zeros(coefficients.shape)\n",
    "    for i, coefficient in enumerate(coefficients):\n",
    "        if coefficient == 0:\n",
    "            continue\n",
    "        distorted_coefficient_array = 10 * np.random.randn(10)\n",
    "        for distorted_coefficient in distorted_coefficient_array:\n",
    "            test_coefficients = copy.copy(coefficients)\n",
    "            test_coefficients[i] = distorted_coefficient\n",
    "            if np.linalg.norm(y - model.predict(test_coefficients, x)) > 10 ** (-8):\n",
    "                a[i] = 1\n",
    "                break\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "cd48ffc4-549f-4930-9da2-760138c81bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_training(device):\n",
    "    np.random.seed(12345)\n",
    "    torch.manual_seed(12345)\n",
    "    print(f'Using {device}')\n",
    "    a = 5 * np.random.randn(1)\n",
    "    x = np.arange(1, 10, 0.05)\n",
    "    x = x.reshape(len(x), 1)\n",
    "\n",
    "    test_model = False\n",
    "\n",
    "    def func(a, x, module):\n",
    "        # Good approximations with both, however, never yields a simple formula\n",
    "        return module.sin((a[0] * x + 1) / (0.1 * x + 2))\n",
    "        # return module.sin((a[0] * x + 1) / (0.1 * x))  # Works for lambda_1=0.1, lambda_denom=0 (not = 1)\n",
    "        # return module.sin((a[0] * x))  # Works for lambda_1=0.1, lambda_denom=1\n",
    "        # return a[0] * x\n",
    "        # return 0.2 * module.sin(a[0] * x) / x\n",
    "        # return 0.5 * x / (x + 1)  # Only works when using normalize_denom=True and lambda_1>=0.3\n",
    "\n",
    "    y = func(a, x, np).squeeze(-1)\n",
    "    print(f'Target formula: {func(a, sympy.Symbol(\"x\"), sympy)}')\n",
    "\n",
    "    functions = [torch.sin]\n",
    "    function_names = [sympy.sin]\n",
    "    model = ParFamTorch(n_input=1, degree_input_polynomials=2, degree_output_polynomials=2, width=1,\n",
    "                        functions=functions, function_names=function_names, maximal_potence=2,\n",
    "                        degree_output_polynomials_specific=[1], enforce_function=False,\n",
    "                        degree_input_denominator=2, degree_output_denominator=2, normalize_denom=True,\n",
    "                        degree_output_polynomials_denominator_specific=[1], device=device)\n",
    "\n",
    "    n_params = model.get_number_parameters()\n",
    "    print(f'Number parameters: {n_params}')\n",
    "    evaluator = Evaluator(x, y, model=model, lambda_0=0, lambda_1=0.001, lambda_denom=0, n_params=n_params)\n",
    "    model.prepare_input_monomials(x)\n",
    "\n",
    "    lw = [-10] * n_params\n",
    "    up = [10] * n_params\n",
    "    t_0 = time.time()\n",
    "    x0 = np.random.randn(n_params)\n",
    "    ret = basinhopping(evaluator.loss_func, niter=50, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})\n",
    "    t_1 = time.time()\n",
    "    print(f'Training time: {t_1 - t_0}')\n",
    "    if not test_model:\n",
    "        model.testing_mode()\n",
    "\n",
    "    print(f'Coefficients: {ret.x}')\n",
    "    print(\n",
    "        f'Relative l2 distance: {np.linalg.norm(y - model.predict(torch.tensor(ret.x, device=device), x).cpu().detach().numpy(), ord=2) / np.linalg.norm(y, ord=2)}')\n",
    "    print(f'Formula: {model.get_formula(torch.tensor(ret.x), decimals=10)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168dfc43-32e1-4d93-9cb1-ef2a238a966b",
   "metadata": {},
   "source": [
    "## Orignal run benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6e453988-b711-4144-94d2-0d51c157753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_training('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3754138f-623b-464e-88f3-9c4a3abd6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_training('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e561c62-1996-41bd-a3b8-97cb5cf4ff65",
   "metadata": {},
   "source": [
    "## Optimizing for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998787a7-dabe-416b-b197-ce19cb91d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Target formula: sin((7.13565254211426*x + 1)/(0.1*x + 2))\n",
      "Number parameters: 16\n"
     ]
    }
   ],
   "source": [
    "# Input parameters\n",
    "device = 'cuda'\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "print(f'Using {device}')\n",
    "a = 5 * torch.randn(1)\n",
    "x = np.arange(1, 10, 0.05)\n",
    "x = x.reshape(len(x), 1)\n",
    "x = torch.tensor(x).to(device)\n",
    "\n",
    "test_model = False\n",
    "\n",
    "def func(a, x, module):\n",
    "    # Good approximations with both, however, never yields a simple formula\n",
    "    return module.sin((a[0] * x + 1) / (0.1 * x + 2))\n",
    "\n",
    "y = func(a, x, torch).squeeze(-1)\n",
    "print(f'Target formula: {func(a, sympy.Symbol(\"x\"), sympy)}')\n",
    "\n",
    "functions = [torch.sin]\n",
    "function_names = [sympy.sin]\n",
    "\n",
    "model = ParFamTorch(n_input=1, degree_input_polynomials=2, degree_output_polynomials=2, width=1,\n",
    "                    functions=functions, function_names=function_names, maximal_potence=2,\n",
    "                    degree_output_polynomials_specific=[1], enforce_function=False,\n",
    "                    degree_input_denominator=2, degree_output_denominator=2, normalize_denom=True,\n",
    "                    degree_output_polynomials_denominator_specific=[1], device=device)\n",
    "\n",
    "n_params = model.get_number_parameters()\n",
    "print(f'Number parameters: {n_params}')\n",
    "evaluator = Evaluator(x, y, model=model, lambda_0=0, lambda_1=0.001, lambda_denom=0, n_params=n_params)\n",
    "model.prepare_input_monomials(x)\n",
    "\n",
    "lw = [-10] * n_params\n",
    "up = [10] * n_params\n",
    "t_0 = time.time()\n",
    "x0 = np.random.randn(n_params)\n",
    "ret = basinhopping(evaluator.loss_func, niter=10, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})\n",
    "t_1 = time.time()\n",
    "print(\"Running time \", t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0b9955bf-2fcf-4ff8-9e33-a9f3826a76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         336137 function calls in 6.840 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "     1131    3.460    0.003    3.460    0.003 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
       "     2328    0.709    0.000    0.773    0.000 3236217983.py:615(_evaluate_monomial_features_v1)\n",
       "     1164    0.410    0.000    2.527    0.002 3236217983.py:698(loss_func_torch)\n",
       "     1164    0.294    0.000    1.872    0.002 3236217983.py:236(predict)\n",
       "     3493    0.157    0.000    0.157    0.000 {built-in method torch.tensor}\n",
       "     2328    0.153    0.000    0.212    0.000 3236217983.py:332(stabilize_denominator)\n",
       "     4656    0.131    0.000    0.131    0.000 {built-in method torch.matmul}\n",
       "     2295    0.111    0.000    0.111    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
       "     3492    0.100    0.000    0.100    0.000 {built-in method torch.norm}\n",
       "        3    0.096    0.032    6.839    2.280 _optimize.py:1318(_minimize_bfgs)\n",
       "     3492    0.085    0.000    0.085    0.000 {built-in method torch.abs}\n",
       "     1164    0.078    0.000    0.078    0.000 {built-in method torch.frobenius_norm}\n",
       "     2328    0.063    0.000    0.063    0.000 {built-in method torch.ones}\n",
       "     2328    0.054    0.000    0.107    0.000 _tensor.py:207(_reduce_ex_internal)\n",
       "     4656    0.050    0.000    0.246    0.000 functional.py:1345(norm)\n",
       "     1164    0.048    0.000    0.048    0.000 {built-in method torch.sum}\n",
       "     4552    0.048    0.000    0.048    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "     1164    0.047    0.000    0.047    0.000 {built-in method torch.zeros}\n",
       "     1164    0.045    0.000    0.045    0.000 {built-in method torch._C._linalg.linalg_norm}\n",
       "    13974    0.042    0.000    0.164    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "     1131    0.037    0.000    0.037    0.000 {built-in method torch.ones_like}\n",
       "     1161    0.029    0.000    3.732    0.003 _linesearch.py:77(derphi)\n",
       "    10443    0.027    0.000    0.276    0.000 utils.py:14(convert_to_module)\n",
       "      718    0.025    0.000    6.455    0.009 _linesearch.py:91(scalar_search_wolfe1)\n",
       "     2401    0.024    0.000    0.063    0.000 numeric.py:2407(array_equal)\n",
       "     1164    0.022    0.000    0.022    0.000 {built-in method torch.sin}\n",
       "     2328    0.020    0.000    0.020    0.000 {method 'set_' of 'torch._C._TensorBase' objects}\n",
       "     2295    0.019    0.000    0.019    0.000 {method 'detach' of 'torch._C._TensorBase' objects}\n",
       "     1161    0.018    0.000    2.694    0.002 _linesearch.py:73(phi)\n",
       "     2328    0.018    0.000    0.234    0.000 copy.py:66(copy)\n",
       "     2148    0.017    0.000    0.063    0.000 _optimize.py:235(vecnorm)\n",
       "     1164    0.016    0.000    2.744    0.002 _differentiable_functions.py:132(fun_wrapped)\n",
       "     1131    0.014    0.000    3.536    0.003 _tensor.py:340(backward)\n",
       "    39702    0.014    0.000    0.017    0.000 {built-in method builtins.isinstance}\n",
       "     1164    0.014    0.000    2.709    0.002 3236217983.py:769(loss_func)\n",
       "     4656    0.013    0.000    0.917    0.000 3236217983.py:583(_evaluate_polynomial_v2)\n",
       "     2328    0.013    0.000    0.030    0.000 _tensor.py:196(storage)\n",
       "      165    0.012    0.000    0.012    0.000 {built-in method torch.argsort}\n",
       "     1131    0.011    0.000    3.695    0.003 _differentiable_functions.py:162(grad_wrapped)\n",
       "     1131    0.010    0.000    3.661    0.003 3236217983.py:760(gradient)\n",
       "     4656    0.010    0.000    0.013    0.000 storage.py:366(__init__)\n",
       "     1131    0.010    0.000    3.521    0.003 __init__.py:85(backward)\n",
       "     2328    0.010    0.000    0.085    0.000 _utils.py:131(_rebuild_tensor)\n",
       "     1131    0.009    0.000    0.048    0.000 __init__.py:30(_make_grads)\n",
       "     2295    0.009    0.000    0.009    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
       "     1131    0.009    0.000    0.011    0.000 _tensor.py:1071(grad)\n",
       "     2298    0.009    0.000    0.012    0.000 shape_base.py:23(atleast_1d)\n",
       "     9315    0.009    0.000    0.009    0.000 {built-in method builtins.getattr}\n",
       "     2151    0.008    0.000    0.030    0.000 fromnumeric.py:69(_wrapreduction)\n",
       "     4656    0.008    0.000    0.010    0.000 storage.py:299(__new__)\n",
       "     4808    0.007    0.000    0.035    0.000 <__array_function__ internals>:177(dot)\n",
       "     2328    0.007    0.000    0.092    0.000 _utils.py:137(_rebuild_tensor_v2)\n",
       "     2309    0.007    0.000    0.007    0.000 {built-in method numpy.array}\n",
       "      718    0.006    0.000    6.464    0.009 _linesearch.py:31(line_search_wolfe1)\n",
       "      718    0.006    0.000    6.628    0.009 _optimize.py:1144(_line_search_wolfe12)\n",
       "     1217    0.006    0.000    2.778    0.002 _differentiable_functions.py:264(fun)\n",
       "     4656    0.006    0.000    0.008    0.000 _VF.py:25(__getattr__)\n",
       "     9246    0.005    0.000    0.005    0.000 {built-in method torch._C._has_torch_function_unary}\n",
       "     2401    0.005    0.000    0.037    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
       "     2298    0.005    0.000    0.022    0.000 <__array_function__ internals>:177(atleast_1d)\n",
       "     1184    0.005    0.000    3.737    0.003 _differentiable_functions.py:270(grad)\n",
       "     1164    0.005    0.000    0.009    0.000 3236217983.py:692(convert_evaluator_variables_to_module)\n",
       "     2401    0.004    0.000    0.073    0.000 <__array_function__ internals>:177(array_equal)\n",
       "     1161    0.004    0.000    0.016    0.000 _differentiable_functions.py:240(update_x)\n",
       "    11586    0.004    0.000    0.004    0.000 {built-in method builtins.len}\n",
       "     2308    0.004    0.000    0.016    0.000 <__array_function__ internals>:177(copy)\n",
       "     1430    0.004    0.000    0.018    0.000 fromnumeric.py:2188(sum)\n",
       "     2328    0.004    0.000    0.096    0.000 copy.py:258(_reconstruct)\n",
       "     1164    0.004    0.000    0.009    0.000 numeric.py:1878(isscalar)\n",
       "     2328    0.004    0.000    0.111    0.000 _tensor.py:175(__reduce_ex__)\n",
       "     1164    0.003    0.000    0.003    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "      718    0.003    0.000    0.020    0.000 fromnumeric.py:2703(amax)\n",
       "     2328    0.003    0.000    0.003    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "     8419    0.003    0.000    0.003    0.000 {built-in method numpy.asarray}\n",
       "     1187    0.003    0.000    3.701    0.003 _differentiable_functions.py:254(_update_grad)\n",
       "     1131    0.003    0.000    3.698    0.003 _differentiable_functions.py:166(update_grad)\n",
       "     4656    0.003    0.000    0.003    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "     2401    0.003    0.000    0.032    0.000 _methods.py:61(_all)\n",
       "     1430    0.003    0.000    0.023    0.000 <__array_function__ internals>:177(sum)\n",
       "     1439    0.003    0.000    0.003    0.000 {built-in method numpy.zeros}\n",
       "     3492    0.003    0.000    0.003    0.000 functional.py:1450(<listcomp>)\n",
       "     8089    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}\n",
       "     2151    0.003    0.000    0.003    0.000 fromnumeric.py:70(<dictcomp>)\n",
       "     1220    0.002    0.000    2.748    0.002 _differentiable_functions.py:249(_update_fun)\n",
       "     2328    0.002    0.000    0.004    0.000 _namedtensor_internals.py:10(check_serializing_named_tensor)\n",
       "     1164    0.002    0.000    0.002    0.000 {method 'requires_grad_' of 'torch._C._TensorBase' objects}\n",
       "     2295    0.002    0.000    0.003    0.000 __init__.py:281(is_tensor)\n",
       "     6984    0.002    0.000    0.002    0.000 storage.py:437(_untyped)\n",
       "     4656    0.002    0.000    0.002    0.000 {built-in method __new__ of type object at 0x55b9e68d4620}\n",
       "     1164    0.002    0.000    2.746    0.002 _differentiable_functions.py:154(update_fun)\n",
       "     1166    0.002    0.000    0.002    0.000 {built-in method _abc._abc_instancecheck}\n",
       "     2328    0.002    0.000    0.002    0.000 {method '_storage' of 'torch._C._TensorBase' objects}\n",
       "     2308    0.002    0.000    0.009    0.000 function_base.py:871(copy)\n",
       "     2328    0.002    0.000    0.002    0.000 hooks.py:54(warn_if_has_hooks)\n",
       "      718    0.002    0.000    0.024    0.000 <__array_function__ internals>:177(amax)\n",
       "     2328    0.002    0.000    0.002    0.000 {method 'stride' of 'torch._C._TensorBase' objects}\n",
       "       40    0.002    0.000    0.003    0.000 _linesearch.py:462(_cubicmin)\n",
       "     4808    0.002    0.000    0.002    0.000 multiarray.py:740(dot)\n",
       "     4656    0.002    0.000    0.002    0.000 {method 'get' of 'dict' objects}\n",
       "     2328    0.001    0.000    0.001    0.000 {method 'data_ptr' of 'torch._C._TensorBase' objects}\n",
       "     2328    0.001    0.000    0.001    0.000 {method 'has_names' of 'torch._C._TensorBase' objects}\n",
       "     2328    0.001    0.000    0.001    0.000 {method 'storage_offset' of 'torch._C._TensorBase' objects}\n",
       "     1164    0.001    0.000    0.001    0.000 {method 'item' of 'numpy.ndarray' objects}\n",
       "     1166    0.001    0.000    0.003    0.000 abc.py:96(__instancecheck__)\n",
       "     1131    0.001    0.000    0.001    0.000 __init__.py:77(_tensor_or_tensors_to_tuple)\n",
       "      733    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "     2401    0.001    0.000    0.001    0.000 numeric.py:2403(_array_equal_dispatcher)\n",
       "     2301    0.001    0.000    0.001    0.000 {built-in method numpy.asanyarray}\n",
       "     2308    0.001    0.000    0.001    0.000 function_base.py:867(_copy_dispatcher)\n",
       "     2334    0.001    0.000    0.001    0.000 {built-in method builtins.issubclass}\n",
       "     1131    0.001    0.000    0.001    0.000 {method 'numel' of 'torch._C._TensorBase' objects}\n",
       "     2298    0.001    0.000    0.001    0.000 shape_base.py:19(_atleast_1d_dispatcher)\n",
       "     2151    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
       "       53    0.001    0.000    0.103    0.002 _linesearch.py:274(phi)\n",
       "        6    0.001    0.000    0.139    0.023 _linesearch.py:517(_zoom)\n",
       "     1430    0.000    0.000    0.000    0.000 fromnumeric.py:2183(_sum_dispatcher)\n",
       "      130    0.000    0.000    0.001    0.000 _ufunc_config.py:33(seterr)\n",
       "       20    0.000    0.000    0.050    0.002 _linesearch.py:280(derphi)\n",
       "      130    0.000    0.000    0.000    0.000 _ufunc_config.py:132(geterr)\n",
       "      718    0.000    0.000    0.000    0.000 fromnumeric.py:2698(_amax_dispatcher)\n",
       "      718    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "       23    0.000    0.000    0.001    0.000 _linesearch.py:496(_quadmin)\n",
       "      130    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "       65    0.000    0.000    0.001    0.000 _ufunc_config.py:430(__enter__)\n",
       "       43    0.000    0.000    0.000    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
       "        6    0.000    0.000    0.157    0.026 _linesearch.py:317(scalar_search_wolfe2)\n",
       "      260    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "       65    0.000    0.000    0.001    0.000 _ufunc_config.py:435(__exit__)\n",
       "       40    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
       "        3    0.000    0.000    0.034    0.011 _differentiable_functions.py:86(__init__)\n",
       "        1    0.000    0.000    6.840    6.840 {built-in method builtins.exec}\n",
       "        3    0.000    0.000    6.839    2.280 _minimize.py:45(minimize)\n",
       "        6    0.000    0.000    0.157    0.026 _linesearch.py:181(line_search_wolfe2)\n",
       "        1    0.000    0.000    6.839    6.839 2998932354.py:1(basinhopping)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'uniform' of 'numpy.random.mtrand.RandomState' objects}\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:458(__enter__)\n",
       "       65    0.000    0.000    0.000    0.000 _ufunc_config.py:426(__init__)\n",
       "        2    0.000    0.000    5.322    2.661 _basinhopping.py:91(_monte_carlo_step)\n",
       "        2    0.000    0.000    5.322    2.661 _basinhopping.py:145(one_cycle)\n",
       "        3    0.000    0.000    0.000    0.000 twodim_base.py:162(eye)\n",
       "        3    0.000    0.000    0.000    0.000 linalg.py:2342(norm)\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
       "        1    0.000    0.000    1.517    1.517 _basinhopping.py:57(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
       "       52    0.000    0.000    0.000    0.000 _optimize.py:172(__getattr__)\n",
       "        3    0.000    0.000    0.034    0.011 _optimize.py:244(_prepare_scalar_function)\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _warnings.warn}\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:321(accept_reject)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
       "        6    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:272(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 _util.py:176(check_random_state)\n",
       "        3    0.000    0.000    0.000    0.000 _minimize.py:973(standardize_constraints)\n",
       "        3    0.000    0.000    0.000    0.000 shape_base.py:81(atleast_2d)\n",
       "        3    0.000    0.000    6.839    2.280 _basinhopping.py:287(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 _basinhopping.py:19(_add)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'dot' of 'numpy.ndarray' objects}\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
       "        1    0.000    0.000    6.839    6.839 <string>:1(<module>)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2333(any)\n",
       "       11    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "       18    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:23(update)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(any)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(norm)\n",
       "       24    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:234(take_step)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:342(__call__)\n",
       "        3    0.000    0.000    0.000    0.000 _base.py:1301(isspmatrix)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(shape)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method math.exp}\n",
       "        3    0.000    0.000    0.000    0.000 linalg.py:117(isComplexType)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:203(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:215(__call__)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:314(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:268(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:16(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:282(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1991(shape)\n",
       "        2    0.000    0.000    0.000    0.000 _basinhopping.py:241(report)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
       "        3    0.000    0.000    0.000    0.000 _optimize.py:216(_check_unknown_options)\n",
       "        3    0.000    0.000    0.000    0.000 _linesearch.py:399(<lambda>)\n",
       "        3    0.000    0.000    0.000    0.000 linalg.py:2338(_norm_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2328(_any_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 shape_base.py:77(_atleast_2d_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 _optimize.py:324(hess)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:1987(_shape_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 _basinhopping.py:30(get_lowest)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun basinhopping(evaluator.loss_func, niter=2, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b952ed50-d02b-4a9c-a4aa-59affe57fec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.31 s  1.28 s per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit basinhopping(evaluator.loss_func, niter=2, x0=x0, minimizer_kwargs={'jac': evaluator.gradient})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14f0cc-1077-4b0b-abb4-63b6470f075f",
   "metadata": {},
   "source": [
    "## Notes for code optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc6c69-0309-4fde-95cf-7acdc361bf7a",
   "metadata": {},
   "source": [
    "Things slowing down the forward and backward pass: \n",
    "- The constant jump between torch and numpy arrays (convert_to_module) while basin hopping interacts just with loss_func which might have a fixed input/output type.\n",
    "    - Idea to solve this, wrap up Scipy's basin_hoppiung\n",
    "- The for loops with array slicing, multiplication, and replacement (see for example ParFamTorch.predict).\n",
    "    - Idea to solve this, replace for loops and slicings with einsums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "698daf6b-147e-45d6-a1be-236238f2ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_monomial_features_v1(inputs, multiindices, device, batchsize=1, monomial_features_dict=None,\n",
    "                                       n_input=None):\n",
    "        if batchsize == 1:\n",
    "            monomial_features = torch.ones((inputs.shape[0], len(multiindices)), dtype=torch.float64, device=device)\n",
    "        else:\n",
    "            monomial_features = torch.ones((inputs.shape[0], len(multiindices), batchsize), dtype=torch.float64,\n",
    "                                           device=device)\n",
    "        if monomial_features_dict is None:\n",
    "            for i, multiindex in enumerate(multiindices):\n",
    "                for j, index in enumerate(multiindex):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_features[:, i] *= inputs[:, j]\n",
    "                    else:\n",
    "                        monomial_features[:, i] *= inputs[:, j] ** index\n",
    "        else:\n",
    "            for i, multiindex in enumerate(multiindices):\n",
    "                reduced_multiindex_input = multiindex[:n_input]\n",
    "                reduced_multiindex_func = multiindex[n_input:]\n",
    "                monomial_features[:, i] = monomial_features_dict[reduced_multiindex_input]\n",
    "                for j, index in enumerate(reduced_multiindex_func):\n",
    "                    if index == 0:\n",
    "                        continue\n",
    "                    elif index == 1:\n",
    "                        monomial_features[:, i] *= inputs[:, j + n_input]\n",
    "                    else:\n",
    "                        monomial_features[:, i] *= inputs[:, j + n_input] ** index\n",
    "        return monomial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dda87d67-66a6-4f11-afcd-c4e14d1dd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ParFamTorch(n_input=1, degree_input_polynomials=2, degree_output_polynomials=2, width=1,\n",
    "                    functions=functions, function_names=function_names, maximal_potence=2,\n",
    "                    degree_output_polynomials_specific=[1], enforce_function=False,\n",
    "                    degree_input_denominator=2, degree_output_denominator=2, normalize_denom=True,\n",
    "                    degree_output_polynomials_denominator_specific=[1], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "091676bd-6d91-40e5-bc2d-cacfcf11d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(500,2).to('cuda')\n",
    "multiindices = model.monomial_list_output\n",
    "monomial_features = torch.ones((inputs.shape[0], len(multiindices)), dtype=torch.float64, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "85f79185-a9c2-49b6-a48a-d2c8f59ca5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for j, multiindex in enumerate(multiindices):\n",
    "    for k, index in enumerate(multiindex):\n",
    "        indexes.append((j,k,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0498641e-cb2c-4aae-9a99-dc38c2ab650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_old():\n",
    "    for j, multiindex in enumerate(multiindices):\n",
    "        for k, index in enumerate(multiindex):\n",
    "            monomial_features[:, j] *= inputs[:,k] ** index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "53112a3c-c132-4fe3-b9fd-02983141a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 s  694 ns per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4bdc2abf-2c90-4902-98e9-fac9ecc48051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 1, 0),\n",
       " (1, 0, 0),\n",
       " (1, 1, 1),\n",
       " (2, 0, 1),\n",
       " (2, 1, 0),\n",
       " (3, 0, 1),\n",
       " (3, 1, 1),\n",
       " (4, 0, 2),\n",
       " (4, 1, 0)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c398a-1d9d-4aaa-9474-f715bd2da8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
