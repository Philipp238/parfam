INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file train.ini ###############
INFO:root:######## Starting with training set size 2000000 out of [2000000] now. ########
INFO:root:Creating the dataset took 176.049s.
INFO:root:###### Starting with 100 hidden neurons out of [100, 200, 500, 1000] now. ######
INFO:root:#### Starting with 100 batches out of [100, 500] now. ####
INFO:root:Batch size: 20000
INFO:root:[  100] Training loss: 0.19802951, Validation loss: 0.20723204
INFO:root:[  200] Training loss: 0.16419578, Validation loss: 0.16643338
INFO:root:[  300] Training loss: 0.14843162, Validation loss: 0.14575386
INFO:root:[  400] Training loss: 0.13873986, Validation loss: 0.14166804
INFO:root:[  500] Training loss: 0.13117290, Validation loss: 0.12901308
INFO:root:[  600] Training loss: 0.12626980, Validation loss: 0.12361888
INFO:root:[  700] Training loss: 0.12232253, Validation loss: 0.12115570
INFO:root:[  800] Training loss: 0.11793720, Validation loss: 0.11489116
INFO:root:[  900] Training loss: 0.11426591, Validation loss: 0.11487498
INFO:root:[ 1000] Training loss: 0.11069525, Validation loss: 0.10825432
INFO:root:[ 1100] Training loss: 0.10851128, Validation loss: 0.10768142
INFO:root:[ 1200] Training loss: 0.10560652, Validation loss: 0.10456668
INFO:root:[ 1300] Training loss: 0.10318768, Validation loss: 0.10851976
INFO:root:[ 1400] Training loss: 0.10268272, Validation loss: 0.10032795
INFO:root:[ 1500] Training loss: 0.09968500, Validation loss: 0.09791421
INFO:root:[ 1600] Training loss: 0.09925205, Validation loss: 0.09919325
INFO:root:[ 1700] Training loss: 0.09843306, Validation loss: 0.10052800
INFO:root:[ 1800] Training loss: 0.09531143, Validation loss: 0.10521881
INFO:root:[ 1900] Training loss: 0.09369923, Validation loss: 0.09263501
INFO:root:[ 2000] Training loss: 0.09401538, Validation loss: 0.09685825
INFO:root:[ 2100] Training loss: 0.09294098, Validation loss: 0.09052164
INFO:root:[ 2200] Training loss: 0.09177550, Validation loss: 0.09150231
INFO:root:[ 2300] Training loss: 0.09080882, Validation loss: 0.08651422
INFO:root:[ 2400] Training loss: 0.08936182, Validation loss: 0.08886442
INFO:root:[ 2500] Training loss: 0.08857194, Validation loss: 0.09584270
INFO:root:[ 2600] Training loss: 0.08774887, Validation loss: 0.09345374
INFO:root:[ 2700] Training loss: 0.08615051, Validation loss: 0.08346610
INFO:root:[ 2800] Training loss: 0.08792165, Validation loss: 0.08406738
INFO:root:[ 2900] Training loss: 0.08543099, Validation loss: 0.08624657
INFO:root:[ 3000] Training loss: 0.08575848, Validation loss: 0.08282408
INFO:root:[ 3100] Training loss: 0.08476873, Validation loss: 0.08078419
INFO:root:[ 3200] Training loss: 0.08437010, Validation loss: 0.08715966
INFO:root:[ 3300] Training loss: 0.08280945, Validation loss: 0.08783416
INFO:root:[ 3400] Training loss: 0.08333424, Validation loss: 0.08849414
INFO:root:[ 3500] Training loss: 0.08382228, Validation loss: 0.07923858
INFO:root:[ 3600] Training loss: 0.08256101, Validation loss: 0.09501635
INFO:root:[ 3700] Training loss: 0.08216910, Validation loss: 0.08184949
INFO:root:[ 3800] Training loss: 0.08195734, Validation loss: 0.07803918
INFO:root:[ 3900] Training loss: 0.08181955, Validation loss: 0.07989043
INFO:root:[ 4000] Training loss: 0.08011420, Validation loss: 0.07841846
INFO:root:[ 4100] Training loss: 0.08040931, Validation loss: 0.08593231
INFO:root:[ 4200] Training loss: 0.08091115, Validation loss: 0.10348041
INFO:root:[ 4300] Training loss: 0.08158701, Validation loss: 0.08213367
INFO:root:[ 4400] Training loss: 0.07963535, Validation loss: 0.07775313
INFO:root:[ 4500] Training loss: 0.07732092, Validation loss: 0.07701512
INFO:root:[ 4600] Training loss: 0.07886922, Validation loss: 0.08459809
INFO:root:[ 4700] Training loss: 0.07789385, Validation loss: 0.08491732
INFO:root:[ 4800] Training loss: 0.07772420, Validation loss: 0.09180029
INFO:root:[ 4900] Training loss: 0.07820692, Validation loss: 0.07891663
INFO:root:[ 5000] Training loss: 0.08099582, Validation loss: 0.08048439
INFO:root:[ 5100] Training loss: 0.07670821, Validation loss: 0.08020952
INFO:root:[ 5200] Training loss: 0.07802730, Validation loss: 0.08212077
INFO:root:[ 5300] Training loss: 0.07816862, Validation loss: 0.08863594
INFO:root:[ 5400] Training loss: 0.07891736, Validation loss: 0.07959804
INFO:root:[ 5500] Training loss: 0.07541210, Validation loss: 0.07250411
INFO:root:[ 5600] Training loss: 0.07614840, Validation loss: 0.07604927
INFO:root:[ 5700] Training loss: 0.07373985, Validation loss: 0.09092725
INFO:root:[ 5800] Training loss: 0.07518461, Validation loss: 0.07713928
INFO:root:[ 5900] Training loss: 0.07650622, Validation loss: 0.08248121
INFO:root:[ 6000] Training loss: 0.07456052, Validation loss: 0.07442762
INFO:root:[ 6100] Training loss: 0.07390031, Validation loss: 0.09824969
INFO:root:[ 6200] Training loss: 0.07304895, Validation loss: 0.08428461
INFO:root:[ 6300] Training loss: 0.07286117, Validation loss: 0.07264000
INFO:root:[ 6400] Training loss: 0.07388321, Validation loss: 0.07292072
INFO:root:[ 6500] Training loss: 0.07275653, Validation loss: 0.07221403
INFO:root:[ 6600] Training loss: 0.07337045, Validation loss: 0.08049460
INFO:root:[ 6700] Training loss: 0.07619282, Validation loss: 0.07494226
INFO:root:[ 6800] Training loss: 0.07125458, Validation loss: 0.08125050
INFO:root:[ 6900] Training loss: 0.07186664, Validation loss: 0.07755532
INFO:root:[ 7000] Training loss: 0.07606079, Validation loss: 0.07413793
INFO:root:[ 7100] Training loss: 0.07142839, Validation loss: 0.08363347
INFO:root:[ 7200] Training loss: 0.07311833, Validation loss: 0.07345207
INFO:root:[ 7300] Training loss: 0.07240218, Validation loss: 0.08281829
INFO:root:[ 7400] Training loss: 0.07217827, Validation loss: 0.07009129
INFO:root:[ 7500] Training loss: 0.07134289, Validation loss: 0.07710867
INFO:root:[ 7600] Training loss: 0.07024416, Validation loss: 0.07543904
INFO:root:[ 7700] Training loss: 0.07059300, Validation loss: 0.07026401
INFO:root:[ 7800] Training loss: 0.07080217, Validation loss: 0.08139354
INFO:root:[ 7900] Training loss: 0.06950145, Validation loss: 0.06689135
INFO:root:[ 8000] Training loss: 0.07011649, Validation loss: 0.07587809
INFO:root:[ 8100] Training loss: 0.07096517, Validation loss: 0.06748310
INFO:root:[ 8200] Training loss: 0.07028588, Validation loss: 0.06839039
INFO:root:[ 8300] Training loss: 0.06838178, Validation loss: 0.06840823
INFO:root:[ 8400] Training loss: 0.06908372, Validation loss: 0.07309606
INFO:root:[ 8500] Training loss: 0.07153307, Validation loss: 0.07000247
INFO:root:[ 8600] Training loss: 0.06843247, Validation loss: 0.08653302
INFO:root:[ 8700] Training loss: 0.07280199, Validation loss: 0.07044479
INFO:root:[ 8800] Training loss: 0.06838732, Validation loss: 0.07984343
INFO:root:[ 8900] Training loss: 0.06735530, Validation loss: 0.06397860
INFO:root:[ 9000] Training loss: 0.06834177, Validation loss: 0.07312850
INFO:root:[ 9100] Training loss: 0.06815439, Validation loss: 0.08809508
INFO:root:[ 9200] Training loss: 0.06818934, Validation loss: 0.06961279
INFO:root:[ 9300] Training loss: 0.06860913, Validation loss: 0.07229418
INFO:root:[ 9400] Training loss: 0.06680180, Validation loss: 0.09130029
INFO:root:[ 9500] Training loss: 0.06750396, Validation loss: 0.07025529
INFO:root:[ 9600] Training loss: 0.06894441, Validation loss: 0.11595259
INFO:root:[ 9700] Training loss: 0.06785627, Validation loss: 0.06941577
INFO:root:[ 9800] Training loss: 0.06677766, Validation loss: 0.07963371
INFO:root:[ 9900] Training loss: 0.06713609, Validation loss: 0.07201494
INFO:root:[10000] Training loss: 0.06603071, Validation loss: 0.06679659
INFO:root:[10100] Training loss: 0.06687747, Validation loss: 0.06788667
INFO:root:[10200] Training loss: 0.06721642, Validation loss: 0.06508108
INFO:root:[10300] Training loss: 0.06680122, Validation loss: 0.06518907
INFO:root:[10400] Training loss: 0.06572920, Validation loss: 0.08220617
INFO:root:[10500] Training loss: 0.06602671, Validation loss: 0.06508663
INFO:root:[10600] Training loss: 0.06484297, Validation loss: 0.06785509
INFO:root:[10700] Training loss: 0.06764970, Validation loss: 0.06696102
INFO:root:[10800] Training loss: 0.06559110, Validation loss: 0.07259749
INFO:root:[10900] Training loss: 0.06535148, Validation loss: 0.06242697
INFO:root:[11000] Training loss: 0.06445817, Validation loss: 0.08211463
INFO:root:[11100] Training loss: 0.06546002, Validation loss: 0.07234281
INFO:root:[11200] Training loss: 0.06529724, Validation loss: 0.06146986
INFO:root:[11300] Training loss: 0.06487539, Validation loss: 0.06589802
INFO:root:[11400] Training loss: 0.06414059, Validation loss: 0.10081792
INFO:root:[11500] Training loss: 0.06483427, Validation loss: 0.06506201
INFO:root:[11600] Training loss: 0.06605584, Validation loss: 0.07968853
INFO:root:[11700] Training loss: 0.06518541, Validation loss: 0.06936233
INFO:root:[11800] Training loss: 0.06518182, Validation loss: 0.07028265
INFO:root:[11900] Training loss: 0.06450244, Validation loss: 0.05984256
INFO:root:[12000] Training loss: 0.06410147, Validation loss: 0.06084464
INFO:root:[12100] Training loss: 0.06773144, Validation loss: 0.06660383
INFO:root:[12200] Training loss: 0.06285889, Validation loss: 0.05907709
INFO:root:[12300] Training loss: 0.06414987, Validation loss: 0.06234964
INFO:root:[12400] Training loss: 0.06286246, Validation loss: 0.06198208
INFO:root:[12500] Training loss: 0.06379802, Validation loss: 0.07206856
INFO:root:[12600] Training loss: 0.06388509, Validation loss: 0.06240168
INFO:root:[12700] Training loss: 0.06364016, Validation loss: 0.06061743
INFO:root:[12800] Training loss: 0.06304220, Validation loss: 0.06608873
INFO:root:[12900] Training loss: 0.06349288, Validation loss: 0.06729324
INFO:root:[13000] Training loss: 0.06449538, Validation loss: 0.06253990
INFO:root:[13100] Training loss: 0.06376338, Validation loss: 0.06267082
INFO:root:[13200] Training loss: 0.06456892, Validation loss: 0.06904481
INFO:root:[13300] Training loss: 0.06367788, Validation loss: 0.06519574
INFO:root:[13400] Training loss: 0.06344861, Validation loss: 0.06456746
INFO:root:[13500] Training loss: 0.06239337, Validation loss: 0.05802321
INFO:root:[13600] Training loss: 0.06315288, Validation loss: 0.07635234
INFO:root:[13700] Training loss: 0.06829953, Validation loss: 0.06050203
INFO:root:[13800] Training loss: 0.06307124, Validation loss: 0.06041074
INFO:root:[13900] Training loss: 0.06234470, Validation loss: 0.06033381
INFO:root:[14000] Training loss: 0.06280531, Validation loss: 0.06512200
INFO:root:[14100] Training loss: 0.06230647, Validation loss: 0.05908052
INFO:root:[14200] Training loss: 0.06186681, Validation loss: 0.07339028
INFO:root:[14300] Training loss: 0.06307976, Validation loss: 0.05741737
INFO:root:[14400] Training loss: 0.06213660, Validation loss: 0.06045176
INFO:root:[14500] Training loss: 0.06098013, Validation loss: 0.06264520
INFO:root:[14600] Training loss: 0.06331380, Validation loss: 0.05865051
INFO:root:[14700] Training loss: 0.06298255, Validation loss: 0.06851859
INFO:root:[14800] Training loss: 0.06212775, Validation loss: 0.05860047
INFO:root:[14900] Training loss: 0.06206001, Validation loss: 0.06623925
INFO:root:[15000] Training loss: 0.06404511, Validation loss: 0.06483177
INFO:root:[15100] Training loss: 0.06201794, Validation loss: 0.05720204
INFO:root:[15200] Training loss: 0.06077942, Validation loss: 0.06153865
INFO:root:[15300] Training loss: 0.06310674, Validation loss: 0.06714564
INFO:root:[15400] Training loss: 0.06026000, Validation loss: 0.05618817
INFO:root:[15500] Training loss: 0.06221893, Validation loss: 0.06302242
INFO:root:[15600] Training loss: 0.06107286, Validation loss: 0.05949935
INFO:root:[15700] Training loss: 0.06008866, Validation loss: 0.06965564
INFO:root:[15800] Training loss: 0.06001631, Validation loss: 0.05736332
INFO:root:[15900] Training loss: 0.06032219, Validation loss: 0.05686498
INFO:root:[16000] Training loss: 0.06050655, Validation loss: 0.06118298
INFO:root:[16100] Training loss: 0.06171196, Validation loss: 0.05677868
INFO:root:[16200] Training loss: 0.05920854, Validation loss: 0.05787691
INFO:root:[16300] Training loss: 0.06015451, Validation loss: 0.05685936
INFO:root:[16400] Training loss: 0.05849668, Validation loss: 0.06002948
INFO:root:[16500] Training loss: 0.06027829, Validation loss: 0.08799857
INFO:root:[16600] Training loss: 0.05881491, Validation loss: 0.05732444
INFO:root:[16700] Training loss: 0.06201486, Validation loss: 0.05760460
INFO:root:[16800] Training loss: 0.05959701, Validation loss: 0.05715965
INFO:root:[16900] Training loss: 0.06173760, Validation loss: 0.06453639
INFO:root:[17000] Training loss: 0.06030318, Validation loss: 0.08368875
INFO:root:[17100] Training loss: 0.05937412, Validation loss: 0.05676866
INFO:root:[17200] Training loss: 0.05964734, Validation loss: 0.06037930
INFO:root:[17300] Training loss: 0.05734405, Validation loss: 0.06104653
INFO:root:[17400] Training loss: 0.05921209, Validation loss: 0.07408136
INFO:root:[17500] Training loss: 0.05921548, Validation loss: 0.05722116
INFO:root:[17600] Training loss: 0.06398367, Validation loss: 0.06473840
INFO:root:[17700] Training loss: 0.05936443, Validation loss: 0.07934339
INFO:root:[17800] Training loss: 0.05849479, Validation loss: 0.06531726
INFO:root:[17900] Training loss: 0.05964142, Validation loss: 0.05925947
INFO:root:[18000] Training loss: 0.05978460, Validation loss: 0.05964588
INFO:root:[18100] Training loss: 0.06171461, Validation loss: 0.05935309
INFO:root:[18200] Training loss: 0.05891236, Validation loss: 0.05657573
INFO:root:[18300] Training loss: 0.05833708, Validation loss: 0.06225907
INFO:root:[18400] Training loss: 0.05881580, Validation loss: 0.06986178
INFO:root:[18500] Training loss: 0.06044306, Validation loss: 0.05662576
INFO:root:[18600] Training loss: 0.05955164, Validation loss: 0.06179042
INFO:root:[18700] Training loss: 0.05756373, Validation loss: 0.06207802
INFO:root:[18800] Training loss: 0.05818651, Validation loss: 0.08650596
INFO:root:[18900] Training loss: 0.05881250, Validation loss: 0.07633945
INFO:root:[19000] Training loss: 0.05758778, Validation loss: 0.07981546
INFO:root:[19100] Training loss: 0.05919128, Validation loss: 0.06364469
INFO:root:[19200] Training loss: 0.05756952, Validation loss: 0.07037975
INFO:root:[19300] Training loss: 0.05785231, Validation loss: 0.06202304
INFO:root:[19400] Training loss: 0.05977200, Validation loss: 0.06108799
INFO:root:[19500] Training loss: 0.05811412, Validation loss: 0.05630397
INFO:root:[19600] Training loss: 0.05856065, Validation loss: 0.06118548
INFO:root:[19700] Training loss: 0.05725826, Validation loss: 0.06737599
INFO:root:[19800] Training loss: 0.05819733, Validation loss: 0.07673459
INFO:root:[19900] Training loss: 0.05790667, Validation loss: 0.07208003
INFO:root:[20000] Training loss: 0.06100558, Validation loss: 0.06748749
INFO:root:Training the model took 3803.284s.
INFO:root:Precision score (training data): 0.97567
INFO:root:Recall score (training data): 0.94786
INFO:root:F1 score (training data): 0.96156
INFO:root:Precision score (test data): 0.97489
INFO:root:Recall score (test data): 0.94565
INFO:root:F1 score (test data): 0.96005
INFO:root:Covering score (training data) (quantile cutoff): 0.96539
INFO:root:Complete cover size (training data) (quantile cutoff): 0.29744152289087405
INFO:root:Covering score (test data) (quantile cutoff): 0.9619
INFO:root:Complete cover size (test data) (quantile cutoff): 0.29719825345670026
INFO:root:Covering score (training data) (0.5 cutoff): 0.89286
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.24048413308185793
INFO:root:Covering score (test data) (0.5 cutoff): 0.8871
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.23998609702025325
INFO:root:MSE (test data): 0.01593
INFO:root:BCE (test data): 0.06453
INFO:root:#### Starting with 500 batches out of [100, 500] now. ####
INFO:root:Batch size: 4000
INFO:root:[  100] Training loss: 0.14942773, Validation loss: 0.14781244
INFO:root:[  200] Training loss: 0.12735472, Validation loss: 0.12386737
INFO:root:[  300] Training loss: 0.11469780, Validation loss: 0.11012858
INFO:root:[  400] Training loss: 0.10661529, Validation loss: 0.11076665
INFO:root:[  500] Training loss: 0.10074698, Validation loss: 0.09585790
INFO:root:[  600] Training loss: 0.09587698, Validation loss: 0.09171443
INFO:root:[  700] Training loss: 0.09177143, Validation loss: 0.09254251
INFO:root:[  800] Training loss: 0.08784759, Validation loss: 0.08601691
INFO:root:[  900] Training loss: 0.08573459, Validation loss: 0.09746367
INFO:root:[ 1000] Training loss: 0.08294577, Validation loss: 0.08611576
INFO:root:[ 1100] Training loss: 0.08093220, Validation loss: 0.08401480
INFO:root:[ 1200] Training loss: 0.07870032, Validation loss: 0.08138149
INFO:root:[ 1300] Training loss: 0.07774595, Validation loss: 0.07957081
INFO:root:[ 1400] Training loss: 0.07610779, Validation loss: 0.08185355
INFO:root:[ 1500] Training loss: 0.07562776, Validation loss: 0.07513110
INFO:root:[ 1600] Training loss: 0.07375963, Validation loss: 0.08114853
INFO:root:[ 1700] Training loss: 0.07286344, Validation loss: 0.08739471
INFO:root:[ 1800] Training loss: 0.07192284, Validation loss: 0.07269800
INFO:root:[ 1900] Training loss: 0.07106501, Validation loss: 0.07160392
INFO:root:[ 2000] Training loss: 0.07027159, Validation loss: 0.07297210
INFO:root:[ 2100] Training loss: 0.06945891, Validation loss: 0.07176115
INFO:root:[ 2200] Training loss: 0.06913285, Validation loss: 0.07118667
INFO:root:[ 2300] Training loss: 0.06849141, Validation loss: 0.07516022
INFO:root:[ 2400] Training loss: 0.06734413, Validation loss: 0.06610639
INFO:root:[ 2500] Training loss: 0.06691542, Validation loss: 0.06567078
INFO:root:[ 2600] Training loss: 0.06630740, Validation loss: 0.06839913
INFO:root:[ 2700] Training loss: 0.06528547, Validation loss: 0.07355864
INFO:root:[ 2800] Training loss: 0.06488315, Validation loss: 0.06688072
INFO:root:[ 2900] Training loss: 0.06440553, Validation loss: 0.06333978
INFO:root:[ 3000] Training loss: 0.06386902, Validation loss: 0.06346396
INFO:root:[ 3100] Training loss: 0.06356643, Validation loss: 0.06861480
INFO:root:[ 3200] Training loss: 0.06320669, Validation loss: 0.07236895
INFO:root:[ 3300] Training loss: 0.06318087, Validation loss: 0.06681928
INFO:root:[ 3400] Training loss: 0.06346389, Validation loss: 0.06632084
INFO:root:[ 3500] Training loss: 0.06260140, Validation loss: 0.06220711
INFO:root:[ 3600] Training loss: 0.06235185, Validation loss: 0.06727549
INFO:root:[ 3700] Training loss: 0.06162502, Validation loss: 0.06317665
INFO:root:[ 3800] Training loss: 0.06148893, Validation loss: 0.05963590
INFO:root:[ 3900] Training loss: 0.06166563, Validation loss: 0.06264810
INFO:root:[ 4000] Training loss: 0.06115781, Validation loss: 0.06128092
INFO:root:[ 4100] Training loss: 0.06093927, Validation loss: 0.05879325
INFO:root:[ 4200] Training loss: 0.06057553, Validation loss: 0.06565637
INFO:root:[ 4300] Training loss: 0.05998929, Validation loss: 0.05777148
INFO:root:[ 4400] Training loss: 0.06028061, Validation loss: 0.05896970
INFO:root:[ 4500] Training loss: 0.05974293, Validation loss: 0.05881073
INFO:root:[ 4600] Training loss: 0.05965272, Validation loss: 0.05680181
INFO:root:[ 4700] Training loss: 0.05946564, Validation loss: 0.05865189
INFO:root:[ 4800] Training loss: 0.05924905, Validation loss: 0.06064159
INFO:root:[ 4900] Training loss: 0.05911121, Validation loss: 0.07033411
INFO:root:[ 5000] Training loss: 0.05927676, Validation loss: 0.05941675
INFO:root:[ 5100] Training loss: 0.05873388, Validation loss: 0.05712943
INFO:root:[ 5200] Training loss: 0.05844252, Validation loss: 0.05459951
INFO:root:[ 5300] Training loss: 0.05852692, Validation loss: 0.05893571
INFO:root:[ 5400] Training loss: 0.05887908, Validation loss: 0.06463602
INFO:root:[ 5500] Training loss: 0.05819154, Validation loss: 0.06050848
INFO:root:[ 5600] Training loss: 0.05789014, Validation loss: 0.05689752
INFO:root:[ 5700] Training loss: 0.05783302, Validation loss: 0.05683344
INFO:root:[ 5800] Training loss: 0.05777035, Validation loss: 0.07061442
INFO:root:[ 5900] Training loss: 0.05755293, Validation loss: 0.07226535
INFO:root:[ 6000] Training loss: 0.05722947, Validation loss: 0.05982195
INFO:root:[ 6100] Training loss: 0.05721038, Validation loss: 0.11513586
INFO:root:[ 6200] Training loss: 0.05717678, Validation loss: 0.05611315
INFO:root:[ 6300] Training loss: 0.05688134, Validation loss: 0.05621061
INFO:root:[ 6400] Training loss: 0.05653995, Validation loss: 0.05528338
INFO:root:[ 6500] Training loss: 0.05630373, Validation loss: 0.05705930
INFO:root:[ 6600] Training loss: 0.05591841, Validation loss: 0.06253600
INFO:root:[ 6700] Training loss: 0.05634184, Validation loss: 0.06159516
INFO:root:[ 6800] Training loss: 0.05615093, Validation loss: 0.05822730
INFO:root:[ 6900] Training loss: 0.05598607, Validation loss: 0.05694647
INFO:root:[ 7000] Training loss: 0.05591001, Validation loss: 0.05343905
INFO:root:[ 7100] Training loss: 0.05585882, Validation loss: 0.05494150
INFO:root:[ 7200] Training loss: 0.05581883, Validation loss: 0.05897900
INFO:root:[ 7300] Training loss: 0.05505811, Validation loss: 0.05535498
INFO:root:[ 7400] Training loss: 0.05586431, Validation loss: 0.07266538
INFO:root:[ 7500] Training loss: 0.05560917, Validation loss: 0.05386238
INFO:root:[ 7600] Training loss: 0.05492859, Validation loss: 0.05542147
INFO:root:[ 7700] Training loss: 0.05461964, Validation loss: 0.05554180
INFO:root:[ 7800] Training loss: 0.05471395, Validation loss: 0.05577137
INFO:root:[ 7900] Training loss: 0.05434904, Validation loss: 0.05469058
INFO:root:[ 8000] Training loss: 0.05461695, Validation loss: 0.06101774
INFO:root:[ 8100] Training loss: 0.05534560, Validation loss: 0.05438307
INFO:root:[ 8200] Training loss: 0.05448418, Validation loss: 0.05293955
INFO:root:[ 8300] Training loss: 0.05413361, Validation loss: 0.05699460
INFO:root:[ 8400] Training loss: 0.05386568, Validation loss: 0.05503877
INFO:root:[ 8500] Training loss: 0.05389314, Validation loss: 0.06258292
INFO:root:[ 8600] Training loss: 0.05375416, Validation loss: 0.05488231
INFO:root:[ 8700] Training loss: 0.05329952, Validation loss: 0.05519047
INFO:root:[ 8800] Training loss: 0.05392117, Validation loss: 0.06177428
INFO:root:[ 8900] Training loss: 0.05328540, Validation loss: 0.05248806
INFO:root:[ 9000] Training loss: 0.05456688, Validation loss: 0.05590756
INFO:root:[ 9100] Training loss: 0.05342648, Validation loss: 0.05566818
INFO:root:[ 9200] Training loss: 0.05355828, Validation loss: 0.05383275
INFO:root:[ 9300] Training loss: 0.05299951, Validation loss: 0.05875507
INFO:root:[ 9400] Training loss: 0.05322252, Validation loss: 0.05415398
INFO:root:[ 9500] Training loss: 0.05377689, Validation loss: 0.05114383
INFO:root:[ 9600] Training loss: 0.05431148, Validation loss: 0.05393791
INFO:root:[ 9700] Training loss: 0.05401284, Validation loss: 0.05227517
INFO:root:[ 9800] Training loss: 0.05302205, Validation loss: 0.05141833
INFO:root:[ 9900] Training loss: 0.05233030, Validation loss: 0.05487622
INFO:root:[10000] Training loss: 0.05246249, Validation loss: 0.05249969
INFO:root:[10100] Training loss: 0.05243264, Validation loss: 0.05188332
INFO:root:[10200] Training loss: 0.05302576, Validation loss: 0.05309631
INFO:root:[10300] Training loss: 0.05209568, Validation loss: 0.05205624
INFO:root:[10400] Training loss: 0.05385505, Validation loss: 0.06006242
INFO:root:[10500] Training loss: 0.05198355, Validation loss: 0.05406813
INFO:root:[10600] Training loss: 0.05214303, Validation loss: 0.05386984
INFO:root:[10700] Training loss: 0.05300700, Validation loss: 0.06354481
INFO:root:[10800] Training loss: 0.05224823, Validation loss: 0.05968315
INFO:root:[10900] Training loss: 0.05190897, Validation loss: 0.05408192
INFO:root:[11000] Training loss: 0.05159364, Validation loss: 0.06293137
INFO:root:[11100] Training loss: 0.05216010, Validation loss: 0.05298816
INFO:root:[11200] Training loss: 0.05219509, Validation loss: 0.05276267
INFO:root:[11300] Training loss: 0.05150543, Validation loss: 0.05581185
INFO:root:[11400] Training loss: 0.05122988, Validation loss: 0.04924952
INFO:root:[11500] Training loss: 0.05133751, Validation loss: 0.05596862
INFO:root:[11600] Training loss: 0.05220050, Validation loss: 0.05073666
INFO:root:[11700] Training loss: 0.05094470, Validation loss: 0.05203428
INFO:root:[11800] Training loss: 0.05126532, Validation loss: 0.05601207
INFO:root:[11900] Training loss: 0.05085934, Validation loss: 0.05143506
INFO:root:[12000] Training loss: 0.05129999, Validation loss: 0.05289702
INFO:root:[12100] Training loss: 0.05063104, Validation loss: 0.06050824
INFO:root:[12200] Training loss: 0.05076801, Validation loss: 0.05023864
INFO:root:[12300] Training loss: 0.05077463, Validation loss: 0.05370782
INFO:root:[12400] Training loss: 0.05171484, Validation loss: 0.05363410
INFO:root:[12500] Training loss: 0.05075267, Validation loss: 0.05524071
INFO:root:[12600] Training loss: 0.05067875, Validation loss: 0.05394534
INFO:root:[12700] Training loss: 0.05048517, Validation loss: 0.06200318
INFO:root:[12800] Training loss: 0.05028124, Validation loss: 0.05511030
INFO:root:[12900] Training loss: 0.05077579, Validation loss: 0.05086692
INFO:root:[13000] Training loss: 0.05010825, Validation loss: 0.05100294
INFO:root:[13100] Training loss: 0.05066249, Validation loss: 0.05403986
INFO:root:[13200] Training loss: 0.05035424, Validation loss: 0.04986950
INFO:root:[13300] Training loss: 0.05004992, Validation loss: 0.05210359
INFO:root:[13400] Training loss: 0.05022684, Validation loss: 0.04896092
INFO:root:[13500] Training loss: 0.05011088, Validation loss: 0.04967503
INFO:root:[13600] Training loss: 0.05125948, Validation loss: 0.05173365
INFO:root:[13700] Training loss: 0.05024500, Validation loss: 0.04906626
INFO:root:[13800] Training loss: 0.05002812, Validation loss: 0.04688452
INFO:root:[13900] Training loss: 0.04978404, Validation loss: 0.05552758
INFO:root:[14000] Training loss: 0.05046077, Validation loss: 0.04854075
INFO:root:[14100] Training loss: 0.04959701, Validation loss: 0.05035161
INFO:root:[14200] Training loss: 0.04946236, Validation loss: 0.04900977
INFO:root:[14300] Training loss: 0.04953206, Validation loss: 0.04908452
INFO:root:[14400] Training loss: 0.04991929, Validation loss: 0.04945695
INFO:root:[14500] Training loss: 0.05008274, Validation loss: 0.04755278
INFO:root:[14600] Training loss: 0.04936306, Validation loss: 0.05667048
INFO:root:[14700] Training loss: 0.04958077, Validation loss: 0.05076006
INFO:root:[14800] Training loss: 0.04931355, Validation loss: 0.05415470
INFO:root:[14900] Training loss: 0.04929765, Validation loss: 0.05767480
INFO:root:[15000] Training loss: 0.04959778, Validation loss: 0.05165574
INFO:root:[15100] Training loss: 0.04924603, Validation loss: 0.05020749
INFO:root:[15200] Training loss: 0.04927724, Validation loss: 0.06116826
INFO:root:[15300] Training loss: 0.05156453, Validation loss: 0.05997208
INFO:root:[15400] Training loss: 0.04897783, Validation loss: 0.04877437
INFO:root:[15500] Training loss: 0.04988894, Validation loss: 0.04795813
INFO:root:[15600] Training loss: 0.04913850, Validation loss: 0.05051288
INFO:root:[15700] Training loss: 0.04895956, Validation loss: 0.05109011
INFO:root:[15800] Training loss: 0.04931216, Validation loss: 0.04778099
INFO:root:[15900] Training loss: 0.04961441, Validation loss: 0.05281056
INFO:root:[16000] Training loss: 0.04890159, Validation loss: 0.04832182
INFO:root:[16100] Training loss: 0.04848167, Validation loss: 0.04688213
INFO:root:[16200] Training loss: 0.04941414, Validation loss: 0.05474781
INFO:root:[16300] Training loss: 0.04929509, Validation loss: 0.04819424
INFO:root:[16400] Training loss: 0.05050126, Validation loss: 0.04886153
INFO:root:[16500] Training loss: 0.04884372, Validation loss: 0.04968737
INFO:root:[16600] Training loss: 0.04869825, Validation loss: 0.05154687
INFO:root:[16700] Training loss: 0.04887230, Validation loss: 0.04981766
INFO:root:[16800] Training loss: 0.04831574, Validation loss: 0.06050048
INFO:root:[16900] Training loss: 0.04785127, Validation loss: 0.04933656
INFO:root:[17000] Training loss: 0.04925373, Validation loss: 0.04926578
INFO:root:[17100] Training loss: 0.04880473, Validation loss: 0.05080043
INFO:root:[17200] Training loss: 0.04809269, Validation loss: 0.04709810
INFO:root:[17300] Training loss: 0.04835024, Validation loss: 0.04681912
INFO:root:[17400] Training loss: 0.04839602, Validation loss: 0.04855199
INFO:root:[17500] Training loss: 0.04841129, Validation loss: 0.05389709
INFO:root:[17600] Training loss: 0.04804138, Validation loss: 0.04740145
INFO:root:[17700] Training loss: 0.04794654, Validation loss: 0.04628717
INFO:root:[17800] Training loss: 0.04851777, Validation loss: 0.04832368
INFO:root:[17900] Training loss: 0.04771219, Validation loss: 0.05152670
INFO:root:[18000] Training loss: 0.04794300, Validation loss: 0.04843800
INFO:root:[18100] Training loss: 0.04772977, Validation loss: 0.05009311
INFO:root:[18200] Training loss: 0.04790853, Validation loss: 0.05074061
INFO:root:[18300] Training loss: 0.04809144, Validation loss: 0.05878194
INFO:root:[18400] Training loss: 0.04787552, Validation loss: 0.04768129
INFO:root:[18500] Training loss: 0.04832355, Validation loss: 0.05240978
INFO:root:[18600] Training loss: 0.04978615, Validation loss: 0.04824189
INFO:root:[18700] Training loss: 0.04745884, Validation loss: 0.04858609
INFO:root:[18800] Training loss: 0.04871256, Validation loss: 0.04958492
INFO:root:[18900] Training loss: 0.04761002, Validation loss: 0.04711737
INFO:root:[19000] Training loss: 0.04802198, Validation loss: 0.04661355
INFO:root:[19100] Training loss: 0.04735257, Validation loss: 0.04981055
INFO:root:[19200] Training loss: 0.04786075, Validation loss: 0.04731042
INFO:root:[19300] Training loss: 0.04834342, Validation loss: 0.05305601
INFO:root:[19400] Training loss: 0.04759153, Validation loss: 0.04567081
INFO:root:[19500] Training loss: 0.04879525, Validation loss: 0.04729024
INFO:root:[19600] Training loss: 0.04703745, Validation loss: 0.04748323
INFO:root:[19700] Training loss: 0.04723411, Validation loss: 0.04711150
INFO:root:[19800] Training loss: 0.04880793, Validation loss: 0.04757219
INFO:root:[19900] Training loss: 0.04717144, Validation loss: 0.04858499
INFO:root:[20000] Training loss: 0.04702800, Validation loss: 0.05202462
INFO:root:Training the model took 13638.845s.
INFO:root:Precision score (training data): 0.98244
INFO:root:Recall score (training data): 0.96111
INFO:root:F1 score (training data): 0.97166
INFO:root:Precision score (test data): 0.98174
INFO:root:Recall score (test data): 0.96052
INFO:root:F1 score (test data): 0.97101
INFO:root:Covering score (training data) (quantile cutoff): 0.97863
INFO:root:Complete cover size (training data) (quantile cutoff): 0.29831006791098613
INFO:root:Covering score (test data) (quantile cutoff): 0.9767
INFO:root:Complete cover size (test data) (quantile cutoff): 0.2982577386437323
INFO:root:Covering score (training data) (0.5 cutoff): 0.92085
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.2425835399808402
INFO:root:Covering score (test data) (0.5 cutoff): 0.9176
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.24283456843940715
INFO:root:MSE (test data): 0.01182
INFO:root:BCE (test data): 0.05
INFO:root:###### Starting with 200 hidden neurons out of [100, 200, 500, 1000] now. ######
INFO:root:#### Starting with 100 batches out of [100, 500] now. ####
INFO:root:Batch size: 20000
INFO:root:[  100] Training loss: 0.17394822, Validation loss: 0.17398474
INFO:root:[  200] Training loss: 0.14181456, Validation loss: 0.13964103
INFO:root:[  300] Training loss: 0.12768937, Validation loss: 0.12799647
INFO:root:[  400] Training loss: 0.11835085, Validation loss: 0.11814975
INFO:root:[  500] Training loss: 0.11172696, Validation loss: 0.10972949
INFO:root:[  600] Training loss: 0.10593232, Validation loss: 0.11477403
INFO:root:[  700] Training loss: 0.10199688, Validation loss: 0.09877872
INFO:root:[  800] Training loss: 0.09741259, Validation loss: 0.10418133
INFO:root:[  900] Training loss: 0.09535280, Validation loss: 0.10106426
INFO:root:[ 1000] Training loss: 0.09213737, Validation loss: 0.09127072
INFO:root:[ 1100] Training loss: 0.09004786, Validation loss: 0.08724445
INFO:root:[ 1200] Training loss: 0.08815561, Validation loss: 0.08937956
INFO:root:[ 1300] Training loss: 0.08603099, Validation loss: 0.08593911
INFO:root:[ 1400] Training loss: 0.08499826, Validation loss: 0.08305307
INFO:root:[ 1500] Training loss: 0.08310891, Validation loss: 0.11806598
INFO:root:[ 1600] Training loss: 0.08203711, Validation loss: 0.08128032
INFO:root:[ 1700] Training loss: 0.08007169, Validation loss: 0.09259298
INFO:root:[ 1800] Training loss: 0.08041294, Validation loss: 0.08046791
INFO:root:[ 1900] Training loss: 0.07880344, Validation loss: 0.08803234
INFO:root:[ 2000] Training loss: 0.07677832, Validation loss: 0.08412656
INFO:root:[ 2100] Training loss: 0.07720245, Validation loss: 0.07729051
INFO:root:[ 2200] Training loss: 0.07673934, Validation loss: 0.07903292
INFO:root:[ 2300] Training loss: 0.07555132, Validation loss: 0.10759292
INFO:root:[ 2400] Training loss: 0.07307696, Validation loss: 0.07426640
INFO:root:[ 2500] Training loss: 0.07348962, Validation loss: 0.09877011
INFO:root:[ 2600] Training loss: 0.07170609, Validation loss: 0.08214476
INFO:root:[ 2700] Training loss: 0.07051482, Validation loss: 0.07409323
INFO:root:[ 2800] Training loss: 0.07104769, Validation loss: 0.07356123
INFO:root:[ 2900] Training loss: 0.07006426, Validation loss: 0.07050049
INFO:root:[ 3000] Training loss: 0.06992687, Validation loss: 0.07592717
INFO:root:[ 3100] Training loss: 0.06906120, Validation loss: 0.07029433
INFO:root:[ 3200] Training loss: 0.06873488, Validation loss: 0.07112925
INFO:root:[ 3300] Training loss: 0.06746800, Validation loss: 0.07031231
INFO:root:[ 3400] Training loss: 0.06835355, Validation loss: 0.07332881
INFO:root:[ 3500] Training loss: 0.06740304, Validation loss: 0.07150576
INFO:root:[ 3600] Training loss: 0.06588930, Validation loss: 0.07563205
INFO:root:[ 3700] Training loss: 0.06495151, Validation loss: 0.06788649
INFO:root:[ 3800] Training loss: 0.06524483, Validation loss: 0.06532522
INFO:root:[ 3900] Training loss: 0.06551814, Validation loss: 0.06775257
INFO:root:[ 4000] Training loss: 0.06539072, Validation loss: 0.08973955
INFO:root:[ 4100] Training loss: 0.06417996, Validation loss: 0.06932781
INFO:root:[ 4200] Training loss: 0.06597341, Validation loss: 0.06463453
INFO:root:[ 4300] Training loss: 0.06470491, Validation loss: 0.07025824
INFO:root:[ 4400] Training loss: 0.06378847, Validation loss: 0.06559914
INFO:root:[ 4500] Training loss: 0.06322293, Validation loss: 0.07316560
INFO:root:[ 4600] Training loss: 0.06365265, Validation loss: 0.06563700
INFO:root:[ 4700] Training loss: 0.06444388, Validation loss: 0.06586640
INFO:root:[ 4800] Training loss: 0.06271198, Validation loss: 0.07371017
INFO:root:[ 4900] Training loss: 0.06263738, Validation loss: 0.06564934
INFO:root:[ 5000] Training loss: 0.06353066, Validation loss: 0.06503838
INFO:root:[ 5100] Training loss: 0.06044208, Validation loss: 0.06184575
INFO:root:[ 5200] Training loss: 0.06145763, Validation loss: 0.06510528
INFO:root:[ 5300] Training loss: 0.06128875, Validation loss: 0.06578617
INFO:root:[ 5400] Training loss: 0.06024539, Validation loss: 0.06610528
INFO:root:[ 5500] Training loss: 0.06120889, Validation loss: 0.06864730
INFO:root:[ 5600] Training loss: 0.06171750, Validation loss: 0.06161614
INFO:root:[ 5700] Training loss: 0.06202018, Validation loss: 0.07358481
INFO:root:[ 5800] Training loss: 0.06102202, Validation loss: 0.06262156
INFO:root:[ 5900] Training loss: 0.05919840, Validation loss: 0.09068851
INFO:root:[ 6000] Training loss: 0.06053078, Validation loss: 0.07179476
INFO:root:[ 6100] Training loss: 0.06213207, Validation loss: 0.06215543
INFO:root:[ 6200] Training loss: 0.06223908, Validation loss: 0.07734121
INFO:root:[ 6300] Training loss: 0.05948743, Validation loss: 0.05991895
INFO:root:[ 6400] Training loss: 0.05895148, Validation loss: 0.06581530
INFO:root:[ 6500] Training loss: 0.05970279, Validation loss: 0.06894247
INFO:root:[ 6600] Training loss: 0.05771335, Validation loss: 0.06835602
INFO:root:[ 6700] Training loss: 0.05911573, Validation loss: 0.07964370
INFO:root:[ 6800] Training loss: 0.05996167, Validation loss: 0.06062738
INFO:root:[ 6900] Training loss: 0.05803148, Validation loss: 0.06060654
INFO:root:[ 7000] Training loss: 0.05805195, Validation loss: 0.05960333
INFO:root:[ 7100] Training loss: 0.05860314, Validation loss: 0.06116810
INFO:root:[ 7200] Training loss: 0.05731574, Validation loss: 0.05924402
INFO:root:[ 7300] Training loss: 0.05660393, Validation loss: 0.05982144
INFO:root:[ 7400] Training loss: 0.05699591, Validation loss: 0.05833142
INFO:root:[ 7500] Training loss: 0.05833563, Validation loss: 0.06425399
INFO:root:[ 7600] Training loss: 0.05702899, Validation loss: 0.05695950
INFO:root:[ 7700] Training loss: 0.05884949, Validation loss: 0.11879829
INFO:root:[ 7800] Training loss: 0.05640578, Validation loss: 0.06374060
INFO:root:[ 7900] Training loss: 0.05804471, Validation loss: 0.05635483
INFO:root:[ 8000] Training loss: 0.05765732, Validation loss: 0.05852367
INFO:root:[ 8100] Training loss: 0.05588553, Validation loss: 0.05885696
INFO:root:[ 8200] Training loss: 0.05591338, Validation loss: 0.05592679
INFO:root:[ 8300] Training loss: 0.05607719, Validation loss: 0.05648868
INFO:root:[ 8400] Training loss: 0.05625015, Validation loss: 0.06330471
INFO:root:[ 8500] Training loss: 0.05435989, Validation loss: 0.05633090
INFO:root:[ 8600] Training loss: 0.05555736, Validation loss: 0.05929998
INFO:root:[ 8700] Training loss: 0.05444846, Validation loss: 0.06201505
INFO:root:[ 8800] Training loss: 0.05636326, Validation loss: 0.06425474
INFO:root:[ 8900] Training loss: 0.05495464, Validation loss: 0.07311352
INFO:root:[ 9000] Training loss: 0.05662829, Validation loss: 0.06005827
INFO:root:[ 9100] Training loss: 0.05452843, Validation loss: 0.05635191
INFO:root:[ 9200] Training loss: 0.05519220, Validation loss: 0.05568605
