INFO:root:Starting the logger.
INFO:root:Using cuda.
INFO:root:############### Starting experiment with config file train.ini ###############
INFO:root:######## Starting with training set size 2000000 out of [2000000] now. ########
INFO:root:Creating the dataset took 174.751s.
INFO:root:###### Starting with 100 hidden neurons out of [100, 200, 500, 1000] now. ######
INFO:root:#### Starting with 100 batches out of [100, 500] now. ####
INFO:root:Batch size: 20000
INFO:root:[  100] Training loss: 0.25127440, Validation loss: 0.25130856
INFO:root:[  200] Training loss: 0.21834517, Validation loss: 0.22052024
INFO:root:[  300] Training loss: 0.20437618, Validation loss: 0.20482555
INFO:root:[  400] Training loss: 0.19475145, Validation loss: 0.19607875
INFO:root:[  500] Training loss: 0.18823266, Validation loss: 0.18866356
INFO:root:[  600] Training loss: 0.18363027, Validation loss: 0.18275559
INFO:root:[  700] Training loss: 0.17960040, Validation loss: 0.18198675
INFO:root:[  800] Training loss: 0.17555262, Validation loss: 0.17792609
INFO:root:[  900] Training loss: 0.17364258, Validation loss: 0.17961758
INFO:root:[ 1000] Training loss: 0.16919107, Validation loss: 0.17046115
INFO:root:[ 1100] Training loss: 0.16684461, Validation loss: 0.16941659
INFO:root:[ 1200] Training loss: 0.16400428, Validation loss: 0.16922316
INFO:root:[ 1300] Training loss: 0.16265971, Validation loss: 0.16777700
INFO:root:[ 1400] Training loss: 0.16048036, Validation loss: 0.16299695
INFO:root:[ 1500] Training loss: 0.15910074, Validation loss: 0.16578476
INFO:root:[ 1600] Training loss: 0.15722064, Validation loss: 0.15581615
INFO:root:[ 1700] Training loss: 0.15558612, Validation loss: 0.15784051
INFO:root:[ 1800] Training loss: 0.15454069, Validation loss: 0.15573201
INFO:root:[ 1900] Training loss: 0.15334288, Validation loss: 0.15332770
INFO:root:[ 2000] Training loss: 0.15159441, Validation loss: 0.15405656
INFO:root:[ 2100] Training loss: 0.14999392, Validation loss: 0.16112983
INFO:root:[ 2200] Training loss: 0.14900177, Validation loss: 0.15552282
INFO:root:[ 2300] Training loss: 0.14708565, Validation loss: 0.15366201
INFO:root:[ 2400] Training loss: 0.14627673, Validation loss: 0.14743437
INFO:root:[ 2500] Training loss: 0.14520548, Validation loss: 0.14610012
INFO:root:[ 2600] Training loss: 0.14395958, Validation loss: 0.14731973
INFO:root:[ 2700] Training loss: 0.14230926, Validation loss: 0.14760354
INFO:root:[ 2800] Training loss: 0.14177378, Validation loss: 0.14189771
INFO:root:[ 2900] Training loss: 0.14051373, Validation loss: 0.14491202
INFO:root:[ 3000] Training loss: 0.13977438, Validation loss: 0.14984447
INFO:root:[ 3100] Training loss: 0.13941082, Validation loss: 0.14256257
INFO:root:[ 3200] Training loss: 0.13863209, Validation loss: 0.13885890
INFO:root:[ 3300] Training loss: 0.13723515, Validation loss: 0.13897738
INFO:root:[ 3400] Training loss: 0.13603998, Validation loss: 0.13745749
INFO:root:[ 3500] Training loss: 0.13557726, Validation loss: 0.13782156
INFO:root:[ 3600] Training loss: 0.13533017, Validation loss: 0.13596952
INFO:root:[ 3700] Training loss: 0.13413522, Validation loss: 0.13706218
INFO:root:[ 3800] Training loss: 0.13342627, Validation loss: 0.13309519
INFO:root:[ 3900] Training loss: 0.13261441, Validation loss: 0.13872576
INFO:root:[ 4000] Training loss: 0.13209098, Validation loss: 0.13328388
INFO:root:[ 4100] Training loss: 0.13161810, Validation loss: 0.12952678
INFO:root:[ 4200] Training loss: 0.13103804, Validation loss: 0.13053592
INFO:root:[ 4300] Training loss: 0.13134629, Validation loss: 0.12950155
INFO:root:[ 4400] Training loss: 0.12983097, Validation loss: 0.12753364
INFO:root:[ 4500] Training loss: 0.13000700, Validation loss: 0.13025616
INFO:root:[ 4600] Training loss: 0.12910982, Validation loss: 0.13009141
INFO:root:[ 4700] Training loss: 0.12863494, Validation loss: 0.14062867
INFO:root:[ 4800] Training loss: 0.12809707, Validation loss: 0.12592161
INFO:root:[ 4900] Training loss: 0.12758414, Validation loss: 0.12947354
INFO:root:[ 5000] Training loss: 0.12763263, Validation loss: 0.13312885
INFO:root:[ 5100] Training loss: 0.12865445, Validation loss: 0.14386432
INFO:root:[ 5200] Training loss: 0.12653338, Validation loss: 0.12868410
INFO:root:[ 5300] Training loss: 0.12636334, Validation loss: 0.12989174
INFO:root:[ 5400] Training loss: 0.12572095, Validation loss: 0.13280299
INFO:root:[ 5500] Training loss: 0.12495209, Validation loss: 0.12406196
INFO:root:[ 5600] Training loss: 0.12501867, Validation loss: 0.12569946
INFO:root:[ 5700] Training loss: 0.12428857, Validation loss: 0.13300268
INFO:root:[ 5800] Training loss: 0.12493094, Validation loss: 0.12889387
INFO:root:[ 5900] Training loss: 0.12303800, Validation loss: 0.12280940
INFO:root:[ 6000] Training loss: 0.12405836, Validation loss: 0.12479552
INFO:root:[ 6100] Training loss: 0.12280679, Validation loss: 0.12664586
INFO:root:[ 6200] Training loss: 0.12315014, Validation loss: 0.12302772
INFO:root:[ 6300] Training loss: 0.12313294, Validation loss: 0.12241529
INFO:root:[ 6400] Training loss: 0.12428307, Validation loss: 0.12756954
INFO:root:[ 6500] Training loss: 0.12191961, Validation loss: 0.12781039
INFO:root:[ 6600] Training loss: 0.12168783, Validation loss: 0.12110258
INFO:root:[ 6700] Training loss: 0.12212070, Validation loss: 0.12198862
INFO:root:[ 6800] Training loss: 0.12113762, Validation loss: 0.12335421
INFO:root:[ 6900] Training loss: 0.12208703, Validation loss: 0.12608229
INFO:root:[ 7000] Training loss: 0.12155047, Validation loss: 0.13315161
INFO:root:[ 7100] Training loss: 0.12078111, Validation loss: 0.12702349
INFO:root:[ 7200] Training loss: 0.12320644, Validation loss: 0.12246681
INFO:root:[ 7300] Training loss: 0.12021239, Validation loss: 0.14401259
INFO:root:[ 7400] Training loss: 0.12085412, Validation loss: 0.12432397
INFO:root:[ 7500] Training loss: 0.12127589, Validation loss: 0.11970013
INFO:root:[ 7600] Training loss: 0.11867911, Validation loss: 0.12453605
INFO:root:[ 7700] Training loss: 0.11914946, Validation loss: 0.12815353
INFO:root:[ 7800] Training loss: 0.11891679, Validation loss: 0.11753284
INFO:root:[ 7900] Training loss: 0.12029936, Validation loss: 0.12263629
INFO:root:[ 8000] Training loss: 0.11888713, Validation loss: 0.12373930
INFO:root:[ 8100] Training loss: 0.11862867, Validation loss: 0.12265250
INFO:root:[ 8200] Training loss: 0.11783556, Validation loss: 0.11852717
INFO:root:[ 8300] Training loss: 0.11817010, Validation loss: 0.12082825
INFO:root:[ 8400] Training loss: 0.11716244, Validation loss: 0.11715388
INFO:root:[ 8500] Training loss: 0.11743847, Validation loss: 0.11496551
INFO:root:[ 8600] Training loss: 0.12053706, Validation loss: 0.11817214
INFO:root:[ 8700] Training loss: 0.11673780, Validation loss: 0.11582782
INFO:root:[ 8800] Training loss: 0.11917216, Validation loss: 0.13269439
INFO:root:[ 8900] Training loss: 0.11657220, Validation loss: 0.12841772
INFO:root:[ 9000] Training loss: 0.11638196, Validation loss: 0.12808520
INFO:root:[ 9100] Training loss: 0.11778939, Validation loss: 0.11644707
INFO:root:[ 9200] Training loss: 0.12782343, Validation loss: 0.14873293
INFO:root:[ 9300] Training loss: 0.11686018, Validation loss: 0.11312264
INFO:root:[ 9400] Training loss: 0.11804939, Validation loss: 0.11698157
INFO:root:[ 9500] Training loss: 0.11632561, Validation loss: 0.11587030
INFO:root:[ 9600] Training loss: 0.11641769, Validation loss: 0.12155573
INFO:root:[ 9700] Training loss: 0.11726528, Validation loss: 0.11333296
INFO:root:[ 9800] Training loss: 0.11550537, Validation loss: 0.11449271
INFO:root:[ 9900] Training loss: 0.11467617, Validation loss: 0.11322271
INFO:root:[10000] Training loss: 0.11551399, Validation loss: 0.11897239
INFO:root:[10100] Training loss: 0.11521966, Validation loss: 0.12743628
INFO:root:[10200] Training loss: 0.11560564, Validation loss: 0.12663431
INFO:root:[10300] Training loss: 0.11482818, Validation loss: 0.13497110
INFO:root:[10400] Training loss: 0.11386872, Validation loss: 0.11318801
INFO:root:[10500] Training loss: 0.11516898, Validation loss: 0.11363701
INFO:root:[10600] Training loss: 0.11383703, Validation loss: 0.11214653
INFO:root:[10700] Training loss: 0.11296152, Validation loss: 0.11262202
INFO:root:[10800] Training loss: 0.11378943, Validation loss: 0.11189831
INFO:root:[10900] Training loss: 0.11378302, Validation loss: 0.11248863
INFO:root:[11000] Training loss: 0.11480241, Validation loss: 0.11263087
INFO:root:[11100] Training loss: 0.11285542, Validation loss: 0.11166590
INFO:root:[11200] Training loss: 0.11416294, Validation loss: 0.11603709
INFO:root:[11300] Training loss: 0.11266885, Validation loss: 0.12050129
INFO:root:[11400] Training loss: 0.11491031, Validation loss: 0.11416871
INFO:root:[11500] Training loss: 0.11394871, Validation loss: 0.11748929
INFO:root:[11600] Training loss: 0.11300197, Validation loss: 0.11394287
INFO:root:[11700] Training loss: 0.11888513, Validation loss: 0.11569890
INFO:root:[11800] Training loss: 0.11299108, Validation loss: 0.11242554
INFO:root:[11900] Training loss: 0.11208157, Validation loss: 0.11041810
INFO:root:[12000] Training loss: 0.11241001, Validation loss: 0.11368245
INFO:root:[12100] Training loss: 0.11230610, Validation loss: 0.12089050
INFO:root:[12200] Training loss: 0.11213578, Validation loss: 0.12056947
INFO:root:[12300] Training loss: 0.11404441, Validation loss: 0.11042403
INFO:root:[12400] Training loss: 0.11285689, Validation loss: 0.11007926
INFO:root:[12500] Training loss: 0.11204509, Validation loss: 0.11064118
INFO:root:[12600] Training loss: 0.11208183, Validation loss: 0.10972659
INFO:root:[12700] Training loss: 0.11299728, Validation loss: 0.10952566
INFO:root:[12800] Training loss: 0.11148039, Validation loss: 0.10995529
INFO:root:[12900] Training loss: 0.11136627, Validation loss: 0.12091508
INFO:root:[13000] Training loss: 0.11177207, Validation loss: 0.11523800
INFO:root:[13100] Training loss: 0.11089059, Validation loss: 0.11933996
INFO:root:[13200] Training loss: 0.11211667, Validation loss: 0.11526766
INFO:root:[13300] Training loss: 0.11118660, Validation loss: 0.11482730
INFO:root:[13400] Training loss: 0.11099410, Validation loss: 0.11445294
INFO:root:[13500] Training loss: 0.11091124, Validation loss: 0.11277197
INFO:root:[13600] Training loss: 0.11052150, Validation loss: 0.11023250
INFO:root:[13700] Training loss: 0.11070266, Validation loss: 0.10862872
INFO:root:[13800] Training loss: 0.11281123, Validation loss: 0.11786223
INFO:root:[13900] Training loss: 0.11452589, Validation loss: 0.12298740
INFO:root:[14000] Training loss: 0.10953224, Validation loss: 0.11067933
INFO:root:[14100] Training loss: 0.11261301, Validation loss: 0.11606063
INFO:root:[14200] Training loss: 0.10969699, Validation loss: 0.12291211
INFO:root:[14300] Training loss: 0.11825784, Validation loss: 0.10837502
INFO:root:[14400] Training loss: 0.10976772, Validation loss: 0.10900187
INFO:root:[14500] Training loss: 0.10992074, Validation loss: 0.10800771
INFO:root:[14600] Training loss: 0.11046580, Validation loss: 0.12323721
INFO:root:[14700] Training loss: 0.11141949, Validation loss: 0.11212271
INFO:root:[14800] Training loss: 0.11037498, Validation loss: 0.11213771
INFO:root:[14900] Training loss: 0.11124953, Validation loss: 0.12375600
INFO:root:[15000] Training loss: 0.10916152, Validation loss: 0.10848701
INFO:root:[15100] Training loss: 0.10929628, Validation loss: 0.10790242
INFO:root:[15200] Training loss: 0.10929701, Validation loss: 0.10905928
INFO:root:[15300] Training loss: 0.10918746, Validation loss: 0.10745190
INFO:root:[15400] Training loss: 0.10932044, Validation loss: 0.11209515
INFO:root:[15500] Training loss: 0.10860310, Validation loss: 0.10738404
INFO:root:[15600] Training loss: 0.10882295, Validation loss: 0.11131815
INFO:root:[15700] Training loss: 0.10890194, Validation loss: 0.11020643
INFO:root:[15800] Training loss: 0.10898039, Validation loss: 0.10732729
INFO:root:[15900] Training loss: 0.10793239, Validation loss: 0.11468284
INFO:root:[16000] Training loss: 0.10886191, Validation loss: 0.10597993
INFO:root:[16100] Training loss: 0.10885110, Validation loss: 0.10877888
INFO:root:[16200] Training loss: 0.10826463, Validation loss: 0.11695804
INFO:root:[16300] Training loss: 0.10867905, Validation loss: 0.10704193
INFO:root:[16400] Training loss: 0.10969042, Validation loss: 0.10741920
INFO:root:[16500] Training loss: 0.10928395, Validation loss: 0.12426190
INFO:root:[16600] Training loss: 0.10748651, Validation loss: 0.10872917
INFO:root:[16700] Training loss: 0.10882527, Validation loss: 0.11267446
INFO:root:[16800] Training loss: 0.11097563, Validation loss: 0.10764796
INFO:root:[16900] Training loss: 0.10962949, Validation loss: 0.11142381
INFO:root:[17000] Training loss: 0.10788837, Validation loss: 0.11220681
INFO:root:[17100] Training loss: 0.10843370, Validation loss: 0.10732249
INFO:root:[17200] Training loss: 0.10739412, Validation loss: 0.10671288
INFO:root:[17300] Training loss: 0.10869761, Validation loss: 0.12568966
INFO:root:[17400] Training loss: 0.10788982, Validation loss: 0.11258268
INFO:root:[17500] Training loss: 0.11091510, Validation loss: 0.10851011
INFO:root:[17600] Training loss: 0.10736249, Validation loss: 0.11087184
INFO:root:[17700] Training loss: 0.10719802, Validation loss: 0.12944426
INFO:root:[17800] Training loss: 0.10759182, Validation loss: 0.10639659
INFO:root:[17900] Training loss: 0.10828566, Validation loss: 0.11216980
INFO:root:[18000] Training loss: 0.10779064, Validation loss: 0.10740001
INFO:root:[18100] Training loss: 0.10713620, Validation loss: 0.10902856
INFO:root:[18200] Training loss: 0.10860545, Validation loss: 0.11879349
INFO:root:[18300] Training loss: 0.10737157, Validation loss: 0.10738743
INFO:root:[18400] Training loss: 0.10678544, Validation loss: 0.10384187
INFO:root:[18500] Training loss: 0.11328722, Validation loss: 0.10668384
INFO:root:[18600] Training loss: 0.10684201, Validation loss: 0.10836220
INFO:root:[18700] Training loss: 0.10741985, Validation loss: 0.10923231
INFO:root:[18800] Training loss: 0.10596095, Validation loss: 0.10437685
INFO:root:[18900] Training loss: 0.10748686, Validation loss: 0.11923291
INFO:root:[19000] Training loss: 0.10723953, Validation loss: 0.10880581
INFO:root:[19100] Training loss: 0.10682567, Validation loss: 0.10909694
INFO:root:[19200] Training loss: 0.10769088, Validation loss: 0.12386312
INFO:root:[19300] Training loss: 0.10626091, Validation loss: 0.10907456
INFO:root:[19400] Training loss: 0.10607281, Validation loss: 0.10933077
INFO:root:[19500] Training loss: 0.10669548, Validation loss: 0.10739870
INFO:root:[19600] Training loss: 0.10603081, Validation loss: 0.11349057
INFO:root:[19700] Training loss: 0.10629856, Validation loss: 0.10543041
INFO:root:[19800] Training loss: 0.10597779, Validation loss: 0.11727861
INFO:root:[19900] Training loss: 0.10754840, Validation loss: 0.10498364
INFO:root:[20000] Training loss: 0.10442909, Validation loss: 0.11124606
INFO:root:Training the model took 3870.257s.
INFO:root:Precision score (training data): 0.93832
INFO:root:Recall score (training data): 0.90592
INFO:root:F1 score (training data): 0.92183
INFO:root:Precision score (test data): 0.93648
INFO:root:Recall score (test data): 0.90673
INFO:root:F1 score (test data): 0.92136
INFO:root:Covering score (training data) (quantile cutoff): 0.91438
INFO:root:Complete cover size (training data) (quantile cutoff): 0.2927479981014499
INFO:root:Covering score (test data) (quantile cutoff): 0.9093
INFO:root:Complete cover size (test data) (quantile cutoff): 0.2914643917613235
INFO:root:Covering score (training data) (0.5 cutoff): 0.78487
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.22635083820683702
INFO:root:Covering score (test data) (0.5 cutoff): 0.781
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.22566306932504115
INFO:root:MSE (test data): 0.02997
INFO:root:BCE (test data): 0.10931
INFO:root:#### Starting with 500 batches out of [100, 500] now. ####
INFO:root:Batch size: 4000
INFO:root:[  100] Training loss: 0.20644195, Validation loss: 0.20578276
INFO:root:[  200] Training loss: 0.18126307, Validation loss: 0.18327345
INFO:root:[  300] Training loss: 0.16570571, Validation loss: 0.16829510
INFO:root:[  400] Training loss: 0.15510096, Validation loss: 0.15490812
INFO:root:[  500] Training loss: 0.14836684, Validation loss: 0.15175137
INFO:root:[  600] Training loss: 0.14348237, Validation loss: 0.14219373
INFO:root:[  700] Training loss: 0.13951074, Validation loss: 0.14091231
INFO:root:[  800] Training loss: 0.13649421, Validation loss: 0.13566779
INFO:root:[  900] Training loss: 0.13402919, Validation loss: 0.13881765
INFO:root:[ 1000] Training loss: 0.13204864, Validation loss: 0.13452610
INFO:root:[ 1100] Training loss: 0.13019938, Validation loss: 0.13129078
INFO:root:[ 1200] Training loss: 0.12852048, Validation loss: 0.13081871
INFO:root:[ 1300] Training loss: 0.12684404, Validation loss: 0.12701067
INFO:root:[ 1400] Training loss: 0.12528788, Validation loss: 0.12586726
INFO:root:[ 1500] Training loss: 0.12431887, Validation loss: 0.12323131
INFO:root:[ 1600] Training loss: 0.12310498, Validation loss: 0.12081654
INFO:root:[ 1700] Training loss: 0.12203354, Validation loss: 0.12209821
INFO:root:[ 1800] Training loss: 0.12086989, Validation loss: 0.12351614
INFO:root:[ 1900] Training loss: 0.11970796, Validation loss: 0.11936869
INFO:root:[ 2000] Training loss: 0.11903757, Validation loss: 0.11875139
INFO:root:[ 2100] Training loss: 0.11831151, Validation loss: 0.11787246
INFO:root:[ 2200] Training loss: 0.11751282, Validation loss: 0.11766622
INFO:root:[ 2300] Training loss: 0.11655729, Validation loss: 0.11670001
INFO:root:[ 2400] Training loss: 0.11620385, Validation loss: 0.11703978
INFO:root:[ 2500] Training loss: 0.11520699, Validation loss: 0.11382692
INFO:root:[ 2600] Training loss: 0.11487355, Validation loss: 0.11258228
INFO:root:[ 2700] Training loss: 0.11399866, Validation loss: 0.11315051
INFO:root:[ 2800] Training loss: 0.11370956, Validation loss: 0.11738760
INFO:root:[ 2900] Training loss: 0.11335043, Validation loss: 0.11284648
INFO:root:[ 3000] Training loss: 0.11289549, Validation loss: 0.11066909
INFO:root:[ 3100] Training loss: 0.11206345, Validation loss: 0.11181346
INFO:root:[ 3200] Training loss: 0.11190225, Validation loss: 0.11354891
INFO:root:[ 3300] Training loss: 0.11105981, Validation loss: 0.11446565
INFO:root:[ 3400] Training loss: 0.11091548, Validation loss: 0.11162980
INFO:root:[ 3500] Training loss: 0.11042228, Validation loss: 0.10931279
INFO:root:[ 3600] Training loss: 0.10991156, Validation loss: 0.11650006
INFO:root:[ 3700] Training loss: 0.10975635, Validation loss: 0.11376499
INFO:root:[ 3800] Training loss: 0.10954962, Validation loss: 0.10829069
INFO:root:[ 3900] Training loss: 0.10900015, Validation loss: 0.10752335
INFO:root:[ 4000] Training loss: 0.10882894, Validation loss: 0.10644283
INFO:root:[ 4100] Training loss: 0.10868418, Validation loss: 0.10619959
INFO:root:[ 4200] Training loss: 0.10818862, Validation loss: 0.10583139
INFO:root:[ 4300] Training loss: 0.10781201, Validation loss: 0.11067282
INFO:root:[ 4400] Training loss: 0.10747935, Validation loss: 0.10960498
INFO:root:[ 4500] Training loss: 0.10719139, Validation loss: 0.11216245
INFO:root:[ 4600] Training loss: 0.10706806, Validation loss: 0.10696966
INFO:root:[ 4700] Training loss: 0.10706943, Validation loss: 0.10770005
INFO:root:[ 4800] Training loss: 0.10682317, Validation loss: 0.10780758
INFO:root:[ 4900] Training loss: 0.10648535, Validation loss: 0.10539275
INFO:root:[ 5000] Training loss: 0.10624564, Validation loss: 0.11180191
INFO:root:[ 5100] Training loss: 0.10597067, Validation loss: 0.11650489
INFO:root:[ 5200] Training loss: 0.10577952, Validation loss: 0.10506842
INFO:root:[ 5300] Training loss: 0.10576650, Validation loss: 0.10585928
INFO:root:[ 5400] Training loss: 0.10535415, Validation loss: 0.10747080
INFO:root:[ 5500] Training loss: 0.10550012, Validation loss: 0.12125266
INFO:root:[ 5600] Training loss: 0.10510142, Validation loss: 0.11063235
INFO:root:[ 5700] Training loss: 0.10498588, Validation loss: 0.10803873
INFO:root:[ 5800] Training loss: 0.10520997, Validation loss: 0.10475358
INFO:root:[ 5900] Training loss: 0.10469794, Validation loss: 0.10468776
INFO:root:[ 6000] Training loss: 0.10451790, Validation loss: 0.10500413
INFO:root:[ 6100] Training loss: 0.10444854, Validation loss: 0.10455375
INFO:root:[ 6200] Training loss: 0.10463186, Validation loss: 0.10542811
INFO:root:[ 6300] Training loss: 0.10415637, Validation loss: 0.10600511
INFO:root:[ 6400] Training loss: 0.10434719, Validation loss: 0.11194653
INFO:root:[ 6500] Training loss: 0.10388971, Validation loss: 0.10695489
INFO:root:[ 6600] Training loss: 0.10400346, Validation loss: 0.10866711
INFO:root:[ 6700] Training loss: 0.10377297, Validation loss: 0.10443513
INFO:root:[ 6800] Training loss: 0.10347854, Validation loss: 0.10614559
INFO:root:[ 6900] Training loss: 0.10345631, Validation loss: 0.10772984
INFO:root:[ 7000] Training loss: 0.10308797, Validation loss: 0.10360292
INFO:root:[ 7100] Training loss: 0.10314250, Validation loss: 0.10469732
INFO:root:[ 7200] Training loss: 0.10307962, Validation loss: 0.10519037
INFO:root:[ 7300] Training loss: 0.10318587, Validation loss: 0.10287665
INFO:root:[ 7400] Training loss: 0.10276994, Validation loss: 0.10406230
INFO:root:[ 7500] Training loss: 0.10245133, Validation loss: 0.10516123
INFO:root:[ 7600] Training loss: 0.10313605, Validation loss: 0.10282211
INFO:root:[ 7700] Training loss: 0.10230196, Validation loss: 0.10364056
INFO:root:[ 7800] Training loss: 0.10325426, Validation loss: 0.11021475
INFO:root:[ 7900] Training loss: 0.10241638, Validation loss: 0.10132006
INFO:root:[ 8000] Training loss: 0.10231665, Validation loss: 0.10072619
INFO:root:[ 8100] Training loss: 0.10228033, Validation loss: 0.10341497
INFO:root:[ 8200] Training loss: 0.10326750, Validation loss: 0.10131433
INFO:root:[ 8300] Training loss: 0.10218278, Validation loss: 0.10338929
INFO:root:[ 8400] Training loss: 0.10162744, Validation loss: 0.10196196
INFO:root:[ 8500] Training loss: 0.10166939, Validation loss: 0.10209024
INFO:root:[ 8600] Training loss: 0.10162194, Validation loss: 0.10248222
INFO:root:[ 8700] Training loss: 0.10161812, Validation loss: 0.10321281
INFO:root:[ 8800] Training loss: 0.10200253, Validation loss: 0.10225200
INFO:root:[ 8900] Training loss: 0.10135295, Validation loss: 0.10009773
INFO:root:[ 9000] Training loss: 0.10179775, Validation loss: 0.10267472
INFO:root:[ 9100] Training loss: 0.10116647, Validation loss: 0.10289869
INFO:root:[ 9200] Training loss: 0.10123335, Validation loss: 0.10276268
INFO:root:[ 9300] Training loss: 0.10114329, Validation loss: 0.10073704
INFO:root:[ 9400] Training loss: 0.10091957, Validation loss: 0.10211013
INFO:root:[ 9500] Training loss: 0.10088224, Validation loss: 0.09943789
INFO:root:[ 9600] Training loss: 0.10098816, Validation loss: 0.10362263
INFO:root:[ 9700] Training loss: 0.10070433, Validation loss: 0.11004458
INFO:root:[ 9800] Training loss: 0.10072244, Validation loss: 0.10161813
INFO:root:[ 9900] Training loss: 0.10062466, Validation loss: 0.10262692
INFO:root:[10000] Training loss: 0.10030982, Validation loss: 0.09925508
INFO:root:[10100] Training loss: 0.10036730, Validation loss: 0.10058024
INFO:root:[10200] Training loss: 0.10022386, Validation loss: 0.10199892
INFO:root:[10300] Training loss: 0.10068178, Validation loss: 0.10157416
INFO:root:[10400] Training loss: 0.10112211, Validation loss: 0.10074084
INFO:root:[10500] Training loss: 0.10043080, Validation loss: 0.10319736
INFO:root:[10600] Training loss: 0.10045013, Validation loss: 0.10134339
INFO:root:[10700] Training loss: 0.10027921, Validation loss: 0.09778098
INFO:root:[10800] Training loss: 0.10103330, Validation loss: 0.10189633
INFO:root:[10900] Training loss: 0.09963513, Validation loss: 0.10434853
INFO:root:[11000] Training loss: 0.09995059, Validation loss: 0.10492846
INFO:root:[11100] Training loss: 0.09984554, Validation loss: 0.09943319
INFO:root:[11200] Training loss: 0.09934525, Validation loss: 0.10096975
INFO:root:[11300] Training loss: 0.09965344, Validation loss: 0.09827844
INFO:root:[11400] Training loss: 0.09922089, Validation loss: 0.09884202
INFO:root:[11500] Training loss: 0.09907434, Validation loss: 0.10151722
INFO:root:[11600] Training loss: 0.09887180, Validation loss: 0.10982249
INFO:root:[11700] Training loss: 0.09878620, Validation loss: 0.10010408
INFO:root:[11800] Training loss: 0.09881855, Validation loss: 0.09930909
INFO:root:[11900] Training loss: 0.09914744, Validation loss: 0.09941479
INFO:root:[12000] Training loss: 0.09878178, Validation loss: 0.09836046
INFO:root:[12100] Training loss: 0.09891602, Validation loss: 0.10682762
INFO:root:[12200] Training loss: 0.09888917, Validation loss: 0.10256425
INFO:root:[12300] Training loss: 0.09841627, Validation loss: 0.10165066
INFO:root:[12400] Training loss: 0.09859287, Validation loss: 0.10064156
INFO:root:[12500] Training loss: 0.09878329, Validation loss: 0.10210754
INFO:root:[12600] Training loss: 0.09858118, Validation loss: 0.10151542
INFO:root:[12700] Training loss: 0.09862254, Validation loss: 0.10732171
INFO:root:[12800] Training loss: 0.09834409, Validation loss: 0.09683955
INFO:root:[12900] Training loss: 0.09832150, Validation loss: 0.09959919
INFO:root:[13000] Training loss: 0.09830085, Validation loss: 0.09744220
INFO:root:[13100] Training loss: 0.09807619, Validation loss: 0.10152524
INFO:root:[13200] Training loss: 0.09860847, Validation loss: 0.09707579
INFO:root:[13300] Training loss: 0.09781573, Validation loss: 0.10217581
INFO:root:[13400] Training loss: 0.09794494, Validation loss: 0.09960812
INFO:root:[13500] Training loss: 0.09902491, Validation loss: 0.09979490
INFO:root:[13600] Training loss: 0.09768422, Validation loss: 0.09770109
INFO:root:[13700] Training loss: 0.09767963, Validation loss: 0.09795936
INFO:root:[13800] Training loss: 0.09887266, Validation loss: 0.10349329
INFO:root:[13900] Training loss: 0.09761479, Validation loss: 0.09903121
INFO:root:[14000] Training loss: 0.09752633, Validation loss: 0.09977207
INFO:root:[14100] Training loss: 0.09755962, Validation loss: 0.10001200
INFO:root:[14200] Training loss: 0.09757457, Validation loss: 0.09932845
INFO:root:[14300] Training loss: 0.09794515, Validation loss: 0.09905505
INFO:root:[14400] Training loss: 0.09729084, Validation loss: 0.09826516
INFO:root:[14500] Training loss: 0.09748644, Validation loss: 0.10100339
INFO:root:[14600] Training loss: 0.09731588, Validation loss: 0.09802359
INFO:root:[14700] Training loss: 0.09713837, Validation loss: 0.09937028
INFO:root:[14800] Training loss: 0.09721255, Validation loss: 0.10670052
INFO:root:[14900] Training loss: 0.09766829, Validation loss: 0.09946613
INFO:root:[15000] Training loss: 0.09707614, Validation loss: 0.09898052
INFO:root:[15100] Training loss: 0.09694551, Validation loss: 0.09802846
INFO:root:[15200] Training loss: 0.09715819, Validation loss: 0.09781767
INFO:root:[15300] Training loss: 0.09725635, Validation loss: 0.09734458
INFO:root:[15400] Training loss: 0.09690707, Validation loss: 0.09955385
INFO:root:[15500] Training loss: 0.09836443, Validation loss: 0.09812663
INFO:root:[15600] Training loss: 0.09725620, Validation loss: 0.09680212
INFO:root:[15700] Training loss: 0.09730312, Validation loss: 0.09803066
INFO:root:[15800] Training loss: 0.09707890, Validation loss: 0.09618215
INFO:root:[15900] Training loss: 0.09667357, Validation loss: 0.09706312
INFO:root:[16000] Training loss: 0.09653568, Validation loss: 0.09879878
INFO:root:[16100] Training loss: 0.09653971, Validation loss: 0.09881376
INFO:root:[16200] Training loss: 0.09724140, Validation loss: 0.09863472
INFO:root:[16300] Training loss: 0.09651467, Validation loss: 0.09633412
INFO:root:[16400] Training loss: 0.09652622, Validation loss: 0.09749620
INFO:root:[16500] Training loss: 0.09649153, Validation loss: 0.09938789
INFO:root:[16600] Training loss: 0.09633366, Validation loss: 0.09686775
INFO:root:[16700] Training loss: 0.09635458, Validation loss: 0.10016163
INFO:root:[16800] Training loss: 0.09686202, Validation loss: 0.10809833
INFO:root:[16900] Training loss: 0.09656514, Validation loss: 0.10044691
INFO:root:[17000] Training loss: 0.09618166, Validation loss: 0.10735323
INFO:root:[17100] Training loss: 0.09622256, Validation loss: 0.10183167
INFO:root:[17200] Training loss: 0.09606901, Validation loss: 0.09616479
INFO:root:[17300] Training loss: 0.09621141, Validation loss: 0.10185687
INFO:root:[17400] Training loss: 0.09665895, Validation loss: 0.09745567
INFO:root:[17500] Training loss: 0.09615864, Validation loss: 0.09871024
INFO:root:[17600] Training loss: 0.09600458, Validation loss: 0.09870093
INFO:root:[17700] Training loss: 0.09596236, Validation loss: 0.09882940
INFO:root:[17800] Training loss: 0.09583630, Validation loss: 0.09940434
INFO:root:[17900] Training loss: 0.09724362, Validation loss: 0.09887034
INFO:root:[18000] Training loss: 0.09595423, Validation loss: 0.09861176
INFO:root:[18100] Training loss: 0.09586302, Validation loss: 0.09957770
INFO:root:[18200] Training loss: 0.09561398, Validation loss: 0.09710557
INFO:root:[18300] Training loss: 0.09561770, Validation loss: 0.10049982
INFO:root:[18400] Training loss: 0.09591357, Validation loss: 0.09654866
INFO:root:[18500] Training loss: 0.09600390, Validation loss: 0.10596748
INFO:root:[18600] Training loss: 0.09557389, Validation loss: 0.09674847
INFO:root:[18700] Training loss: 0.09748425, Validation loss: 0.09791391
INFO:root:[18800] Training loss: 0.09574230, Validation loss: 0.09672750
INFO:root:[18900] Training loss: 0.09600183, Validation loss: 0.10073553
INFO:root:[19000] Training loss: 0.09676912, Validation loss: 0.09609527
INFO:root:[19100] Training loss: 0.09689667, Validation loss: 0.09666510
INFO:root:[19200] Training loss: 0.09553880, Validation loss: 0.09688264
INFO:root:[19300] Training loss: 0.09546311, Validation loss: 0.10179128
INFO:root:[19400] Training loss: 0.09555196, Validation loss: 0.09619451
INFO:root:[19500] Training loss: 0.09513041, Validation loss: 0.09528435
INFO:root:[19600] Training loss: 0.09535184, Validation loss: 0.09751330
INFO:root:[19700] Training loss: 0.09524221, Validation loss: 0.09805408
INFO:root:[19800] Training loss: 0.09553231, Validation loss: 0.09676727
INFO:root:[19900] Training loss: 0.09532172, Validation loss: 0.09939822
INFO:root:[20000] Training loss: 0.09536162, Validation loss: 0.09712344
INFO:root:Training the model took 14047.935s.
INFO:root:Precision score (training data): 0.94724
INFO:root:Recall score (training data): 0.9182
INFO:root:F1 score (training data): 0.9325
INFO:root:Precision score (test data): 0.94253
INFO:root:Recall score (test data): 0.91635
INFO:root:F1 score (test data): 0.92925
INFO:root:Covering score (training data) (quantile cutoff): 0.93547
INFO:root:Complete cover size (training data) (quantile cutoff): 0.29431879917062537
INFO:root:Covering score (test data) (quantile cutoff): 0.9304
INFO:root:Complete cover size (test data) (quantile cutoff): 0.293644822503378
INFO:root:Covering score (training data) (0.5 cutoff): 0.80781
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.2279407317596802
INFO:root:Covering score (test data) (0.5 cutoff): 0.8039
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.227435892879356
INFO:root:MSE (test data): 0.02658
INFO:root:BCE (test data): 0.09696
INFO:root:###### Starting with 200 hidden neurons out of [100, 200, 500, 1000] now. ######
INFO:root:#### Starting with 100 batches out of [100, 500] now. ####
INFO:root:Batch size: 20000
INFO:root:[  100] Training loss: 0.22738992, Validation loss: 0.22779396
INFO:root:[  200] Training loss: 0.19647536, Validation loss: 0.20054947
INFO:root:[  300] Training loss: 0.18143859, Validation loss: 0.18319111
INFO:root:[  400] Training loss: 0.17139705, Validation loss: 0.17512584
INFO:root:[  500] Training loss: 0.16435806, Validation loss: 0.16834234
INFO:root:[  600] Training loss: 0.15821803, Validation loss: 0.16138324
INFO:root:[  700] Training loss: 0.15377031, Validation loss: 0.15352039
INFO:root:[  800] Training loss: 0.14944835, Validation loss: 0.14720510
INFO:root:[  900] Training loss: 0.14572057, Validation loss: 0.14903027
INFO:root:[ 1000] Training loss: 0.14344890, Validation loss: 0.14809106
INFO:root:[ 1100] Training loss: 0.13935965, Validation loss: 0.14171115
INFO:root:[ 1200] Training loss: 0.13744911, Validation loss: 0.14755365
INFO:root:[ 1300] Training loss: 0.13492595, Validation loss: 0.13629179
INFO:root:[ 1400] Training loss: 0.13287892, Validation loss: 0.13316804
INFO:root:[ 1500] Training loss: 0.13101639, Validation loss: 0.14482473
INFO:root:[ 1600] Training loss: 0.12932041, Validation loss: 0.13109860
INFO:root:[ 1700] Training loss: 0.12853350, Validation loss: 0.13410878
INFO:root:[ 1800] Training loss: 0.12741233, Validation loss: 0.13515496
INFO:root:[ 1900] Training loss: 0.12371726, Validation loss: 0.13336460
INFO:root:[ 2000] Training loss: 0.12370843, Validation loss: 0.13209966
INFO:root:[ 2100] Training loss: 0.12144919, Validation loss: 0.12558293
INFO:root:[ 2200] Training loss: 0.11986127, Validation loss: 0.13061228
INFO:root:[ 2300] Training loss: 0.11968222, Validation loss: 0.14063284
INFO:root:[ 2400] Training loss: 0.11993013, Validation loss: 0.12057419
INFO:root:[ 2500] Training loss: 0.11793269, Validation loss: 0.14033780
INFO:root:[ 2600] Training loss: 0.11659542, Validation loss: 0.11748927
INFO:root:[ 2700] Training loss: 0.11515078, Validation loss: 0.11666990
INFO:root:[ 2800] Training loss: 0.11849042, Validation loss: 0.11504334
INFO:root:[ 2900] Training loss: 0.11366575, Validation loss: 0.11395688
INFO:root:[ 3000] Training loss: 0.11328601, Validation loss: 0.11824579
INFO:root:[ 3100] Training loss: 0.11551184, Validation loss: 0.11568698
INFO:root:[ 3200] Training loss: 0.11279168, Validation loss: 0.11763128
INFO:root:[ 3300] Training loss: 0.11006239, Validation loss: 0.11214417
INFO:root:[ 3400] Training loss: 0.10995512, Validation loss: 0.11899953
INFO:root:[ 3500] Training loss: 0.10988352, Validation loss: 0.11696661
INFO:root:[ 3600] Training loss: 0.10837549, Validation loss: 0.11142525
INFO:root:[ 3700] Training loss: 0.11313089, Validation loss: 0.10998393
INFO:root:[ 3800] Training loss: 0.10874969, Validation loss: 0.11150533
INFO:root:[ 3900] Training loss: 0.10581376, Validation loss: 0.11249333
INFO:root:[ 4000] Training loss: 0.10586938, Validation loss: 0.15117835
INFO:root:[ 4100] Training loss: 0.10563552, Validation loss: 0.11424515
INFO:root:[ 4200] Training loss: 0.10409935, Validation loss: 0.10589523
INFO:root:[ 4300] Training loss: 0.10475855, Validation loss: 0.10534795
INFO:root:[ 4400] Training loss: 0.10553468, Validation loss: 0.10950369
INFO:root:[ 4500] Training loss: 0.10316055, Validation loss: 0.11340579
INFO:root:[ 4600] Training loss: 0.12939331, Validation loss: 0.10947432
INFO:root:[ 4700] Training loss: 0.10243303, Validation loss: 0.10480003
INFO:root:[ 4800] Training loss: 0.10225462, Validation loss: 0.10857050
INFO:root:[ 4900] Training loss: 0.10532429, Validation loss: 0.10674003
INFO:root:[ 5000] Training loss: 0.10110575, Validation loss: 0.10379776
INFO:root:[ 5100] Training loss: 0.09946705, Validation loss: 0.10524295
INFO:root:[ 5200] Training loss: 0.10547759, Validation loss: 0.10835811
INFO:root:[ 5300] Training loss: 0.10042223, Validation loss: 0.10661014
INFO:root:[ 5400] Training loss: 0.09803595, Validation loss: 0.10113041
INFO:root:[ 5500] Training loss: 0.10079749, Validation loss: 0.10063362
INFO:root:[ 5600] Training loss: 0.10086384, Validation loss: 0.10975575
INFO:root:[ 5700] Training loss: 0.10076711, Validation loss: 0.11604277
INFO:root:[ 5800] Training loss: 0.09736309, Validation loss: 0.10971910
INFO:root:[ 5900] Training loss: 0.09838123, Validation loss: 0.10537073
INFO:root:[ 6000] Training loss: 0.10409878, Validation loss: 0.11717767
INFO:root:[ 6100] Training loss: 0.09690427, Validation loss: 0.10013156
INFO:root:[ 6200] Training loss: 0.10181516, Validation loss: 0.10261532
INFO:root:[ 6300] Training loss: 0.09890260, Validation loss: 0.11732277
INFO:root:[ 6400] Training loss: 0.09787206, Validation loss: 0.11001917
INFO:root:[ 6500] Training loss: 0.09595642, Validation loss: 0.09829718
INFO:root:[ 6600] Training loss: 0.09938965, Validation loss: 0.10796099
INFO:root:[ 6700] Training loss: 0.09648849, Validation loss: 0.09951029
INFO:root:[ 6800] Training loss: 0.09606081, Validation loss: 0.10549319
INFO:root:[ 6900] Training loss: 0.09737569, Validation loss: 0.10380072
INFO:root:[ 7000] Training loss: 0.09441378, Validation loss: 0.09909750
INFO:root:[ 7100] Training loss: 0.09477864, Validation loss: 0.10502807
INFO:root:[ 7200] Training loss: 0.09525012, Validation loss: 0.10040385
INFO:root:[ 7300] Training loss: 0.09574036, Validation loss: 0.09684492
INFO:root:[ 7400] Training loss: 0.09235606, Validation loss: 0.09720254
INFO:root:[ 7500] Training loss: 0.09304245, Validation loss: 0.09441288
INFO:root:[ 7600] Training loss: 0.09346679, Validation loss: 0.10036787
INFO:root:[ 7700] Training loss: 0.09284878, Validation loss: 0.09491032
INFO:root:[ 7800] Training loss: 0.10306428, Validation loss: 0.10120242
INFO:root:[ 7900] Training loss: 0.09265519, Validation loss: 0.13669102
INFO:root:[ 8000] Training loss: 0.09340620, Validation loss: 0.09677418
INFO:root:[ 8100] Training loss: 0.09265917, Validation loss: 0.11196046
INFO:root:[ 8200] Training loss: 0.09554835, Validation loss: 0.09824088
INFO:root:[ 8300] Training loss: 0.09271986, Validation loss: 0.09589561
INFO:root:[ 8400] Training loss: 0.09139076, Validation loss: 0.09929555
INFO:root:[ 8500] Training loss: 0.09144746, Validation loss: 0.09657702
INFO:root:[ 8600] Training loss: 0.09225696, Validation loss: 0.10859866
INFO:root:[ 8700] Training loss: 0.09105748, Validation loss: 0.09734405
INFO:root:[ 8800] Training loss: 0.09106024, Validation loss: 0.09391029
INFO:root:[ 8900] Training loss: 0.09025772, Validation loss: 0.09358662
INFO:root:[ 9000] Training loss: 0.09352449, Validation loss: 0.09532067
INFO:root:[ 9100] Training loss: 0.09102369, Validation loss: 0.09487070
INFO:root:[ 9200] Training loss: 0.09037578, Validation loss: 0.09202711
INFO:root:[ 9300] Training loss: 0.09223488, Validation loss: 0.09645115
INFO:root:[ 9400] Training loss: 0.09408767, Validation loss: 0.11166456
INFO:root:[ 9500] Training loss: 0.09108207, Validation loss: 0.09421416
INFO:root:[ 9600] Training loss: 0.09126441, Validation loss: 0.09229547
INFO:root:[ 9700] Training loss: 0.08953027, Validation loss: 0.09584154
INFO:root:[ 9800] Training loss: 0.09140236, Validation loss: 0.09610146
INFO:root:[ 9900] Training loss: 0.08820046, Validation loss: 0.10466311
INFO:root:[10000] Training loss: 0.09032928, Validation loss: 0.10234806
INFO:root:[10100] Training loss: 0.09060631, Validation loss: 0.09537366
INFO:root:[10200] Training loss: 0.08860807, Validation loss: 0.09011995
INFO:root:[10300] Training loss: 0.09044164, Validation loss: 0.11225402
INFO:root:[10400] Training loss: 0.08958593, Validation loss: 0.09924711
INFO:root:[10500] Training loss: 0.08900724, Validation loss: 0.09297290
INFO:root:[10600] Training loss: 0.08802933, Validation loss: 0.09207667
INFO:root:[10700] Training loss: 0.08829498, Validation loss: 0.09869932
INFO:root:[10800] Training loss: 0.10721404, Validation loss: 0.10030038
INFO:root:[10900] Training loss: 0.08753483, Validation loss: 0.09045460
INFO:root:[11000] Training loss: 0.08759051, Validation loss: 0.09167810
INFO:root:[11100] Training loss: 0.11153821, Validation loss: 0.09641500
INFO:root:[11200] Training loss: 0.08866015, Validation loss: 0.09537788
INFO:root:[11300] Training loss: 0.08869413, Validation loss: 0.10681574
INFO:root:[11400] Training loss: 0.09146314, Validation loss: 0.09068342
INFO:root:[11500] Training loss: 0.08829202, Validation loss: 0.09830856
INFO:root:[11600] Training loss: 0.08721107, Validation loss: 0.10435365
INFO:root:[11700] Training loss: 0.08661007, Validation loss: 0.09550370
INFO:root:[11800] Training loss: 0.09194278, Validation loss: 0.09851541
INFO:root:[11900] Training loss: 0.09211059, Validation loss: 0.09463111
INFO:root:[12000] Training loss: 0.08639383, Validation loss: 0.08994086
INFO:root:[12100] Training loss: 0.08828936, Validation loss: 0.09457517
INFO:root:[12200] Training loss: 0.08612868, Validation loss: 0.09086977
INFO:root:[12300] Training loss: 0.08647197, Validation loss: 0.09669725
INFO:root:[12400] Training loss: 0.09015316, Validation loss: 0.09302940
INFO:root:[12500] Training loss: 0.08598325, Validation loss: 0.08998591
INFO:root:[12600] Training loss: 0.08553180, Validation loss: 0.09406457
INFO:root:[12700] Training loss: 0.09119877, Validation loss: 0.09460682
INFO:root:[12800] Training loss: 0.09511277, Validation loss: 0.09803478
INFO:root:[12900] Training loss: 0.08743876, Validation loss: 0.09958420
INFO:root:[13000] Training loss: 0.09075378, Validation loss: 0.09664390
INFO:root:[13100] Training loss: 0.08688311, Validation loss: 0.09545877
INFO:root:[13200] Training loss: 0.08686887, Validation loss: 0.11791325
INFO:root:[13300] Training loss: 0.09055355, Validation loss: 0.09676377
INFO:root:[13400] Training loss: 0.08791888, Validation loss: 0.09747232
INFO:root:[13500] Training loss: 0.08484062, Validation loss: 0.12088309
INFO:root:[13600] Training loss: 0.08488554, Validation loss: 0.09298293
INFO:root:[13700] Training loss: 0.10764912, Validation loss: 0.11046728
INFO:root:[13800] Training loss: 0.08567621, Validation loss: 0.09025733
INFO:root:[13900] Training loss: 0.08741301, Validation loss: 0.09119827
INFO:root:[14000] Training loss: 0.08519639, Validation loss: 0.09927398
INFO:root:[14100] Training loss: 0.08410430, Validation loss: 0.08784955
INFO:root:[14200] Training loss: 0.08483435, Validation loss: 0.09236156
INFO:root:[14300] Training loss: 0.08495910, Validation loss: 0.09693220
INFO:root:[14400] Training loss: 0.08340267, Validation loss: 0.09045616
INFO:root:[14500] Training loss: 0.09897536, Validation loss: 0.10356772
INFO:root:[14600] Training loss: 0.09240695, Validation loss: 0.10123187
INFO:root:[14700] Training loss: 0.08312480, Validation loss: 0.08845765
INFO:root:[14800] Training loss: 0.08428179, Validation loss: 0.09009475
INFO:root:[14900] Training loss: 0.08330132, Validation loss: 0.08704817
INFO:root:[15000] Training loss: 0.08501195, Validation loss: 0.09177757
INFO:root:[15100] Training loss: 0.08590260, Validation loss: 0.09178606
INFO:root:[15200] Training loss: 0.08454565, Validation loss: 0.09667093
INFO:root:[15300] Training loss: 0.09569622, Validation loss: 0.10439780
INFO:root:[15400] Training loss: 0.08299758, Validation loss: 0.08965710
INFO:root:[15500] Training loss: 0.08250642, Validation loss: 0.09396318
INFO:root:[15600] Training loss: 0.08529046, Validation loss: 0.08966894
INFO:root:[15700] Training loss: 0.08405457, Validation loss: 0.09792902
INFO:root:[15800] Training loss: 0.08297326, Validation loss: 0.09205128
INFO:root:[15900] Training loss: 0.08383407, Validation loss: 0.09131740
INFO:root:[16000] Training loss: 0.09065745, Validation loss: 0.09010779
INFO:root:[16100] Training loss: 0.08368771, Validation loss: 0.09970469
INFO:root:[16200] Training loss: 0.08302814, Validation loss: 0.09613292
INFO:root:[16300] Training loss: 0.08303854, Validation loss: 0.11266685
INFO:root:[16400] Training loss: 0.08535980, Validation loss: 0.09041265
INFO:root:[16500] Training loss: 0.08393825, Validation loss: 0.09393156
INFO:root:[16600] Training loss: 0.08316941, Validation loss: 0.09485549
INFO:root:[16700] Training loss: 0.08220013, Validation loss: 0.09043015
INFO:root:[16800] Training loss: 0.08383287, Validation loss: 0.09774829
INFO:root:[16900] Training loss: 0.08205245, Validation loss: 0.08903839
INFO:root:[17000] Training loss: 0.08181892, Validation loss: 0.09193046
INFO:root:[17100] Training loss: 0.08190935, Validation loss: 0.09255646
INFO:root:[17200] Training loss: 0.08124042, Validation loss: 0.08666112
INFO:root:[17300] Training loss: 0.08147997, Validation loss: 0.08873568
INFO:root:[17400] Training loss: 0.08443581, Validation loss: 0.08916620
INFO:root:[17500] Training loss: 0.08308663, Validation loss: 0.08845506
INFO:root:[17600] Training loss: 0.08282777, Validation loss: 0.08619694
INFO:root:[17700] Training loss: 0.08108470, Validation loss: 0.08558084
INFO:root:[17800] Training loss: 0.08155313, Validation loss: 0.08523531
INFO:root:[17900] Training loss: 0.08130831, Validation loss: 0.08982384
INFO:root:[18000] Training loss: 0.08312483, Validation loss: 0.09613083
INFO:root:[18100] Training loss: 0.08427597, Validation loss: 0.08549657
INFO:root:[18200] Training loss: 0.08348589, Validation loss: 0.08773287
INFO:root:[18300] Training loss: 0.11925087, Validation loss: 0.10445773
INFO:root:[18400] Training loss: 0.08124254, Validation loss: 0.09509272
INFO:root:[18500] Training loss: 0.08078170, Validation loss: 0.08785245
INFO:root:[18600] Training loss: 0.08313201, Validation loss: 0.08702691
INFO:root:[18700] Training loss: 0.08017636, Validation loss: 0.08911674
INFO:root:[18800] Training loss: 0.07993502, Validation loss: 0.09752305
INFO:root:[18900] Training loss: 0.08769332, Validation loss: 0.09005923
INFO:root:[19000] Training loss: 0.08156717, Validation loss: 0.10396576
INFO:root:[19100] Training loss: 0.08601685, Validation loss: 0.09177386
INFO:root:[19200] Training loss: 0.08103222, Validation loss: 0.09249199
INFO:root:[19300] Training loss: 0.08006098, Validation loss: 0.08962151
INFO:root:[19400] Training loss: 0.08298616, Validation loss: 0.10324454
INFO:root:[19500] Training loss: 0.08121828, Validation loss: 0.08952168
INFO:root:[19600] Training loss: 0.08127401, Validation loss: 0.08843669
INFO:root:[19700] Training loss: 0.08062492, Validation loss: 0.09091608
INFO:root:[19800] Training loss: 0.08194071, Validation loss: 0.09193682
INFO:root:[19900] Training loss: 0.08124348, Validation loss: 0.08976583
INFO:root:[20000] Training loss: 0.08097034, Validation loss: 0.09155235
INFO:root:Training the model took 3956.939s.
INFO:root:Precision score (training data): 0.95154
INFO:root:Recall score (training data): 0.92997
INFO:root:F1 score (training data): 0.94063
INFO:root:Precision score (test data): 0.94475
INFO:root:Recall score (test data): 0.92645
INFO:root:F1 score (test data): 0.93551
INFO:root:Covering score (training data) (quantile cutoff): 0.94733
INFO:root:Complete cover size (training data) (quantile cutoff): 0.29584890808062186
INFO:root:Covering score (test data) (quantile cutoff): 0.9367
INFO:root:Complete cover size (test data) (quantile cutoff): 0.29442266924918786
INFO:root:Covering score (training data) (0.5 cutoff): 0.83279
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.23343646330746334
INFO:root:Covering score (test data) (0.5 cutoff): 0.8261
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.23327338440520864
INFO:root:MSE (test data): 0.02439
INFO:root:BCE (test data): 0.09139
INFO:root:#### Starting with 500 batches out of [100, 500] now. ####
INFO:root:Batch size: 4000
INFO:root:[  100] Training loss: 0.17266203, Validation loss: 0.17704359
INFO:root:[  200] Training loss: 0.14902615, Validation loss: 0.15027657
INFO:root:[  300] Training loss: 0.13580252, Validation loss: 0.13511324
INFO:root:[  400] Training loss: 0.12675202, Validation loss: 0.12799290
INFO:root:[  500] Training loss: 0.12056442, Validation loss: 0.12568407
INFO:root:[  600] Training loss: 0.11660969, Validation loss: 0.11867876
INFO:root:[  700] Training loss: 0.11214536, Validation loss: 0.11867246
INFO:root:[  800] Training loss: 0.10948846, Validation loss: 0.11313495
INFO:root:[  900] Training loss: 0.10999515, Validation loss: 0.11494887
INFO:root:[ 1000] Training loss: 0.10470257, Validation loss: 0.10667434
INFO:root:[ 1100] Training loss: 0.10307208, Validation loss: 0.10846094
INFO:root:[ 1200] Training loss: 0.10237065, Validation loss: 0.10901891
INFO:root:[ 1300] Training loss: 0.10025036, Validation loss: 0.10601295
INFO:root:[ 1400] Training loss: 0.09989346, Validation loss: 0.11854796
INFO:root:[ 1500] Training loss: 0.09899081, Validation loss: 0.10476760
INFO:root:[ 1600] Training loss: 0.09766560, Validation loss: 0.11243995
INFO:root:[ 1700] Training loss: 0.09639611, Validation loss: 0.10179951
INFO:root:[ 1800] Training loss: 0.09532481, Validation loss: 0.10537988
INFO:root:[ 1900] Training loss: 0.09443784, Validation loss: 0.09872772
INFO:root:[ 2000] Training loss: 0.09349327, Validation loss: 0.10418785
INFO:root:[ 2100] Training loss: 0.09605069, Validation loss: 0.09896781
INFO:root:[ 2200] Training loss: 0.09271614, Validation loss: 0.10178978
INFO:root:[ 2300] Training loss: 0.09170132, Validation loss: 0.10033207
INFO:root:[ 2400] Training loss: 0.09201057, Validation loss: 0.09845973
INFO:root:[ 2500] Training loss: 0.09098087, Validation loss: 0.09682324
INFO:root:[ 2600] Training loss: 0.09900372, Validation loss: 0.10336849
INFO:root:[ 2700] Training loss: 0.09182843, Validation loss: 0.09767625
INFO:root:[ 2800] Training loss: 0.08955999, Validation loss: 0.09741509
INFO:root:[ 2900] Training loss: 0.08907248, Validation loss: 0.09522319
INFO:root:[ 3000] Training loss: 0.09103437, Validation loss: 0.09271499
INFO:root:[ 3100] Training loss: 0.08854886, Validation loss: 0.12355667
INFO:root:[ 3200] Training loss: 0.08722061, Validation loss: 0.09529693
INFO:root:[ 3300] Training loss: 0.08745098, Validation loss: 0.09246983
INFO:root:[ 3400] Training loss: 0.08649618, Validation loss: 0.09505884
INFO:root:[ 3500] Training loss: 0.08586911, Validation loss: 0.09408171
INFO:root:[ 3600] Training loss: 0.08618807, Validation loss: 0.09524758
INFO:root:[ 3700] Training loss: 0.08724320, Validation loss: 0.09412787
INFO:root:[ 3800] Training loss: 0.08526926, Validation loss: 0.09389522
INFO:root:[ 3900] Training loss: 0.08537625, Validation loss: 0.09471264
INFO:root:[ 4000] Training loss: 0.08420567, Validation loss: 0.09185959
INFO:root:[ 4100] Training loss: 0.08573433, Validation loss: 0.09506389
INFO:root:[ 4200] Training loss: 0.08692852, Validation loss: 0.09217665
INFO:root:[ 4300] Training loss: 0.08398007, Validation loss: 0.09434850
INFO:root:[ 4400] Training loss: 0.08623563, Validation loss: 0.09405237
INFO:root:[ 4500] Training loss: 0.08384238, Validation loss: 0.10455049
INFO:root:[ 4600] Training loss: 0.08311325, Validation loss: 0.09661790
INFO:root:[ 4700] Training loss: 0.08660798, Validation loss: 0.09867246
INFO:root:[ 4800] Training loss: 0.08221151, Validation loss: 0.10469299
INFO:root:[ 4900] Training loss: 0.08216942, Validation loss: 0.09552979
INFO:root:[ 5000] Training loss: 0.08266250, Validation loss: 0.09838840
INFO:root:[ 5100] Training loss: 0.08142156, Validation loss: 0.09218753
INFO:root:[ 5200] Training loss: 0.08123327, Validation loss: 0.09584682
INFO:root:[ 5300] Training loss: 0.08229213, Validation loss: 0.09081630
INFO:root:[ 5400] Training loss: 0.08269636, Validation loss: 0.09331734
INFO:root:[ 5500] Training loss: 0.08115248, Validation loss: 0.09187757
INFO:root:[ 5600] Training loss: 0.08071772, Validation loss: 0.09228560
INFO:root:[ 5700] Training loss: 0.08322846, Validation loss: 0.09052520
INFO:root:[ 5800] Training loss: 0.08041562, Validation loss: 0.09429433
INFO:root:[ 5900] Training loss: 0.07969335, Validation loss: 0.08896945
INFO:root:[ 6000] Training loss: 0.07992736, Validation loss: 0.09294259
INFO:root:[ 6100] Training loss: 0.07952508, Validation loss: 0.09070797
INFO:root:[ 6200] Training loss: 0.08017231, Validation loss: 0.09176226
INFO:root:[ 6300] Training loss: 0.07996140, Validation loss: 0.09114130
INFO:root:[ 6400] Training loss: 0.07951694, Validation loss: 0.09213149
INFO:root:[ 6500] Training loss: 0.07889778, Validation loss: 0.09302913
INFO:root:[ 6600] Training loss: 0.08210992, Validation loss: 0.09847280
INFO:root:[ 6700] Training loss: 0.07871621, Validation loss: 0.08859530
INFO:root:[ 6800] Training loss: 0.07846209, Validation loss: 0.09079669
INFO:root:[ 6900] Training loss: 0.07911479, Validation loss: 0.09370209
INFO:root:[ 7000] Training loss: 0.07815496, Validation loss: 0.08845754
INFO:root:[ 7100] Training loss: 0.08338377, Validation loss: 0.09199204
INFO:root:[ 7200] Training loss: 0.07804895, Validation loss: 0.09420844
INFO:root:[ 7300] Training loss: 0.07961849, Validation loss: 0.08807861
INFO:root:[ 7400] Training loss: 0.07854291, Validation loss: 0.08854965
INFO:root:[ 7500] Training loss: 0.08240156, Validation loss: 0.10162070
INFO:root:[ 7600] Training loss: 0.07835385, Validation loss: 0.08733141
INFO:root:[ 7700] Training loss: 0.08287412, Validation loss: 0.08666259
INFO:root:[ 7800] Training loss: 0.07704706, Validation loss: 0.08641640
INFO:root:[ 7900] Training loss: 0.07734595, Validation loss: 0.09002136
INFO:root:[ 8000] Training loss: 0.08070621, Validation loss: 0.08596671
INFO:root:[ 8100] Training loss: 0.07825683, Validation loss: 0.08602342
INFO:root:[ 8200] Training loss: 0.07649999, Validation loss: 0.08732398
INFO:root:[ 8300] Training loss: 0.07715391, Validation loss: 0.08573269
INFO:root:[ 8400] Training loss: 0.07713176, Validation loss: 0.08712271
INFO:root:[ 8500] Training loss: 0.07657248, Validation loss: 0.08680911
INFO:root:[ 8600] Training loss: 0.07697926, Validation loss: 0.08722167
INFO:root:[ 8700] Training loss: 0.07704221, Validation loss: 0.08457238
INFO:root:[ 8800] Training loss: 0.07726545, Validation loss: 0.08716318
INFO:root:[ 8900] Training loss: 0.07641418, Validation loss: 0.08745100
INFO:root:[ 9000] Training loss: 0.08712265, Validation loss: 0.08958600
INFO:root:[ 9100] Training loss: 0.07616818, Validation loss: 0.08491965
INFO:root:[ 9200] Training loss: 0.07574036, Validation loss: 0.08705831
INFO:root:[ 9300] Training loss: 0.07551742, Validation loss: 0.08783871
INFO:root:[ 9400] Training loss: 0.07543560, Validation loss: 0.08978485
INFO:root:[ 9500] Training loss: 0.07554499, Validation loss: 0.08645188
INFO:root:[ 9600] Training loss: 0.07722352, Validation loss: 0.08612859
INFO:root:[ 9700] Training loss: 0.07802004, Validation loss: 0.08472014
INFO:root:[ 9800] Training loss: 0.07527374, Validation loss: 0.08503348
INFO:root:[ 9900] Training loss: 0.07496067, Validation loss: 0.08586045
INFO:root:[10000] Training loss: 0.07512742, Validation loss: 0.08537773
INFO:root:[10100] Training loss: 0.07626339, Validation loss: 0.08530471
INFO:root:[10200] Training loss: 0.07465534, Validation loss: 0.08730035
INFO:root:[10300] Training loss: 0.07473751, Validation loss: 0.08459114
INFO:root:[10400] Training loss: 0.07654987, Validation loss: 0.08436635
INFO:root:[10500] Training loss: 0.07476833, Validation loss: 0.08494213
INFO:root:[10600] Training loss: 0.08008798, Validation loss: 0.08556646
INFO:root:[10700] Training loss: 0.07529428, Validation loss: 0.08390438
INFO:root:[10800] Training loss: 0.07422976, Validation loss: 0.08724373
INFO:root:[10900] Training loss: 0.07478887, Validation loss: 0.08248918
INFO:root:[11000] Training loss: 0.07454889, Validation loss: 0.08982426
INFO:root:[11100] Training loss: 0.07564563, Validation loss: 0.08322647
INFO:root:[11200] Training loss: 0.07475609, Validation loss: 0.08276034
INFO:root:[11300] Training loss: 0.07413604, Validation loss: 0.08283038
INFO:root:[11400] Training loss: 0.07463721, Validation loss: 0.08373491
INFO:root:[11500] Training loss: 0.07404811, Validation loss: 0.08497764
INFO:root:[11600] Training loss: 0.08115895, Validation loss: 0.08320992
INFO:root:[11700] Training loss: 0.07391001, Validation loss: 0.08267538
INFO:root:[11800] Training loss: 0.07401643, Validation loss: 0.08444176
INFO:root:[11900] Training loss: 0.07403090, Validation loss: 0.09161129
INFO:root:[12000] Training loss: 0.07354813, Validation loss: 0.08228878
INFO:root:[12100] Training loss: 0.07477642, Validation loss: 0.08738333
INFO:root:[12200] Training loss: 0.07348014, Validation loss: 0.08272131
INFO:root:[12300] Training loss: 0.07803409, Validation loss: 0.08340093
INFO:root:[12400] Training loss: 0.08113009, Validation loss: 0.08135933
INFO:root:[12500] Training loss: 0.07378585, Validation loss: 0.08510443
INFO:root:[12600] Training loss: 0.07419846, Validation loss: 0.08260528
INFO:root:[12700] Training loss: 0.07320100, Validation loss: 0.08341753
INFO:root:[12800] Training loss: 0.07345755, Validation loss: 0.08196551
INFO:root:[12900] Training loss: 0.07331987, Validation loss: 0.08721398
INFO:root:[13000] Training loss: 0.07327264, Validation loss: 0.08201976
INFO:root:[13100] Training loss: 0.07312850, Validation loss: 0.08288462
INFO:root:[13200] Training loss: 0.07289675, Validation loss: 0.08336267
INFO:root:[13300] Training loss: 0.07255643, Validation loss: 0.08209363
INFO:root:[13400] Training loss: 0.07327741, Validation loss: 0.08333589
INFO:root:[13500] Training loss: 0.07272844, Validation loss: 0.08337177
INFO:root:[13600] Training loss: 0.07272567, Validation loss: 0.08085917
INFO:root:[13700] Training loss: 0.07248266, Validation loss: 0.08238032
INFO:root:[13800] Training loss: 0.07363409, Validation loss: 0.08216936
INFO:root:[13900] Training loss: 0.07287511, Validation loss: 0.08801019
INFO:root:[14000] Training loss: 0.07330434, Validation loss: 0.08304722
INFO:root:[14100] Training loss: 0.07242957, Validation loss: 0.08684808
INFO:root:[14200] Training loss: 0.07346420, Validation loss: 0.08218151
INFO:root:[14300] Training loss: 0.07470967, Validation loss: 0.08461300
INFO:root:[14400] Training loss: 0.07235803, Validation loss: 0.08746903
INFO:root:[14500] Training loss: 0.07370041, Validation loss: 0.08573671
INFO:root:[14600] Training loss: 0.07214046, Validation loss: 0.08318735
INFO:root:[14700] Training loss: 0.07459460, Validation loss: 0.08429192
INFO:root:[14800] Training loss: 0.07366389, Validation loss: 0.08189134
INFO:root:[14900] Training loss: 0.07222333, Validation loss: 0.08040859
INFO:root:[15000] Training loss: 0.07247219, Validation loss: 0.08832807
INFO:root:[15100] Training loss: 0.07413277, Validation loss: 0.08223701
INFO:root:[15200] Training loss: 0.07234344, Validation loss: 0.09132013
INFO:root:[15300] Training loss: 0.07207605, Validation loss: 0.08339418
INFO:root:[15400] Training loss: 0.07430339, Validation loss: 0.08393285
INFO:root:[15500] Training loss: 0.07300516, Validation loss: 0.08255605
INFO:root:[15600] Training loss: 0.07193892, Validation loss: 0.08488823
INFO:root:[15700] Training loss: 0.07165774, Validation loss: 0.08132125
INFO:root:[15800] Training loss: 0.07191777, Validation loss: 0.09738793
INFO:root:[15900] Training loss: 0.07255878, Validation loss: 0.08140978
INFO:root:[16000] Training loss: 0.07215001, Validation loss: 0.08042482
INFO:root:[16100] Training loss: 0.07151813, Validation loss: 0.08442996
INFO:root:[16200] Training loss: 0.07142250, Validation loss: 0.08394445
INFO:root:[16300] Training loss: 0.07172172, Validation loss: 0.08377783
INFO:root:[16400] Training loss: 0.07335772, Validation loss: 0.08087648
INFO:root:[16500] Training loss: 0.07216401, Validation loss: 0.08020028
INFO:root:[16600] Training loss: 0.07138468, Validation loss: 0.08026040
INFO:root:[16700] Training loss: 0.07134190, Validation loss: 0.08174801
INFO:root:[16800] Training loss: 0.07120790, Validation loss: 0.08398125
INFO:root:[16900] Training loss: 0.07444981, Validation loss: 0.09515188
INFO:root:[17000] Training loss: 0.07130154, Validation loss: 0.08254909
INFO:root:[17100] Training loss: 0.07138768, Validation loss: 0.08495753
INFO:root:[17200] Training loss: 0.07195126, Validation loss: 0.08615167
INFO:root:[17300] Training loss: 0.07090110, Validation loss: 0.08249903
INFO:root:[17400] Training loss: 0.07088952, Validation loss: 0.08097667
INFO:root:[17500] Training loss: 0.07100856, Validation loss: 0.08807467
INFO:root:[17600] Training loss: 0.08493720, Validation loss: 0.08103009
INFO:root:[17700] Training loss: 0.07087273, Validation loss: 0.08425475
INFO:root:[17800] Training loss: 0.07153640, Validation loss: 0.08201138
INFO:root:[17900] Training loss: 0.07091161, Validation loss: 0.08580914
INFO:root:[18000] Training loss: 0.07109383, Validation loss: 0.08265781
INFO:root:[18100] Training loss: 0.07038266, Validation loss: 0.08159535
INFO:root:[18200] Training loss: 0.07076972, Validation loss: 0.08221804
INFO:root:[18300] Training loss: 0.07085386, Validation loss: 0.08151505
INFO:root:[18400] Training loss: 0.07188833, Validation loss: 0.08189120
INFO:root:[18500] Training loss: 0.07107664, Validation loss: 0.08131389
INFO:root:[18600] Training loss: 0.07062950, Validation loss: 0.08143771
INFO:root:[18700] Training loss: 0.07075525, Validation loss: 0.08440858
INFO:root:[18800] Training loss: 0.07091558, Validation loss: 0.08145095
INFO:root:[18900] Training loss: 0.07094738, Validation loss: 0.08420021
INFO:root:[19000] Training loss: 0.07034189, Validation loss: 0.08411108
INFO:root:[19100] Training loss: 0.07053483, Validation loss: 0.08265318
INFO:root:[19200] Training loss: 0.07806342, Validation loss: 0.08422589
INFO:root:[19300] Training loss: 0.07048986, Validation loss: 0.08212944
INFO:root:[19400] Training loss: 0.07063671, Validation loss: 0.08477570
INFO:root:[19500] Training loss: 0.07102959, Validation loss: 0.08249700
INFO:root:[19600] Training loss: 0.07263091, Validation loss: 0.08183637
INFO:root:[19700] Training loss: 0.07040862, Validation loss: 0.08015431
INFO:root:[19800] Training loss: 0.07012828, Validation loss: 0.08201181
INFO:root:[19900] Training loss: 0.07221846, Validation loss: 0.08936505
INFO:root:[20000] Training loss: 0.07056183, Validation loss: 0.08160491
INFO:root:Training the model took 13832.618s.
INFO:root:Precision score (training data): 0.96232
INFO:root:Recall score (training data): 0.9376
INFO:root:F1 score (training data): 0.9498
INFO:root:Precision score (test data): 0.95563
INFO:root:Recall score (test data): 0.93222
INFO:root:F1 score (test data): 0.94378
INFO:root:Covering score (training data) (quantile cutoff): 0.96124
INFO:root:Complete cover size (training data) (quantile cutoff): 0.29661904751996904
INFO:root:Covering score (test data) (quantile cutoff): 0.9521
INFO:root:Complete cover size (test data) (quantile cutoff): 0.2957822557654508
INFO:root:Covering score (training data) (0.5 cutoff): 0.84813
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.23284212497919798
INFO:root:Covering score (test data) (0.5 cutoff): 0.8382
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.23172955653270613
INFO:root:MSE (test data): 0.02132
INFO:root:BCE (test data): 0.08063
INFO:root:###### Starting with 500 hidden neurons out of [100, 200, 500, 1000] now. ######
INFO:root:#### Starting with 100 batches out of [100, 500] now. ####
INFO:root:Batch size: 20000
INFO:root:[  100] Training loss: 0.20341404, Validation loss: 0.20583223
INFO:root:[  200] Training loss: 0.16903091, Validation loss: 0.17034841
INFO:root:[  300] Training loss: 0.15122024, Validation loss: 0.15928377
INFO:root:[  400] Training loss: 0.14052438, Validation loss: 0.14939588
INFO:root:[  500] Training loss: 0.13513737, Validation loss: 0.14341015
INFO:root:[  600] Training loss: 0.12808918, Validation loss: 0.14630322
INFO:root:[  700] Training loss: 0.12251427, Validation loss: 0.13252810
INFO:root:[  800] Training loss: 0.11909289, Validation loss: 0.13027722
INFO:root:[  900] Training loss: 0.11641204, Validation loss: 0.12791021
INFO:root:[ 1000] Training loss: 0.11305665, Validation loss: 0.14453369
INFO:root:[ 1100] Training loss: 0.10952480, Validation loss: 0.12600811
INFO:root:[ 1200] Training loss: 0.10756349, Validation loss: 0.11991651
INFO:root:[ 1300] Training loss: 0.10786287, Validation loss: 0.12306280
INFO:root:[ 1400] Training loss: 0.10362093, Validation loss: 0.12918539
INFO:root:[ 1500] Training loss: 0.10111481, Validation loss: 0.12427725
INFO:root:[ 1600] Training loss: 0.10609352, Validation loss: 0.14372140
INFO:root:[ 1700] Training loss: 0.10281510, Validation loss: 0.11873170
INFO:root:[ 1800] Training loss: 0.09820041, Validation loss: 0.12324882
INFO:root:[ 1900] Training loss: 0.10320692, Validation loss: 0.11906298
INFO:root:[ 2000] Training loss: 0.09425593, Validation loss: 0.11745904
INFO:root:[ 2100] Training loss: 0.09367861, Validation loss: 0.11350085
INFO:root:[ 2200] Training loss: 0.09648252, Validation loss: 0.11696105
INFO:root:[ 2300] Training loss: 0.09222133, Validation loss: 0.12327795
INFO:root:[ 2400] Training loss: 0.09248866, Validation loss: 0.12041990
INFO:root:[ 2500] Training loss: 0.09045018, Validation loss: 0.12159088
INFO:root:[ 2600] Training loss: 0.09156290, Validation loss: 0.11557563
INFO:root:[ 2700] Training loss: 0.09043024, Validation loss: 0.11832123
INFO:root:[ 2800] Training loss: 0.10304971, Validation loss: 0.12687527
INFO:root:[ 2900] Training loss: 0.09166118, Validation loss: 0.11176342
INFO:root:[ 3000] Training loss: 0.08845714, Validation loss: 0.12038308
INFO:root:[ 3100] Training loss: 0.08849437, Validation loss: 0.11438549
INFO:root:[ 3200] Training loss: 0.09727706, Validation loss: 0.12574500
INFO:root:[ 3300] Training loss: 0.09039461, Validation loss: 0.11510285
INFO:root:[ 3400] Training loss: 0.08637006, Validation loss: 0.11920211
INFO:root:[ 3500] Training loss: 0.08434701, Validation loss: 0.11869606
INFO:root:[ 3600] Training loss: 0.08388273, Validation loss: 0.11229988
INFO:root:[ 3700] Training loss: 0.08466201, Validation loss: 0.11460945
INFO:root:[ 3800] Training loss: 0.08436507, Validation loss: 0.11167016
INFO:root:[ 3900] Training loss: 0.08931808, Validation loss: 0.11301038
INFO:root:[ 4000] Training loss: 0.08178925, Validation loss: 0.11262991
INFO:root:[ 4100] Training loss: 0.09745800, Validation loss: 0.12641567
INFO:root:[ 4200] Training loss: 0.08243231, Validation loss: 0.12211155
INFO:root:[ 4300] Training loss: 0.08120957, Validation loss: 0.13356392
INFO:root:[ 4400] Training loss: 0.08676248, Validation loss: 0.11691108
INFO:root:[ 4500] Training loss: 0.08245326, Validation loss: 0.11659324
INFO:root:[ 4600] Training loss: 0.08149677, Validation loss: 0.11425111
INFO:root:[ 4700] Training loss: 0.08022857, Validation loss: 0.12504706
INFO:root:[ 4800] Training loss: 0.08722706, Validation loss: 0.12171126
INFO:root:[ 4900] Training loss: 0.08358419, Validation loss: 0.11816203
INFO:root:[ 5000] Training loss: 0.07857225, Validation loss: 0.12359417
INFO:root:[ 5100] Training loss: 0.07983031, Validation loss: 0.12308688
INFO:root:[ 5200] Training loss: 0.07757549, Validation loss: 0.11459818
INFO:root:[ 5300] Training loss: 0.08196258, Validation loss: 0.13285884
INFO:root:[ 5400] Training loss: 0.08072923, Validation loss: 0.12579034
INFO:root:[ 5500] Training loss: 0.08243623, Validation loss: 0.11522001
INFO:root:[ 5600] Training loss: 0.08072917, Validation loss: 0.17544791
INFO:root:[ 5700] Training loss: 0.07678871, Validation loss: 0.11145183
INFO:root:[ 5800] Training loss: 0.08459393, Validation loss: 0.11358042
INFO:root:[ 5900] Training loss: 0.08207846, Validation loss: 0.11345352
INFO:root:[ 6000] Training loss: 0.09237192, Validation loss: 0.11799185
INFO:root:[ 6100] Training loss: 0.08037341, Validation loss: 0.12940677
INFO:root:[ 6200] Training loss: 0.08001416, Validation loss: 0.11482368
INFO:root:[ 6300] Training loss: 0.07606289, Validation loss: 0.12034778
INFO:root:[ 6400] Training loss: 0.07722070, Validation loss: 0.11490639
INFO:root:[ 6500] Training loss: 0.07856385, Validation loss: 0.13745183
INFO:root:[ 6600] Training loss: 0.07598517, Validation loss: 0.11491255
INFO:root:[ 6700] Training loss: 0.07578260, Validation loss: 0.12722154
INFO:root:[ 6800] Training loss: 0.08347231, Validation loss: 0.12260786
INFO:root:[ 6900] Training loss: 0.07556187, Validation loss: 0.11319973
INFO:root:[ 7000] Training loss: 0.07433451, Validation loss: 0.11895739
INFO:root:[ 7100] Training loss: 0.07266606, Validation loss: 0.11616626
INFO:root:[ 7200] Training loss: 0.07425087, Validation loss: 0.11820079
INFO:root:[ 7300] Training loss: 0.07233165, Validation loss: 0.11263810
INFO:root:[ 7400] Training loss: 0.07694770, Validation loss: 0.11662254
INFO:root:[ 7500] Training loss: 0.07396920, Validation loss: 0.11427251
INFO:root:[ 7600] Training loss: 0.07349381, Validation loss: 0.11823775
INFO:root:[ 7700] Training loss: 0.07474603, Validation loss: 0.11335356
INFO:root:[ 7800] Training loss: 0.07393973, Validation loss: 0.11679905
INFO:root:[ 7900] Training loss: 0.07336706, Validation loss: 0.13288173
INFO:root:[ 8000] Training loss: 0.07358518, Validation loss: 0.13145271
INFO:root:[ 8100] Training loss: 0.07196966, Validation loss: 0.11642020
INFO:root:[ 8200] Training loss: 0.07330906, Validation loss: 0.13602975
INFO:root:[ 8300] Training loss: 0.07213736, Validation loss: 0.12019982
INFO:root:[ 8400] Training loss: 0.07325803, Validation loss: 0.11699980
INFO:root:[ 8500] Training loss: 0.07225354, Validation loss: 0.12388273
INFO:root:[ 8600] Training loss: 0.07434335, Validation loss: 0.11605190
INFO:root:[ 8700] Training loss: 0.07312082, Validation loss: 0.17131485
INFO:root:[ 8800] Training loss: 0.07685965, Validation loss: 0.12055534
INFO:root:[ 8900] Training loss: 0.07107381, Validation loss: 0.11379460
INFO:root:[ 9000] Training loss: 0.07254146, Validation loss: 0.11310502
INFO:root:[ 9100] Training loss: 0.07827278, Validation loss: 0.12336247
INFO:root:[ 9200] Training loss: 0.07125320, Validation loss: 0.11600984
INFO:root:[ 9300] Training loss: 0.07113358, Validation loss: 0.13165639
INFO:root:[ 9400] Training loss: 0.07510693, Validation loss: 0.11648648
INFO:root:[ 9500] Training loss: 0.07460020, Validation loss: 0.11643054
INFO:root:[ 9600] Training loss: 0.07816603, Validation loss: 0.12026657
INFO:root:[ 9700] Training loss: 0.07905488, Validation loss: 0.12194085
INFO:root:[ 9800] Training loss: 0.07205565, Validation loss: 0.11875817
INFO:root:[ 9900] Training loss: 0.07088012, Validation loss: 0.12048640
INFO:root:[10000] Training loss: 0.07026210, Validation loss: 0.12590240
INFO:root:[10100] Training loss: 0.07055926, Validation loss: 0.11917702
INFO:root:[10200] Training loss: 0.07078638, Validation loss: 0.12732698
INFO:root:[10300] Training loss: 0.07096480, Validation loss: 0.12313557
INFO:root:[10400] Training loss: 0.07030343, Validation loss: 0.11883222
INFO:root:[10500] Training loss: 0.07565060, Validation loss: 0.13014174
INFO:root:[10600] Training loss: 0.06991388, Validation loss: 0.11943947
INFO:root:[10700] Training loss: 0.07085490, Validation loss: 0.11742120
INFO:root:[10800] Training loss: 0.06888795, Validation loss: 0.11941665
INFO:root:[10900] Training loss: 0.06792329, Validation loss: 0.13226511
INFO:root:[11000] Training loss: 0.06784253, Validation loss: 0.12032513
INFO:root:[11100] Training loss: 0.07181628, Validation loss: 0.11966603
INFO:root:[11200] Training loss: 0.06846204, Validation loss: 0.12185593
INFO:root:[11300] Training loss: 0.07090385, Validation loss: 0.13282697
INFO:root:[11400] Training loss: 0.06736851, Validation loss: 0.11974554
INFO:root:[11500] Training loss: 0.07703179, Validation loss: 0.12816039
INFO:root:[11600] Training loss: 0.06952762, Validation loss: 0.11751190
INFO:root:[11700] Training loss: 0.06954909, Validation loss: 0.14535391
INFO:root:[11800] Training loss: 0.07573889, Validation loss: 0.13528712
INFO:root:[11900] Training loss: 0.06717800, Validation loss: 0.13123278
INFO:root:[12000] Training loss: 0.06781136, Validation loss: 0.12023647
INFO:root:[12100] Training loss: 0.06667978, Validation loss: 0.12132581
INFO:root:[12200] Training loss: 0.06936691, Validation loss: 0.12361924
INFO:root:[12300] Training loss: 0.06847639, Validation loss: 0.12130129
INFO:root:[12400] Training loss: 0.07120725, Validation loss: 0.15813930
INFO:root:[12500] Training loss: 0.06841394, Validation loss: 0.11969152
INFO:root:[12600] Training loss: 0.06698479, Validation loss: 0.12002649
INFO:root:[12700] Training loss: 0.06761213, Validation loss: 0.12264753
INFO:root:[12800] Training loss: 0.06852350, Validation loss: 0.12279343
INFO:root:[12900] Training loss: 0.06587099, Validation loss: 0.12771498
INFO:root:[13000] Training loss: 0.07161115, Validation loss: 0.12834772
INFO:root:[13100] Training loss: 0.06582609, Validation loss: 0.12480095
INFO:root:[13200] Training loss: 0.06716013, Validation loss: 0.12676072
INFO:root:[13300] Training loss: 0.06854967, Validation loss: 0.12186038
INFO:root:[13400] Training loss: 0.06881472, Validation loss: 0.12524013
INFO:root:[13500] Training loss: 0.06670624, Validation loss: 0.12271637
INFO:root:[13600] Training loss: 0.06682629, Validation loss: 0.12032431
INFO:root:[13700] Training loss: 0.06804289, Validation loss: 0.13266493
INFO:root:[13800] Training loss: 0.06617073, Validation loss: 0.12249051
INFO:root:[13900] Training loss: 0.06624023, Validation loss: 0.12608255
INFO:root:[14000] Training loss: 0.07329267, Validation loss: 0.12266231
INFO:root:[14100] Training loss: 0.06534539, Validation loss: 0.13927385
INFO:root:[14200] Training loss: 0.06680416, Validation loss: 0.12754691
INFO:root:[14300] Training loss: 0.06699764, Validation loss: 0.12971599
INFO:root:[14400] Training loss: 0.06543495, Validation loss: 0.13049918
INFO:root:[14500] Training loss: 0.06694228, Validation loss: 0.12053295
INFO:root:[14600] Training loss: 0.06601484, Validation loss: 0.12643714
INFO:root:[14700] Training loss: 0.07437820, Validation loss: 0.12316912
INFO:root:[14800] Training loss: 0.07360376, Validation loss: 0.14676733
INFO:root:[14900] Training loss: 0.06698082, Validation loss: 0.12349080
INFO:root:[15000] Training loss: 0.07300812, Validation loss: 0.12414756
INFO:root:[15100] Training loss: 0.06664317, Validation loss: 0.12742960
INFO:root:[15200] Training loss: 0.06681124, Validation loss: 0.13485396
INFO:root:[15300] Training loss: 0.06527828, Validation loss: 0.13308543
INFO:root:[15400] Training loss: 0.06567876, Validation loss: 0.13343582
INFO:root:[15500] Training loss: 0.06668593, Validation loss: 0.12791157
INFO:root:[15600] Training loss: 0.07056903, Validation loss: 0.13119149
INFO:root:[15700] Training loss: 0.06666330, Validation loss: 0.12027355
INFO:root:[15800] Training loss: 0.06845313, Validation loss: 0.12683666
INFO:root:[15900] Training loss: 0.06994508, Validation loss: 0.12680791
INFO:root:[16000] Training loss: 0.06441964, Validation loss: 0.12871522
INFO:root:[16100] Training loss: 0.06582198, Validation loss: 0.12349267
INFO:root:[16200] Training loss: 0.06428364, Validation loss: 0.14022282
INFO:root:[16300] Training loss: 0.06689679, Validation loss: 0.14136152
INFO:root:[16400] Training loss: 0.06528978, Validation loss: 0.12305864
INFO:root:[16500] Training loss: 0.06519593, Validation loss: 0.13018569
INFO:root:[16600] Training loss: 0.06587374, Validation loss: 0.14527898
INFO:root:[16700] Training loss: 0.06361000, Validation loss: 0.12329432
INFO:root:[16800] Training loss: 0.06554208, Validation loss: 0.13190350
INFO:root:[16900] Training loss: 0.06323835, Validation loss: 0.12580195
INFO:root:[17000] Training loss: 0.06407159, Validation loss: 0.12736571
INFO:root:[17100] Training loss: 0.06405944, Validation loss: 0.12820053
INFO:root:[17200] Training loss: 0.07038123, Validation loss: 0.13405140
INFO:root:[17300] Training loss: 0.06669220, Validation loss: 0.13051684
INFO:root:[17400] Training loss: 0.06401652, Validation loss: 0.14276135
INFO:root:[17500] Training loss: 0.06337410, Validation loss: 0.12460949
INFO:root:[17600] Training loss: 0.07046195, Validation loss: 0.12900612
INFO:root:[17700] Training loss: 0.06685975, Validation loss: 0.12528399
INFO:root:[17800] Training loss: 0.07263519, Validation loss: 0.12877642
INFO:root:[17900] Training loss: 0.06400312, Validation loss: 0.12801759
INFO:root:[18000] Training loss: 0.06339498, Validation loss: 0.13430145
INFO:root:[18100] Training loss: 0.06377169, Validation loss: 0.13679576
INFO:root:[18200] Training loss: 0.06443287, Validation loss: 0.12902796
INFO:root:[18300] Training loss: 0.06556624, Validation loss: 0.12719673
INFO:root:[18400] Training loss: 0.06398863, Validation loss: 0.12639481
INFO:root:[18500] Training loss: 0.06305807, Validation loss: 0.12723900
INFO:root:[18600] Training loss: 0.06666508, Validation loss: 0.14494815
INFO:root:[18700] Training loss: 0.06351398, Validation loss: 0.12571231
INFO:root:[18800] Training loss: 0.06675888, Validation loss: 0.24375661
INFO:root:[18900] Training loss: 0.06351627, Validation loss: 0.14019179
INFO:root:[19000] Training loss: 0.06384745, Validation loss: 0.13061638
INFO:root:[19100] Training loss: 0.06736514, Validation loss: 0.12816048
INFO:root:[19200] Training loss: 0.06858568, Validation loss: 0.13261573
INFO:root:[19300] Training loss: 0.06305017, Validation loss: 0.12955447
INFO:root:[19400] Training loss: 0.06315323, Validation loss: 0.13935447
INFO:root:[19500] Training loss: 0.06165887, Validation loss: 0.13006181
INFO:root:[19600] Training loss: 0.06118508, Validation loss: 0.16372643
INFO:root:[19700] Training loss: 0.06176640, Validation loss: 0.13054606
INFO:root:[19800] Training loss: 0.06271806, Validation loss: 0.15196827
INFO:root:[19900] Training loss: 0.06256964, Validation loss: 0.13117735
INFO:root:[20000] Training loss: 0.06499608, Validation loss: 0.13931409
INFO:root:Training the model took 7823.046s.
INFO:root:Precision score (training data): 0.96846
INFO:root:Recall score (training data): 0.94415
INFO:root:F1 score (training data): 0.95615
INFO:root:Precision score (test data): 0.93926
INFO:root:Recall score (test data): 0.91867
INFO:root:F1 score (test data): 0.92885
INFO:root:Covering score (training data) (quantile cutoff): 0.96364
INFO:root:Complete cover size (training data) (quantile cutoff): 0.2975005664930953
INFO:root:Covering score (test data) (quantile cutoff): 0.9214
INFO:root:Complete cover size (test data) (quantile cutoff): 0.29128810195664984
INFO:root:Covering score (training data) (0.5 cutoff): 0.86046
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.23739096732055767
INFO:root:Covering score (test data) (0.5 cutoff): 0.8111
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.22989062472480054
INFO:root:MSE (test data): 0.02827
INFO:root:BCE (test data): 0.14616
INFO:root:#### Starting with 500 batches out of [100, 500] now. ####
INFO:root:Batch size: 4000
INFO:root:[  100] Training loss: 0.13968268, Validation loss: 0.14431185
INFO:root:[  200] Training loss: 0.11817996, Validation loss: 0.13079886
INFO:root:[  300] Training loss: 0.10621948, Validation loss: 0.12811378
INFO:root:[  400] Training loss: 0.09930878, Validation loss: 0.12431139
INFO:root:[  500] Training loss: 0.09536082, Validation loss: 0.12185431
INFO:root:[  600] Training loss: 0.09074334, Validation loss: 0.12174571
INFO:root:[  700] Training loss: 0.08728210, Validation loss: 0.12167602
INFO:root:[  800] Training loss: 0.08518105, Validation loss: 0.12062716
INFO:root:[  900] Training loss: 0.08262238, Validation loss: 0.12432810
INFO:root:[ 1000] Training loss: 0.08493727, Validation loss: 0.12349007
INFO:root:[ 1100] Training loss: 0.07991611, Validation loss: 0.11936359
INFO:root:[ 1200] Training loss: 0.07946168, Validation loss: 0.11905523
INFO:root:[ 1300] Training loss: 0.07700095, Validation loss: 0.12627257
INFO:root:[ 1400] Training loss: 0.07648704, Validation loss: 0.11948040
INFO:root:[ 1500] Training loss: 0.07450358, Validation loss: 0.12140590
INFO:root:[ 1600] Training loss: 0.07588763, Validation loss: 0.12886158
INFO:root:[ 1700] Training loss: 0.07268362, Validation loss: 0.12254219
INFO:root:[ 1800] Training loss: 0.07346252, Validation loss: 0.12532930
INFO:root:[ 1900] Training loss: 0.07182681, Validation loss: 0.12902084
INFO:root:[ 2000] Training loss: 0.07146685, Validation loss: 0.12605667
INFO:root:[ 2100] Training loss: 0.07044927, Validation loss: 0.12253087
INFO:root:[ 2200] Training loss: 0.06866536, Validation loss: 0.12597866
INFO:root:[ 2300] Training loss: 0.06862797, Validation loss: 0.12864922
INFO:root:[ 2400] Training loss: 0.06795806, Validation loss: 0.12766777
INFO:root:[ 2500] Training loss: 0.06844741, Validation loss: 0.13348044
INFO:root:[ 2600] Training loss: 0.07409501, Validation loss: 0.12905253
INFO:root:[ 2700] Training loss: 0.07412177, Validation loss: 0.12910466
INFO:root:[ 2800] Training loss: 0.06681880, Validation loss: 0.13556510
INFO:root:[ 2900] Training loss: 0.06858504, Validation loss: 0.13533865
INFO:root:[ 3000] Training loss: 0.06631484, Validation loss: 0.12952678
INFO:root:[ 3100] Training loss: 0.06687193, Validation loss: 0.13110794
INFO:root:[ 3200] Training loss: 0.06634719, Validation loss: 0.13242857
INFO:root:[ 3300] Training loss: 0.06453357, Validation loss: 0.13180904
INFO:root:[ 3400] Training loss: 0.06525776, Validation loss: 0.13389383
INFO:root:[ 3500] Training loss: 0.07035341, Validation loss: 0.13488139
INFO:root:[ 3600] Training loss: 0.06474636, Validation loss: 0.13408767
INFO:root:[ 3700] Training loss: 0.06529429, Validation loss: 0.13694811
INFO:root:[ 3800] Training loss: 0.06294488, Validation loss: 0.14104825
INFO:root:[ 3900] Training loss: 0.06246161, Validation loss: 0.13798448
INFO:root:[ 4000] Training loss: 0.06473236, Validation loss: 0.13753097
INFO:root:[ 4100] Training loss: 0.06234759, Validation loss: 0.13932900
INFO:root:[ 4200] Training loss: 0.06124264, Validation loss: 0.13773701
INFO:root:[ 4300] Training loss: 0.06087524, Validation loss: 0.14023158
INFO:root:[ 4400] Training loss: 0.06269657, Validation loss: 0.13856129
INFO:root:[ 4500] Training loss: 0.06103924, Validation loss: 0.14472882
INFO:root:[ 4600] Training loss: 0.06227709, Validation loss: 0.14247961
INFO:root:[ 4700] Training loss: 0.06054744, Validation loss: 0.14128919
INFO:root:[ 4800] Training loss: 0.06053384, Validation loss: 0.14308043
INFO:root:[ 4900] Training loss: 0.06040863, Validation loss: 0.14821957
INFO:root:[ 5000] Training loss: 0.06115855, Validation loss: 0.14726922
INFO:root:[ 5100] Training loss: 0.06096305, Validation loss: 0.14715002
INFO:root:[ 5200] Training loss: 0.06187653, Validation loss: 0.14715102
INFO:root:[ 5300] Training loss: 0.06026329, Validation loss: 0.14737271
INFO:root:[ 5400] Training loss: 0.05915748, Validation loss: 0.15403424
INFO:root:[ 5500] Training loss: 0.05949648, Validation loss: 0.14654931
INFO:root:[ 5600] Training loss: 0.05928429, Validation loss: 0.15003730
INFO:root:[ 5700] Training loss: 0.05975631, Validation loss: 0.15077232
INFO:root:[ 5800] Training loss: 0.05833250, Validation loss: 0.15309526
INFO:root:[ 5900] Training loss: 0.05793978, Validation loss: 0.15151387
INFO:root:[ 6000] Training loss: 0.05852686, Validation loss: 0.15394469
INFO:root:[ 6100] Training loss: 0.05869030, Validation loss: 0.15336046
INFO:root:[ 6200] Training loss: 0.05725279, Validation loss: 0.15531631
INFO:root:[ 6300] Training loss: 0.05739181, Validation loss: 0.15852587
INFO:root:[ 6400] Training loss: 0.05775409, Validation loss: 0.15354341
INFO:root:[ 6500] Training loss: 0.05681199, Validation loss: 0.15660952
INFO:root:[ 6600] Training loss: 0.05657185, Validation loss: 0.15800960
INFO:root:[ 6700] Training loss: 0.05690405, Validation loss: 0.15816545
INFO:root:[ 6800] Training loss: 0.05676272, Validation loss: 0.15735359
INFO:root:[ 6900] Training loss: 0.05625011, Validation loss: 0.15747805
INFO:root:[ 7000] Training loss: 0.05617294, Validation loss: 0.15880081
INFO:root:[ 7100] Training loss: 0.05655418, Validation loss: 0.15804622
INFO:root:[ 7200] Training loss: 0.05893398, Validation loss: 0.15463555
INFO:root:[ 7300] Training loss: 0.05617234, Validation loss: 0.16101129
INFO:root:[ 7400] Training loss: 0.05690646, Validation loss: 0.16169156
INFO:root:[ 7500] Training loss: 0.05625374, Validation loss: 0.15876041
INFO:root:[ 7600] Training loss: 0.05701367, Validation loss: 0.16328286
INFO:root:[ 7700] Training loss: 0.05587004, Validation loss: 0.15989162
INFO:root:[ 7800] Training loss: 0.05608648, Validation loss: 0.15604925
INFO:root:[ 7900] Training loss: 0.05539185, Validation loss: 0.16081572
INFO:root:[ 8000] Training loss: 0.06875439, Validation loss: 0.16327870
INFO:root:[ 8100] Training loss: 0.05514980, Validation loss: 0.16318592
INFO:root:[ 8200] Training loss: 0.05543475, Validation loss: 0.16001782
INFO:root:[ 8300] Training loss: 0.05603695, Validation loss: 0.16303369
INFO:root:[ 8400] Training loss: 0.05794205, Validation loss: 0.16151926
INFO:root:[ 8500] Training loss: 0.05572892, Validation loss: 0.16802171
INFO:root:[ 8600] Training loss: 0.05756096, Validation loss: 0.16529322
INFO:root:[ 8700] Training loss: 0.05597688, Validation loss: 0.16457278
INFO:root:[ 8800] Training loss: 0.05431376, Validation loss: 0.17088848
INFO:root:[ 8900] Training loss: 0.05445386, Validation loss: 0.16495477
INFO:root:[ 9000] Training loss: 0.05424583, Validation loss: 0.16439874
INFO:root:[ 9100] Training loss: 0.05588070, Validation loss: 0.16468981
INFO:root:[ 9200] Training loss: 0.05643204, Validation loss: 0.18045087
INFO:root:[ 9300] Training loss: 0.05375680, Validation loss: 0.16641769
INFO:root:[ 9400] Training loss: 0.05359051, Validation loss: 0.16674352
INFO:root:[ 9500] Training loss: 0.05363025, Validation loss: 0.16982523
INFO:root:[ 9600] Training loss: 0.05558290, Validation loss: 0.16423474
INFO:root:[ 9700] Training loss: 0.05438450, Validation loss: 0.16396134
INFO:root:[ 9800] Training loss: 0.05475954, Validation loss: 0.16402015
INFO:root:[ 9900] Training loss: 0.05350709, Validation loss: 0.16742413
INFO:root:[10000] Training loss: 0.05369234, Validation loss: 0.16672397
INFO:root:[10100] Training loss: 0.05331820, Validation loss: 0.16981873
INFO:root:[10200] Training loss: 0.05399766, Validation loss: 0.16698717
INFO:root:[10300] Training loss: 0.05310528, Validation loss: 0.16791518
INFO:root:[10400] Training loss: 0.05294405, Validation loss: 0.16759151
INFO:root:[10500] Training loss: 0.05315504, Validation loss: 0.16711031
INFO:root:[10600] Training loss: 0.05288171, Validation loss: 0.17440581
INFO:root:[10700] Training loss: 0.05250985, Validation loss: 0.17384532
INFO:root:[10800] Training loss: 0.05261492, Validation loss: 0.17357467
INFO:root:[10900] Training loss: 0.05278653, Validation loss: 0.17854208
INFO:root:[11000] Training loss: 0.05389679, Validation loss: 0.17024551
INFO:root:[11100] Training loss: 0.05248459, Validation loss: 0.17536621
INFO:root:[11200] Training loss: 0.05298293, Validation loss: 0.17756771
INFO:root:[11300] Training loss: 0.05250452, Validation loss: 0.17276031
INFO:root:[11400] Training loss: 0.05223802, Validation loss: 0.17041574
INFO:root:[11500] Training loss: 0.05250964, Validation loss: 0.18094204
INFO:root:[11600] Training loss: 0.05235641, Validation loss: 0.17095920
INFO:root:[11700] Training loss: 0.05297268, Validation loss: 0.17945516
INFO:root:[11800] Training loss: 0.05299167, Validation loss: 0.17527264
INFO:root:[11900] Training loss: 0.05172508, Validation loss: 0.17526291
INFO:root:[12000] Training loss: 0.05185553, Validation loss: 0.17159623
INFO:root:[12100] Training loss: 0.05195234, Validation loss: 0.17680286
INFO:root:[12200] Training loss: 0.05206379, Validation loss: 0.17400984
INFO:root:[12300] Training loss: 0.05169215, Validation loss: 0.18125917
INFO:root:[12400] Training loss: 0.05158107, Validation loss: 0.17315616
INFO:root:[12500] Training loss: 0.05236434, Validation loss: 0.17729141
INFO:root:[12600] Training loss: 0.05186637, Validation loss: 0.17988317
INFO:root:[12700] Training loss: 0.05212088, Validation loss: 0.18064618
INFO:root:[12800] Training loss: 0.05154447, Validation loss: 0.17720275
INFO:root:[12900] Training loss: 0.05298315, Validation loss: 0.18277417
INFO:root:[13000] Training loss: 0.05129437, Validation loss: 0.17744498
INFO:root:[13100] Training loss: 0.05122946, Validation loss: 0.18121813
INFO:root:[13200] Training loss: 0.05134727, Validation loss: 0.18016374
INFO:root:[13300] Training loss: 0.05124670, Validation loss: 0.18257987
INFO:root:[13400] Training loss: 0.05133651, Validation loss: 0.17961390
INFO:root:[13500] Training loss: 0.05199754, Validation loss: 0.18051760
INFO:root:[13600] Training loss: 0.05107153, Validation loss: 0.18004331
INFO:root:[13700] Training loss: 0.05105898, Validation loss: 0.17934246
INFO:root:[13800] Training loss: 0.05161252, Validation loss: 0.18200244
INFO:root:[13900] Training loss: 0.05089046, Validation loss: 0.18151961
INFO:root:[14000] Training loss: 0.05063897, Validation loss: 0.18637779
INFO:root:[14100] Training loss: 0.05132536, Validation loss: 0.18144923
INFO:root:[14200] Training loss: 0.05068166, Validation loss: 0.18463501
INFO:root:[14300] Training loss: 0.05033035, Validation loss: 0.18455143
INFO:root:[14400] Training loss: 0.05182115, Validation loss: 0.18199921
INFO:root:[14500] Training loss: 0.05096338, Validation loss: 0.18826956
INFO:root:[14600] Training loss: 0.05058559, Validation loss: 0.18909059
INFO:root:[14700] Training loss: 0.05039310, Validation loss: 0.18315539
INFO:root:[14800] Training loss: 0.05040693, Validation loss: 0.18918897
INFO:root:[14900] Training loss: 0.05053798, Validation loss: 0.18449289
INFO:root:[15000] Training loss: 0.05004924, Validation loss: 0.18149726
INFO:root:[15100] Training loss: 0.05003511, Validation loss: 0.18110023
INFO:root:[15200] Training loss: 0.05026776, Validation loss: 0.18358691
INFO:root:[15300] Training loss: 0.05023416, Validation loss: 0.18432160
INFO:root:[15400] Training loss: 0.05017251, Validation loss: 0.18769790
INFO:root:[15500] Training loss: 0.05012077, Validation loss: 0.18373743
INFO:root:[15600] Training loss: 0.04994921, Validation loss: 0.19197068
INFO:root:[15700] Training loss: 0.05016519, Validation loss: 0.18320052
INFO:root:[15800] Training loss: 0.04959834, Validation loss: 0.19012132
INFO:root:[15900] Training loss: 0.04956348, Validation loss: 0.18781777
INFO:root:[16000] Training loss: 0.05118647, Validation loss: 0.18397446
INFO:root:[16100] Training loss: 0.05017535, Validation loss: 0.18742752
INFO:root:[16200] Training loss: 0.05112109, Validation loss: 0.18767031
INFO:root:[16300] Training loss: 0.05019204, Validation loss: 0.18708637
INFO:root:[16400] Training loss: 0.04976778, Validation loss: 0.18883003
INFO:root:[16500] Training loss: 0.04986179, Validation loss: 0.18698047
INFO:root:[16600] Training loss: 0.04979129, Validation loss: 0.18834259
INFO:root:[16700] Training loss: 0.04943296, Validation loss: 0.19064869
INFO:root:[16800] Training loss: 0.04930607, Validation loss: 0.18970934
INFO:root:[16900] Training loss: 0.04932351, Validation loss: 0.18863349
INFO:root:[17000] Training loss: 0.04955736, Validation loss: 0.19177793
INFO:root:[17100] Training loss: 0.05008539, Validation loss: 0.19388621
INFO:root:[17200] Training loss: 0.04896686, Validation loss: 0.19351049
INFO:root:[17300] Training loss: 0.04970873, Validation loss: 0.19378367
INFO:root:[17400] Training loss: 0.04937851, Validation loss: 0.19228032
INFO:root:[17500] Training loss: 0.04918270, Validation loss: 0.19307262
INFO:root:[17600] Training loss: 0.04895840, Validation loss: 0.19096117
INFO:root:[17700] Training loss: 0.04992811, Validation loss: 0.19206616
INFO:root:[17800] Training loss: 0.05064559, Validation loss: 0.19187650
INFO:root:[17900] Training loss: 0.04932601, Validation loss: 0.20050000
INFO:root:[18000] Training loss: 0.05004534, Validation loss: 0.18938068
INFO:root:[18100] Training loss: 0.04871880, Validation loss: 0.20221975
INFO:root:[18200] Training loss: 0.04913263, Validation loss: 0.19080745
INFO:root:[18300] Training loss: 0.04910510, Validation loss: 0.19317199
INFO:root:[18400] Training loss: 0.05004855, Validation loss: 0.19009580
INFO:root:[18500] Training loss: 0.04878990, Validation loss: 0.19395672
INFO:root:[18600] Training loss: 0.04870681, Validation loss: 0.19613558
INFO:root:[18700] Training loss: 0.04883139, Validation loss: 0.19690491
INFO:root:[18800] Training loss: 0.04902540, Validation loss: 0.19277342
INFO:root:[18900] Training loss: 0.04883118, Validation loss: 0.19766326
INFO:root:[19000] Training loss: 0.04863755, Validation loss: 0.19576859
INFO:root:[19100] Training loss: 0.04848703, Validation loss: 0.19699717
INFO:root:[19200] Training loss: 0.04854932, Validation loss: 0.20349692
INFO:root:[19300] Training loss: 0.04901993, Validation loss: 0.19799370
INFO:root:[19400] Training loss: 0.04853382, Validation loss: 0.19379905
INFO:root:[19500] Training loss: 0.04851276, Validation loss: 0.19579586
INFO:root:[19600] Training loss: 0.04940967, Validation loss: 0.19672737
INFO:root:[19700] Training loss: 0.04848585, Validation loss: 0.19526315
INFO:root:[19800] Training loss: 0.04808362, Validation loss: 0.19478889
INFO:root:[19900] Training loss: 0.04829080, Validation loss: 0.19794522
INFO:root:[20000] Training loss: 0.04843848, Validation loss: 0.20966175
INFO:root:Training the model took 12250.464s.
INFO:root:Precision score (training data): 0.97171
INFO:root:Recall score (training data): 0.96026
INFO:root:F1 score (training data): 0.96595
INFO:root:Precision score (test data): 0.93794
INFO:root:Recall score (test data): 0.92947
INFO:root:F1 score (test data): 0.93369
INFO:root:Covering score (training data) (quantile cutoff): 0.97668
INFO:root:Complete cover size (training data) (quantile cutoff): 0.2983884992895063
INFO:root:Covering score (test data) (quantile cutoff): 0.9311
INFO:root:Complete cover size (test data) (quantile cutoff): 0.29169799162281174
INFO:root:Covering score (training data) (0.5 cutoff): 0.8992
INFO:root:Complete cover size (training data) (0.5 cutoff): 0.2433698193677175
INFO:root:Covering score (test data) (0.5 cutoff): 0.8361
INFO:root:Complete cover size (test data) (0.5 cutoff): 0.23351615493703762
INFO:root:MSE (test data): 0.02713
INFO:root:BCE (test data): 0.20957
INFO:root:###### Starting with 1000 hidden neurons out of [100, 200, 500, 1000] now. ######
INFO:root:#### Starting with 100 batches out of [100, 500] now. ####
INFO:root:Batch size: 20000
INFO:root:[  100] Training loss: 0.17431011, Validation loss: 0.17992742
INFO:root:[  200] Training loss: 0.13892417, Validation loss: 0.15082866
INFO:root:[  300] Training loss: 0.12285358, Validation loss: 0.14712460
INFO:root:[  400] Training loss: 0.11318840, Validation loss: 0.14465748
INFO:root:[  500] Training loss: 0.10552956, Validation loss: 0.14071639
INFO:root:[  600] Training loss: 0.09883679, Validation loss: 0.14829987
INFO:root:[  700] Training loss: 0.09540435, Validation loss: 0.15171716
INFO:root:[  800] Training loss: 0.09540707, Validation loss: 0.14573358
INFO:root:[  900] Training loss: 0.09007975, Validation loss: 0.15107839
INFO:root:[ 1000] Training loss: 0.08542035, Validation loss: 0.14680478
INFO:root:[ 1100] Training loss: 0.08550806, Validation loss: 0.15629773
INFO:root:[ 1200] Training loss: 0.10148039, Validation loss: 0.16350582
INFO:root:[ 1300] Training loss: 0.08430949, Validation loss: 0.15709749
INFO:root:[ 1400] Training loss: 0.08187120, Validation loss: 0.16270055
INFO:root:[ 1500] Training loss: 0.08317512, Validation loss: 0.16818047
INFO:root:[ 1600] Training loss: 0.08467725, Validation loss: 0.18323438
INFO:root:[ 1700] Training loss: 0.07651393, Validation loss: 0.17362115
INFO:root:[ 1800] Training loss: 0.07264156, Validation loss: 0.17288515
INFO:root:[ 1900] Training loss: 0.08386242, Validation loss: 0.18077639
INFO:root:[ 2000] Training loss: 0.07544930, Validation loss: 0.18765412
INFO:root:[ 2100] Training loss: 0.08026261, Validation loss: 0.18307540
INFO:root:[ 2200] Training loss: 0.07176559, Validation loss: 0.18844612
INFO:root:[ 2300] Training loss: 0.07605913, Validation loss: 0.19279552
INFO:root:[ 2400] Training loss: 0.08182683, Validation loss: 0.20047291
INFO:root:[ 2500] Training loss: 0.06891572, Validation loss: 0.20468259
INFO:root:[ 2600] Training loss: 0.07064102, Validation loss: 0.20141397
INFO:root:[ 2700] Training loss: 0.06891005, Validation loss: 0.20890078
INFO:root:[ 2800] Training loss: 0.06164727, Validation loss: 0.21329662
INFO:root:[ 2900] Training loss: 0.06468804, Validation loss: 0.22778518
INFO:root:[ 3000] Training loss: 0.06231935, Validation loss: 0.21165074
INFO:root:[ 3100] Training loss: 0.06175082, Validation loss: 0.22008190
INFO:root:[ 3200] Training loss: 0.07363874, Validation loss: 0.23196961
INFO:root:[ 3300] Training loss: 0.05884588, Validation loss: 0.22779281
INFO:root:[ 3400] Training loss: 0.05889168, Validation loss: 0.22575553
INFO:root:[ 3500] Training loss: 0.06033585, Validation loss: 0.22637150
INFO:root:[ 3600] Training loss: 0.05859528, Validation loss: 0.23416275
INFO:root:[ 3700] Training loss: 0.09304771, Validation loss: 0.24215826
INFO:root:[ 3800] Training loss: 0.08004370, Validation loss: 0.23893237
INFO:root:[ 3900] Training loss: 0.08408616, Validation loss: 0.24510349
INFO:root:[ 4000] Training loss: 0.06254204, Validation loss: 0.25338933
INFO:root:[ 4100] Training loss: 0.05612151, Validation loss: 0.25341401
INFO:root:[ 4200] Training loss: 0.07046072, Validation loss: 0.24844970
INFO:root:[ 4300] Training loss: 0.05643429, Validation loss: 0.26326978
INFO:root:[ 4400] Training loss: 0.06019510, Validation loss: 0.27876201
INFO:root:[ 4500] Training loss: 0.06907022, Validation loss: 0.30630362
INFO:root:[ 4600] Training loss: 0.05406068, Validation loss: 0.27452749
INFO:root:[ 4700] Training loss: 0.05753782, Validation loss: 0.28034121
INFO:root:[ 4800] Training loss: 0.05245277, Validation loss: 0.27066329
INFO:root:[ 4900] Training loss: 0.05318234, Validation loss: 0.27357587
INFO:root:[ 5000] Training loss: 0.05884929, Validation loss: 0.27488047
INFO:root:[ 5100] Training loss: 0.05583728, Validation loss: 0.28450692
INFO:root:[ 5200] Training loss: 0.05062877, Validation loss: 0.28589192
INFO:root:[ 5300] Training loss: 0.05478422, Validation loss: 0.28714281
INFO:root:[ 5400] Training loss: 0.06764683, Validation loss: 0.30191621
INFO:root:[ 5500] Training loss: 0.11562395, Validation loss: 0.29586628
INFO:root:[ 5600] Training loss: 0.05069519, Validation loss: 0.30031613
INFO:root:[ 5700] Training loss: 0.05379253, Validation loss: 0.30129460
INFO:root:[ 5800] Training loss: 0.07188511, Validation loss: 0.30525413
INFO:root:[ 5900] Training loss: 0.05073603, Validation loss: 0.30791360
INFO:root:[ 6000] Training loss: 0.05082213, Validation loss: 0.30511487
INFO:root:[ 6100] Training loss: 0.04848240, Validation loss: 0.30901507
INFO:root:[ 6200] Training loss: 0.05579041, Validation loss: 0.42472538
INFO:root:[ 6300] Training loss: 0.05430642, Validation loss: 0.30649614
INFO:root:[ 6400] Training loss: 0.04972847, Validation loss: 0.31283760
INFO:root:[ 6500] Training loss: 0.04757862, Validation loss: 0.31269351
INFO:root:[ 6600] Training loss: 0.05324466, Validation loss: 0.32198590
INFO:root:[ 6700] Training loss: 0.06487004, Validation loss: 0.33351943
INFO:root:[ 6800] Training loss: 0.04809421, Validation loss: 0.34087130
INFO:root:[ 6900] Training loss: 0.05420442, Validation loss: 0.32752800
INFO:root:[ 7000] Training loss: 0.04658503, Validation loss: 0.34561247
INFO:root:[ 7100] Training loss: 0.04759279, Validation loss: 0.33427751
INFO:root:[ 7200] Training loss: 0.04828115, Validation loss: 0.34283054
INFO:root:[ 7300] Training loss: 0.05184085, Validation loss: 0.33694494
INFO:root:[ 7400] Training loss: 0.05466685, Validation loss: 1.00654316
INFO:root:[ 7500] Training loss: 0.04806111, Validation loss: 0.34070456
INFO:root:[ 7600] Training loss: 0.08313451, Validation loss: 0.37456298
INFO:root:[ 7700] Training loss: 0.06157843, Validation loss: 0.34670821
INFO:root:[ 7800] Training loss: 0.05650445, Validation loss: 0.35122862
INFO:root:[ 7900] Training loss: 0.05256846, Validation loss: 0.35604179
INFO:root:[ 8000] Training loss: 0.04494846, Validation loss: 0.35075948
INFO:root:[ 8100] Training loss: 0.05394918, Validation loss: 0.38607624
INFO:root:[ 8200] Training loss: 0.04529395, Validation loss: 0.36344594
INFO:root:[ 8300] Training loss: 0.04486960, Validation loss: 0.41096729
INFO:root:[ 8400] Training loss: 0.10086081, Validation loss: 0.35942006
INFO:root:[ 8500] Training loss: 0.04292957, Validation loss: 0.35891932
INFO:root:[ 8600] Training loss: 0.04401806, Validation loss: 0.36550054
INFO:root:[ 8700] Training loss: 0.05903180, Validation loss: 0.37965646
INFO:root:[ 8800] Training loss: 0.04842377, Validation loss: 0.37331209
INFO:root:[ 8900] Training loss: 0.04737133, Validation loss: 0.44671971
INFO:root:[ 9000] Training loss: 0.05179542, Validation loss: 0.37490690
INFO:root:[ 9100] Training loss: 0.04151154, Validation loss: 0.38660961
INFO:root:[ 9200] Training loss: 0.04429786, Validation loss: 0.38516968
INFO:root:[ 9300] Training loss: 0.04904288, Validation loss: 0.38500911
INFO:root:[ 9400] Training loss: 0.04315428, Validation loss: 0.38254571
INFO:root:[ 9500] Training loss: 0.04579303, Validation loss: 0.39605656
INFO:root:[ 9600] Training loss: 0.04655874, Validation loss: 0.38832337
INFO:root:[ 9700] Training loss: 0.05210750, Validation loss: 0.39216572
INFO:root:[ 9800] Training loss: 0.04095310, Validation loss: 0.38477516
INFO:root:[ 9900] Training loss: 0.06297270, Validation loss: 0.39838529
INFO:root:[10000] Training loss: 0.05834170, Validation loss: 0.40069559
INFO:root:[10100] Training loss: 0.04372574, Validation loss: 0.39385742
INFO:root:[10200] Training loss: 0.04152171, Validation loss: 0.39016476
INFO:root:[10300] Training loss: 0.04492594, Validation loss: 0.40137613
INFO:root:[10400] Training loss: 0.04578323, Validation loss: 0.39720559
INFO:root:[10500] Training loss: 0.06568184, Validation loss: 0.40943345
INFO:root:[10600] Training loss: 0.04095729, Validation loss: 0.40344769
INFO:root:[10700] Training loss: 0.04036321, Validation loss: 0.40299392
INFO:root:[10800] Training loss: 0.04819639, Validation loss: 0.39782488
INFO:root:[10900] Training loss: 0.08138656, Validation loss: 0.40911096
INFO:root:[11000] Training loss: 0.04260449, Validation loss: 0.40629739
INFO:root:[11100] Training loss: 0.06123661, Validation loss: 0.42247856
INFO:root:[11200] Training loss: 0.04129409, Validation loss: 0.41134337
INFO:root:[11300] Training loss: 0.04257849, Validation loss: 0.42305636
INFO:root:[11400] Training loss: 0.04044012, Validation loss: 0.41624889
INFO:root:[11500] Training loss: 0.04035640, Validation loss: 0.42452148
INFO:root:[11600] Training loss: 0.04268520, Validation loss: 0.41991180
INFO:root:[11700] Training loss: 0.04101903, Validation loss: 0.42579603
INFO:root:[11800] Training loss: 0.04265656, Validation loss: 0.42850542
INFO:root:[11900] Training loss: 0.08056447, Validation loss: 0.41618335
INFO:root:[12000] Training loss: 0.04023321, Validation loss: 0.42712948
INFO:root:[12100] Training loss: 0.04753820, Validation loss: 0.43206131
INFO:root:[12200] Training loss: 0.03730513, Validation loss: 0.43150565
INFO:root:[12300] Training loss: 0.04335067, Validation loss: 0.42995265
INFO:root:[12400] Training loss: 0.04346770, Validation loss: 0.43222874
INFO:root:[12500] Training loss: 0.03767288, Validation loss: 0.43127400
INFO:root:[12600] Training loss: 0.04101860, Validation loss: 0.44495448
INFO:root:[12700] Training loss: 0.03812221, Validation loss: 0.43758577
INFO:root:[12800] Training loss: 0.04639008, Validation loss: 0.44571671
INFO:root:[12900] Training loss: 0.05028836, Validation loss: 0.44017366
INFO:root:[13000] Training loss: 0.04278291, Validation loss: 0.45182166
INFO:root:[13100] Training loss: 0.07238032, Validation loss: 0.46127519
INFO:root:[13200] Training loss: 0.03973707, Validation loss: 0.43756557
INFO:root:[13300] Training loss: 0.06337112, Validation loss: 0.44600609
INFO:root:[13400] Training loss: 0.05865513, Validation loss: 0.45351791
INFO:root:[13500] Training loss: 0.04212675, Validation loss: 0.45714381
INFO:root:[13600] Training loss: 0.03655811, Validation loss: 0.45572376
INFO:root:[13700] Training loss: 0.04093267, Validation loss: 0.46019334
INFO:root:[13800] Training loss: 0.04421633, Validation loss: 0.46220031
INFO:root:[13900] Training loss: 0.03793796, Validation loss: 0.47619084
INFO:root:[14000] Training loss: 0.03892918, Validation loss: 0.46548191
INFO:root:[14100] Training loss: 0.04699043, Validation loss: 0.47185704
INFO:root:[14200] Training loss: 0.04021379, Validation loss: 0.46974525
INFO:root:[14300] Training loss: 0.03872351, Validation loss: 0.48163885
INFO:root:[14400] Training loss: 0.03814917, Validation loss: 0.46773857
INFO:root:[14500] Training loss: 0.04039273, Validation loss: 0.47120100
INFO:root:[14600] Training loss: 0.04085347, Validation loss: 0.54651898
INFO:root:[14700] Training loss: 0.05469936, Validation loss: 0.46980384
INFO:root:[14800] Training loss: 0.03844941, Validation loss: 0.47235575
INFO:root:[14900] Training loss: 0.04702629, Validation loss: 0.48599777
INFO:root:[15000] Training loss: 0.03824610, Validation loss: 0.49057713
INFO:root:[15100] Training loss: 0.03988324, Validation loss: 0.48384452
INFO:root:[15200] Training loss: 0.04176062, Validation loss: 0.49225581
INFO:root:[15300] Training loss: 0.03897703, Validation loss: 0.49689928
INFO:root:[15400] Training loss: 0.03906581, Validation loss: 0.49120024
INFO:root:[15500] Training loss: 0.03647065, Validation loss: 0.49653053
INFO:root:[15600] Training loss: 0.04336873, Validation loss: 0.49959332
INFO:root:[15700] Training loss: 0.04469306, Validation loss: 0.50227016
INFO:root:[15800] Training loss: 0.04229664, Validation loss: 0.49725178
INFO:root:[15900] Training loss: 0.04292062, Validation loss: 0.49633330
INFO:root:[16000] Training loss: 0.04190244, Validation loss: 0.50317198
INFO:root:[16100] Training loss: 0.03744642, Validation loss: 0.50772810
INFO:root:[16200] Training loss: 0.03511329, Validation loss: 0.50120360
INFO:root:[16300] Training loss: 0.06143516, Validation loss: 0.51050127
INFO:root:[16400] Training loss: 0.03691640, Validation loss: 0.50490481
INFO:root:[16500] Training loss: 0.05650037, Validation loss: 0.50762898
INFO:root:[16600] Training loss: 0.03579344, Validation loss: 0.53106952
INFO:root:[16700] Training loss: 0.03854841, Validation loss: 0.51380670
INFO:root:[16800] Training loss: 0.04505238, Validation loss: 0.51414490
INFO:root:[16900] Training loss: 0.03826779, Validation loss: 0.51017779
INFO:root:[17000] Training loss: 0.03954795, Validation loss: 0.51984847
INFO:root:[17100] Training loss: 0.04891989, Validation loss: 0.51777643
INFO:root:[17200] Training loss: 0.04229971, Validation loss: 0.51398998
INFO:root:[17300] Training loss: 0.03782479, Validation loss: 0.52264643
INFO:root:[17400] Training loss: 0.04482868, Validation loss: 0.52637619
INFO:root:[17500] Training loss: 0.04436714, Validation loss: 0.52040458
INFO:root:[17600] Training loss: 0.04384035, Validation loss: 0.53365666
INFO:root:[17700] Training loss: 0.04119509, Validation loss: 0.52260011
INFO:root:[17800] Training loss: 0.04002185, Validation loss: 0.52051657
INFO:root:[17900] Training loss: 0.04019825, Validation loss: 0.56637794
INFO:root:[18000] Training loss: 0.03418176, Validation loss: 0.53174013
INFO:root:[18100] Training loss: 0.05580105, Validation loss: 0.52589434
INFO:root:[18200] Training loss: 0.05118986, Validation loss: 0.53218240
INFO:root:[18300] Training loss: 0.03598437, Validation loss: 0.53110808
INFO:root:[18400] Training loss: 0.03482058, Validation loss: 0.53391063
INFO:root:[18500] Training loss: 0.05781836, Validation loss: 0.52994168
INFO:root:[18600] Training loss: 0.03888519, Validation loss: 0.53542811
INFO:root:[18700] Training loss: 0.03287891, Validation loss: 0.54759735
INFO:root:[18800] Training loss: 0.03821456, Validation loss: 0.56079108
INFO:root:[18900] Training loss: 0.03279769, Validation loss: 0.53581822
INFO:root:[19000] Training loss: 0.04001936, Validation loss: 0.54072869
INFO:root:[19100] Training loss: 0.03439787, Validation loss: 0.57270175
INFO:root:[19200] Training loss: 0.03741810, Validation loss: 0.53737837
INFO:root:[19300] Training loss: 0.03358057, Validation loss: 0.54945278
INFO:root:[19400] Training loss: 0.03849310, Validation loss: 0.54986149
INFO:root:[19500] Training loss: 0.04266076, Validation loss: 0.55017424
INFO:root:[19600] Training loss: 0.03607474, Validation loss: 0.55882174
INFO:root:[19700] Training loss: 0.04148511, Validation loss: 0.57834166
INFO:root:[19800] Training loss: 0.03281733, Validation loss: 0.55059832
INFO:root:[19900] Training loss: 0.04351281, Validation loss: 0.77707207
INFO:root:[20000] Training loss: 0.03274538, Validation loss: 0.56317753
INFO:root:Training the model took 18090.346s.
