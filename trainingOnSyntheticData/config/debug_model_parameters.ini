[META]
results_path = trainingOnSyntheticData/results/
data_path = trainingOnSyntheticData/data/
experiment_name = debug_model_parameters
objective = model_parameters

[TRAININGPARAMETERS]
; resume_training = 'trainingOnSyntheticData/results/20240507_104629_debug_model_parameters/Datetime_20240507_104629_Loss_training_set_size_946_batch_size_32_hidden_dim_32.pt'
; resume_training = 'trainingOnSyntheticData/results/Flexible dimension/full_19_big_training_sets/20240427_070711_5M/Datetime_20240427_070717_Loss_training_set_size_797545_batch_size_341_hidden_dim_256.pt'
resume_training = ''
embedding = ['no']
; no, linear, linear-relu, linear-relu-linear 
dim_embedding = None # dimension of the embedding
dim_hidden_embedding = None # dimension of the hidden layer in the embedding network; will only be used if embedding='linear-relu-linear'
hidden_dim = [32]
batch_size =  [32]
num_inds = 32
num_layers_enc = 2
num_layers = 4
n_epochs = 2
loss = 'CE'
early_stopping = 100
gamma_neg = 1
p_target = 0
model = 'set-transformer'
;set-transformer, mlp
n_head = 4
dropout = 0.0
init = 'default' # he, xavier, default
num_layers_classifier = 2
sab_in_output = False
learning_rate = 0.0001
num_example_predictions = 3
balanced_loss = False
lr_schedule = 'no'
; 'no', 'warm-up', 'step'
warmup_steps = 20
one_hot = True
alpha_label_smoothing = 0.0
optimizer = 'adam'
gradient_clipping = 1
layer_normalization = True
plt_activations = False
curriculum_learning = False
data_device = ['cpu'] # cpu or cuda (if available)
data_loader_pin_memory = False  # False if 
data_loader_num_workers = [0]
distributed_training = False

[DATAPARAMETERS]
generate_new_data = True
dataset = ['20240426_163529_1030_99_1K', '20240426_092423_1030_99_1K', '20240426_092440_1030_99_1K', '20240426_094749_1030_99_1K', '20240426_094806_1030_99_1K', '20240426_094825_1030_99_1K', '20240426_094841_1030_99_1K', '20240426_094858_1030_99_1K', '20240426_094914_1030_99_1K', '20240426_094932_1030_99_1K']
; ['20240426_163529_1030_99_1K', '20240426_092423_1030_99_1K', '20240426_092440_1030_99_1K', '20240426_094749_1030_99_1K', '20240426_094806_1030_99_1K']
; ['20240426_094825_1030_99_1K', '20240426_094841_1030_99_1K', '20240426_094858_1030_99_1K', '20240426_094914_1030_99_1K', '20240426_094932_1030_99_1K']
; ['20240426_121908_1030_99_10K']
training_set_size =  [1000]
test_set_size = 1000
validation_set_size = 1000
degree_input_numerator = 1
degree_input_denominator = 0
degree_output_numerator = 2
degree_output_denominator = 0
width = 1
ensure_uniqueness = False
normalization = ['DataSetWiseXY-11']
; 'DataSetWiseXY-11', 'DataSetWise'
n_functions_max = 1
num_samples_min = 200
num_samples_max = 200
function_names_str = ['cos']
num_visualizations = 0
flexible_dim = True 
d_min = 9
d_max = 9
maximal_potence = 3
enforce_function = False
most_complex_function = True
;Generate the function such that for each model parameter setting the "most complicated function" is generated,
; i.e., full use of the degrees and function specified
singularity_free_domain = True 
param_distribution = 'gaussian' # gaussian or bernoulli
enforce_training_set_size = False # should only be used for debugging
predict_only_function = False
use_fisher_yates_shuffle = False
target_noise = 0.0